Label,Authors,Abstract,,
Afanasyev,"Zhang H., Li Y., Zhang Z., Afanasyev A., Zhang L.","As a proposed Internet architecture, Named Data Networking (NDN) changes the network communication model from delivering packets to destinations identified by IP addresses to fetching data packets by names. This architectural change leads to changes of host functions and initial configurations. In this paper we present an overview of the basic functions of a host in an NDN network, together with necessary operations to configure an NDN host. We also compare and contrast the functionality and configuration between an NDN host and an IP host, to show the differences resulted from the different architecture designs. 2018 Association for Computing Machinery. All rights reserved.",,
Afanasyev,"Gibson C., Bermell-Garcia P., Chan K., Ko B., Afanasyev A., Zhang L.","The fundamental aim of this paper is to position the opportunities and challenges for adopting Named Data Networking (NDN) in the specific context of military coalition operations and tactical networks. The characteristic properties of tactical networks include high dynamics in multiple dimensions: bandwidth, network congestion, frequent topological changes, geographical mobility of assets, as well as dynamic changes in information access policies. Furthermore, coalition networks must provide secure and efficient communication across coalition boundaries and mitigate the impact of adversarial entities attempting to obstruct the mission. In this paper, we elaborate on the basic NDN architecture characteristics, including robust data discovery and retrieval over ad hoc and intermittent connectivity, inherent security, efficient content distribution, and automatic in-network caching; we also articulate how the above properties can all be utilized to enable resilient and secure data collection, improve the analytics capacity of the network, and to speed up and improve the quality of distributed decision making in challenging coalition environments. 2017 IEEE.",,
Afanasyev,"Flittner M., Mahfoudi M.N., Saucez D., Hlisch M.W., Iannone L., Bajpai V., Afanasyev A.","Reproducibility of artifacts is a cornerstone of most scientific publications. To improve the current state and strengthen ongoing community efforts towards reproducibility by design, we conducted a survey among the papers published at leading ACM computer networking conferences in 2017: CoNEXT, ICN, IMC, and SIGCOMM. The objective of this paper is to assess the current state of artifact availability and reproducibility based on a survey. We hope that it will serve as a starting point for further discussions to encourage researchers to ease the reproduction of scientific work published within the SIGCOMM community. Furthermore, we hope this work will inspire program chairs of future conferences to emphasize reproducibility within the ACM SIGCOMM community as well as will strengthen awareness of researchers. 2018 Association for Computing Machinery. All rights reserved.",,
Afanasyev,"Zhang Y., Wang L., Afanasyev A., Zhang L.","This papersummarizes our comparative study on the design choices of two classic link-state routing protocols: IS-IS and OSPF. Although both protocols are based on the same algorithmic foundations for computing best paths across large networks, they made different choices at various aspects in the protocol designs. We selected ten major design differences between the two to understand the reasoning behind their choices. We hope that this comparative study helps shed new light on the design space of link-state routing protocols and prove useful to future routing protocol design efforts. 2017 Association for Computing Machinery.",,
Afanasyev,"Chan K., Ko B., Mastorakis S., Afanasyev A., Zhang L.","In the current Named Data Networking implementation, forwarding a data request requires finding an exact match between the prefix of the name carried in the request and a forwarding table entry. However, consumers may not always know the exact naming, or an exact prefix, of their desired data. The current approach to this problem-establishing naming conventions and performing name lookup-can be infeasible in highly ad hoc, heterogeneous, and dynamic environments: the same data can be named using different terms or even languages, naming conventions may be minimal if they exist at all, and name lookups can be costly. In this paper, we present a fuzzy Interest forwarding approach that exploits semantic similarities between the names carried in Interest packets and the names of potentially matching data in CS and entries in FIB. We describe the fuzzy Interest forwarding approach and outline the semantic understanding function that determines the name matching. 2017 Association for Computing Machinery.",,
Afanasyev,"Shang W., Afanasyev A., Zhang L.","Distributed dataset synchronization (sync for short) provides an important abstraction for multi-party data-centric communication in the Named Data Networking (NDN) architecture. Since the beginning of the NDN project, several sync protocols have been developed, each made its own design choices that cause inefficiency under various conditions. Furthermore, none of them provides group membership management, making it difficult to remove departed nodes from the protocol state maintained at each node. This poster presents VectorSync, a new NDN sync protocol that is built upon the lessons learned so far, provides group membership management, and improves the efficiency of dataset synchronization. ©This work is partially supported by the National Science Foundation under awards CNS-1345318, CNS-1629922, and CNS-1719403. 2017 Association for Computing Machinery.",,
Afanasyev,"Zhang Z., Afanasyev A., Zhang L.","The Named Data Networking (NDN) architecture builds the security primitives into the network layer: all retrieved data packets must be signed to ensure their integrity, authenticity, and provenance. To ensure that these primitives are used in a meaningful way without imposing undue burdens on NDN users, the management of cryptographic keys and certificates needs to work in a simple, secure, and user-friendly way. This poster introduces the NDN Trust Management system (NDNCERT) which is designed to fill this need. NDNCERT provides exible mechanisms to delegate trust between certificates, either within a single device (managing permissions for local applications on a node to operate under a given namespace) or across devices/entities. NDNCERT features a modular design for security challenges that establish trust through out-of-band means for certificate issuing. Once a node or an application obtains a valid certificatee for its namespace (or being configured with a self-signed certificate), it automatically becomes a certificate authority for its namespace, and can use the same NDNCERT protocol to produce certificates for the sub-namespaces. 2017 Copyright held by the owner/author(s).",,
Afanasyev,"Zhang Z., Yu Y., Afanasyev A., Burke J., Zhang L.","As a proposed Internet architecture, Named Data Networking must provide effective security support: data authenticity, confidentiality, and availability. This poster focuses on supporting data confidentiality via encryption. The main challenge is to provide an easy-to-use key management mechanism that ensures only authorized parties are given the access to protected data. We describe the design of name-based access control (NAC) which provides automated key management by developing systematic naming conventions for both data and cryptographic keys. We also discuss an enhanced version of NAC that leverages attribute-based encryption mechanisms (NAC-ABE) to improve the flexibility of data access control and reduce communication, storage, and processing overheads. 2017 Copyright held by the owner/author(s).",,
Afanasyev,"Shang W., Li Y., Afanasyev A., Burke J., Zhang L.","Named Data Networking (NDN) architecture uses data-centric communication primitives that naturally support direct device-to-device (D2D) communications. To make NDN-enabled D2D communication a reality, this poster aims at two goals. First, we report our recent progress in enabling NDN connectivity over a number of popular D2D networking technologies. Second, we share with the broader community the roadblocks that we discovered in the process. Our experience suggests that launching a new network protocol stack for D2D communication on common platforms can be a daunting engineering challenge because of the lack of standard cross-platform APIs, limited documentation, and general platform restrictions to use L2 interfaces directly. Moreover, platforms are often equipped with different D2D networking technologies, forcing one to use many different means to interconnect different types of systems. 2017 Association for Computing Machinery.",,
Afanasyev,"Yu Y., Afanasyev A., Seedorf J., Zhang Z., Zhang L.","Named Data Networking (NDN) enables data-centric security in network communication by mandating digital signatures on networklayer data packets. Since the lifetime of some data can extend to many years, they outlive the lifetime of their signatures. This paper introduces NDN DeLorean, an authentication framework to ensure the long-term authenticity of long-lived data. The design of De- Lorean takes a publicly auditable bookkeeping service approach to keep permanent proofs of data signatures and the times when the signatures were generated. To assess DeLorean's feasibility, the paper presents a set of analytical evaluations on the operational cost as a function of data archive volumes. The paper also identifies several remaining issues that must be addressed in order to make DeLorean a general solution to authenticating long-lived data. 2017 ACM.",,
Afanasyev,"Mastorakis S., Afanasyev A., Yu Y., Zhang L.","BitTorrent is a popular application for peer-to-peer file sharing in today's Internet. To achieve robust and efficient data dissemination as an application overlay, BitTorrent implements a data-centric paradigm on top of TCP/IP's point-to-point packet delivery, which requires each peer to obtain network layer connectivity information (e.g., peer IP address, distance to each peer, routing policies) that is exclusively available at the network layer in order to select the best peers for data retrieval. This paper presents the design of nTorrent, which provides BitTorrent-like functions natively in Named Data Networking (NDN). We use simulations to examine how well the NDN's data-centric communication model can natively support such an application. Our work exposes the differences between the IP- based BitTorrent and nTorrent, and the issues and impact of moving IP-based applications to NDN-enabled networks. 2017 IEEE.",,
Afanasyev,"Afanasyev A., Jiang X., Yu Y., Tan J., Xia Y., Mankin A., Zhang L.","DNS provides a global-scale distributed lookup service to retrieve data of all types for a given name, be it IP addresses, service records, or cryptographic keys. This service has proven essential in today's operational Internet. Our experience with the design and development of Named Data Networking (NDN) suggests the need for a similar always-on lookup service. To fulfill this need we have designed the NDNS (NDN DNS) protocol, and learned several interesting lessons through the process. Although DNS's request-response operations seem closely resembling NDN's Interest-Data packet exchanges, they operate at different layers in the protocol stack. Comparing DNS's implementations over IP protocol stack with NDNS's implementation over NDN reveals several fundamental differences between applications designs for host-centric IP architecture and data-centric NDN architecture. 2017 IEEE.",,
Afanasyev,"Mastorakis S., Afanasyev A., Zhang L.","As a proposed Internet architecture, Named Data Networking (NDN) takes a fundamental departure from today's TCP/IP architecture, thus requiring extensive experimentation and evaluation. To facilitate such experimentation, we have developed ndnSIM, an open-source NDN simulator based on the NS-3 simulation framework. Since its first release in 2012, ndnSIM has gone through five years of active development and integration with the NDN prototype implementations, and has become a popular platform used by hundreds of researchers around the world. This paper presents an overview of the ndnSIM design, the ndnSIM development process, the design tradeoffs, and the reasons behind the design decisions. We also share with the community a number of lessons we have learned in the process.",,
Afanasyev,"Shang W., Wang Z., Afanasyev A., Burke J., Zhang L.","Many emerging IoT approaches depend on cloud services to facilitate interoperation of devices and services within them, even when all the communicating entities reside in the same local environment, as in many ""smart home"" applications. While such designs offer a straightforward way to implement IoT applications using today's TCP/IP protocol stack, they also introduce dependencies on external connectivity and services that are unnecessary and often brittle. This paper uses the design of an IoT-enabled home entertainment application, dubbed Flow, to demonstrate how the Named Data Networking (NDN) architecture enables cloud-independent IoT applications. NDN enables local trust management and rendezvous service, which play a foundational role in realizing other IoT services. By employing application-defined naming rather than host-based addressing at the network layer, and securing data directly, NDN enables straightforward and robust implementation of these two core functions for IoT networks without cloud connectivity. At the same time, NDN-based IoT designs can employ cloud services to complement local system capabilities. After describing the design and implementation of Flow, together with a discussion on preliminary generalization of the design, as an evaluation the paper conducts a brief thought exercise of how Flow could be realized using two popular IoT frameworks, Amazon's AWS IoT service and the Apple HomeKit framework, and compares that with the real implementation over NDN. 2017 ACM.",,
Afanasyev,"Afanasyev A., Halderman J.A., Ruoti S., Seamons K., Yu Y., Zappala D., Zhang L.","The World Wide Web has become the most common platform for building applications and delivering content. Yet despite years of research, the web continues to face severe security challenges related to data integrity and confidentiality. Rather than continuing the exploit-and-patch cycle, we propose addressing these challenges at an architectural level, by supplementing the web's existing connection-based and server-based security models with a new approach: contentbased security. With this approach, content is directly signed and encrypted at rest, enabling it to be delivered via any path and then validated by the browser. We explore how this new architectural approach can be applied to the web and analyze its security benefits. We then discuss a broad research agenda to realize this vision and the challenges that must be overcome. 2016 ACM.",,
Afanasyev,"Zhang Y., Afanasyev A., Burke J., Zhang L.","The initial Named Data Networking (NDN) architecture design provided consumer mobility support automatically, taking advantage of NDN's stateful forwarding plane to return data to mobile consumers; at the same time, the support of data producer mobility was left unspecified. During the past few years, a number of NDN producer mobility support schemes have been proposed. This paper provides a clear definition of NDN mobility support, to enable fetching of data produced by mobile users, and then to classify the proposed solutions by their commonalities and to articulate design tradeoffs of different approaches. We identify remaining challenges and discuss future research directions for effective and efficient data producer mobility support in NDN. 2016 IEEE.",,
Afanasyev,"Shang W., Bannis A., Liang T., Wang Z., Yu Y., Afanasyev A., Thompson J., Burke J., Zhang B., Zhang L.","The Internet of Things (IoT) is a vision for interconnecting all of the world's ""things""-from vehicles to diet scales, smart homes and electrical grids-through a common set of networking technologies. Realizing this vision using a host-to-host communication paradigm, such as that of the Internet Protocol (IP), is challenging in the context of highly heterogeneous, constrained devices that connect intermittently to one or more networks, often using multiple interfaces; communicate within various security regimes; and require both local and global communication capability. Using IP and similar protocols as the narrow waist of interoperability for IoT requires managing data exchange and security in terms that are largely orthogonal to application semantics, while simultaneously needing to minimize resource usage. This paper explores how Named Data Networking (NDN), a proposed future Internet architecture, addresses the root causes of these challenges and can help achieve the IoT vision in a more secure, straightforward, and innovationfriendly manner. NDN's data-centric communication model aligns network and application semantics, enabling developers to work with ""things"" and their data directly, and for IoT networks to be deployed and configured easily. To substantiate the high-level discussion, we give examples of ongoing design and implementation work in IoT over NDN and compare the architecture to wellknown existing protocols and frameworks. Finally, we discuss short- and long-term scenarios for employing NDN to enable the Internet of Things. 2016 IEEE.",,
Afanasyev,"Shang W., Afanasyev A., Zhang L.","The Named Data Networking (NDN) architecture has been proposed as a promising solution for supporting communications in IoT environments. An important class of IoT platform is the constrained devices that have limited computing resources and are connected by constrained networks. This paper presents the design and implementation of the NDN protocol stack for RIOT-OS, a popular operating system for constrained IoT platforms. We succeeded in integrating the core NDN packet forwarding logic into the RIOT-OS kernel together with a high-level application interface with data security support. Our results demonstrated the feasibility of using NDN protocol stack to support applications on constrained devices with only 10s of KB of RAM and flash memory. 2016 IEEE.",,
Afanasyev,"Afanasyev A., Yu Y., Zhang L., Burke J., Claffy K., Polterock J.","This report is a brief summary of the second NDN Community Meeting held at UCLA in Los Angeles, California on September 28-29, 2015. The meeting provided a platform for the attendees from 49 institutions across 13 countries to exchange their recent NDN research and development results, to debate existing and proposed functionality in NDN forwarding, routing, and security, and to provide feedback to the NDN architecture design evolution.",,
Afanasyev,"Afanasyev A., Zhu Z., Yu Y., Wang L., Zhang L.","Information sharing among a group of friends or colleagues in real life is usually a distributed process: we tell each other interesting or important news without any mandatory assistance or approval from a third party. Surprisingly, this is not what happens when sharing files among a group of friends over the Internet. While the goal of file sharing is to disseminate files among multiple parties, due to the constraints imposed by IP's point-to-point communication model, most of today's file sharing applications, such as Drop box, Google Drive, etc., resort to a centralized design paradigm: a user first uploads files to the server (cloud), and the server (cloud) re-distributes these files to other users, resulting in unnecessary tussles and inefficient data distribution paths. To bring the truly distributed file sharing back into the cyberspace, this paper presents Chrono Share, a distributed file sharing application built on top of the Named Data Networking (NDN) architecture. By walking through Chrono Share design details, we show how file sharing, as well as many other similar applications, can be effectively implemented over NDN in a truly distributed and secure manner. 2015 IEEE.",,
Afanasyev,"Yu Y., Afanasyev A., Clark D., Claffy K., Jacobson V., Zhang L.","Securing communication in network applications involves many complex tasks that can be daunting even for security experts. The Named Data Networking (NDN) architecture builds data authentication into the network layer by requiring all applications to sign and authenticate every data packet. To make this authentication usable, the decision about which keys can sign which data and the procedure of signature verification need to be automated. This paper explores the ability of NDN to enable such automation through the use of trust schemas. Trust schemas can provide data consumers an automatic way to discover which keys to use to authenticate individual data packets, and provide data producers an automatic decision process about which keys to use to sign data packets and, if keys are missing, how to create keys while ensuring that they are used only within a narrowly defined scope (""the least privilege principle""). We have developed a set of trust schemas for several prototype NDN applications with different trust models of varying complexity. Our experience suggests that this approach has the potential of being generally applicable to a wide range of NDN applications. 2015 ACM.",,
Afanasyev,"Abraham H.B., Afanasyev A., Yu Y., Zhang L., DiBenedetto S., Thompson J., Burke J.","This full day tutorial on synchronization and security in Named Data Networking (NDN) will share important architectural concepts we are exploring in these areas, the software we have built to perform these tasks, and remaining open issues. In particular, it will emphasize how the existing open source toolset provides a platform for exploring the open research questions. 2015 ACM.",,
Afanasyev,"Shi W., Afanasyev A.","Named Data Networking (NDN) is proposed to embrace the increasing demands of data-sharing and content dissemination. This paper proposes a distributed synchronization model, RepoSync, to provide accurate and efficient content synchronization. RepoSync adopts a combination of action-based and data-based synchronization, where actions are used to record the modifications of datasets, so as to instruct the sync process. Moreover, snapshot is introduced as data-based synchronization to provide the copy of dataset, handling the synchronization of large scale modifications. Simulations reveal that RepoSync can provide a promising performance of synchronization under various network conditions. 2015 IEEE.",,
Afanasyev,"Afanasyev A., Yi C., Wang L., Zhang B., Zhang L.","Named Data Networking (NDN) is a proposed information-centric design for the future Internet architecture, where application names are directly used to route requests for data. This key component of the architecture raises concerns about scalability of the forwarding system in NDN network, i.e., how to keep the routing table sizes under control given unbounded nature of application data namespaces. In this paper we apply a well-known concept of Map-and-Encap to provide a simple and secure namespace mapping solution to the scalability problem. More specifically, whenever necessary, application data names can be mapped to a set of globally routable names that are used to retrieve the data. By including such sets in data requests, we are informing (more precisely, hinting) the forwarding system of the whereabouts of the requested data, and such hints can be used when routers do not know from where to retrieve the data using application data names alone. This solution enables NDN forwarding to scale with the Internet's well-understood routing protocols and operational practice, while keeping all the benefits of the new NDN architecture. 2015 IEEE.",,
Afanasyev,"Claffy K.C., Polterock J., Afanasyev A., Burke J., Zhang L.","This report is a brief summary of the first NDN Community Meeting held at UCLA in Los Angeles, California on September 4-5, 2014. The meeting provided a platform for the attendees from 39 institutions across seven countries to exchange their recent NDN research and development results, to debate existing and proposed functionality in security support, and to provide feedback into the NDN architecture design evolution. 2015, Association for Computing Machinery. All rights reserved.",,
Afanasyev,"Pournaghshband V., Afanasyev A., Reiher P.","Routers or nodes on the Internet sometimes apply link-layer or IP-level compression on traffic flows with no knowledge of the end-hosts. If the end-host applications are aware of the compression already provided by an intermediary, they can save time and resources by not applying compression themselves. The benefits from these savings are even greater in mobile applications. We present a probing technique to detect the compression of traffic flows by intermediaries. Our technique is non-intrusive and robust to cross traffic. It is entirely end-to-end, requiring neither changes to nor information from intermediate nodes. We present two different but similar approaches based on how cooperative the end-hosts are. Our proposed technique only uses packet inter-arrival times for detection. It does not require synchronized clocks at the sender and receiver. Simulations and Internet experiments were used to evaluate our approach. Our findings demonstrate an accurate detection of compression applied to traffic flows by intermediaries. 2014 IEEE.",,
Afanasyev,"Yi C., Abraham J., Afanasyev A., Wang L., Zhang B., Zhang L.","A unique feature of Named Data Networking (NDN) is that its forwarding plane can detect and recover from network faults on its own, enabling each NDN router to handle network failures locally without relying on global routing convergence. This new feature prompts us to re-examine the role of routing in an NDN network: does it still need a routing protocol? If so, what impact may an intelligent forwarding plane have on the design and operation of NDN routing protocols? Through analysis and extensive simulations, we show that routing protocols remain highly beneficial in an NDN network. Routing disseminates initial topology and policy information as well as long-term changes in them, and computes the routing table to guide the forwarding process. However, because the forwarding plane is capable of detecting and recovering from failures quickly, routing no longer needs to handle short-term churns in the network. Freeing routing protocols from short-term churns can greatly improve their scalability and stability, enabling NDN to use routing protocols that were previously viewed as unsuitable for real networks.",,
Afanasyev,"Tikhomirov A., Afanasyev A., Kinash N., Trufanov A., Berestneva O., Rossodivita A., Gnatyuk S., Umerov R.","Abstract. An innovative approach for analysis of “network society” with its large- scale and multicomponent features has been proposed. A new network model - a model of so-called aggregate networks has been developed as a key tool for such analysis. These aggregate structures topologically are not identical in their global and local scales, and thus distinguished from canonical large-scale networks. It was elicited that aggregate network entities have significant features in their topological vulnerability in comparison with canonical ones. This is crucial for building resilient constructions of the network society. Also some additional distinctions for the concepts of “network” and “graph” have been formulated. Springer International Publishing Switzerland 2014.",,
Afanasyev,"Zhang L., Afanasyev A., Burke J., Jacobson V., Claffy K.C., Crowley P., Papadopoulos C., Wang L., Zhang B.","Named Data Networking (NDN) is one of five projects funded by the U.S. National Science Foundation under its Future Internet Architecture Program. NDN has its roots in an earlier project, Content-Centric Networking (CCN), which Van Jacobson first publicly presented in 2006.1 The NDN project investigates Jacobson's proposed evolution from today's host-centric network architecture (IP) to a data-centric network architecture (NDN). This conceptually simple shift has far-reaching implications for how we design, develop, deploy, and use networks and applications. We describe the motivation and vision of this new architecture, and its basic components and operations. We also provide a snapshot of its current design, development status, and research challenges. More information about the project, including prototype implementations, publications, and annual reports, is available on named-data.net.",,
Afanasyev,"Afanasyev A., Mahadevan P., Moiseenko I., Uzun E., Zhang L.","Distributed Denial of Service (DDoS) attacks are an ongoing problem in today's Internet, where packets from a large number of compromised hosts thwart the paths to the victim site and/or overload the victim machines. In a newly proposed future Internet architecture, Named Data Networking (NDN), end users request desired data by sending Interest packets, and the network delivers Data packets upon request only, effectively eliminating many existing DDoS attacks. However, an NDN network can be subject to a new type of DDoS attack, namely Interest packet flooding. In this paper we investigate effective solutions to mitigate Interest flooding. We show that NDN's inherent properties of storing per packet state on each router and maintaining flow balance (i.e., one Interest packet retrieves at most one Data packet) provides the basis for effective DDoS mitigation algorithms. Our evaluation through simulations shows that the solution can quickly and effectively respond and mitigate Interest flooding. 2013 IFIP.",,
Afanasyev,"Yi C., Afanasyev A., Moiseenko I., Wang L., Zhang B., Zhang L.","In Named Data Networking (NDN), packets carry data names instead of source and destination addresses. This paradigm shift leads to a new network forwarding plane: data consumers send Interest packets to request desired data, routers forward Interest packets and maintain the state of all pending Interests, which is then used to guide Data packets back to the consumers. Maintaining the pending Interest state, together with the two-way Interest and Data exchange, enables NDN routers' forwarding process to measure performance of different paths, quickly detect failures and retry alternative paths. In this paper we describe an initial design of NDN's forwarding plane and evaluate its data delivery performance under adverse conditions. Our results show that this stateful forwarding plane can successfully circumvent prefix hijackers, avoid failed links, and utilize multiple paths to mitigate congestion. We also compare NDN's performance with that of IP-based solutions to highlight the advantages of a stateful forwarding plane. 2013 Elsevier B.V. All rights reserved.",,
Afanasyev,"Zhu Z., Afanasyev A.","In supporting many distributed applications, such as group text messaging, file sharing, and joint editing, a basic requirement is the efficient and robust synchronization of knowledge about the dataset such as text messages, changes to the shared folder, or document edits. We propose ChronoSync protocol, which exploits the features of the Named Data Networking architecture to efficiently synchronize the state of a dataset among a distributed group of users. Using appropriate naming rules, ChronoSync summarizes the state of a dataset in a condensed cryptographic digest form and exchange it among the distributed parties. Differences of the dataset can be inferred from the digests and disseminated efficiently to all parties. With the complete and up-to-date knowledge of the dataset changes, applications can decide whether or when to fetch which pieces of the data. We implemented ChronoSync as a C++ library and developed two distributed application prototypes based on it. We show through simulations that ChronoSync is effective and efficient in synchronization dataset state, and is robust against packet losses and network partitions. 2013 IEEE.",,
Afanasyev,"Pournaghshband V., Kleinrock L., Reiher P., Afanasyev A.","Edge network operators have limited tools to control activities on their networks. This paper examines network dissuasion, a new approach to edge network control, based on controlling the fundamental parameters of the network, such as loss rate, delay, and jitter, with the intention of making particular uses of a network intolerable, while providing acceptable services for approved network uses. We investigate using this technique to prevent use of Voice Over IP (VoIP), while allowing other services. We designed network controls to achieve this goal and performed experiments using both measurements and subjective testing with human beings. We report on the degree of success and discuss the general promise of network dissuasion. 2012 IEEE.",,
Afanasyev,"Wang L., Afanasyev A., Kuntz R., Vuyyuru R., Wakikawa R., Zhang L.","This paper applies the Named Data Networking (NDN) concept to vehicle-to-vehicle (V2V) communications. Specifically, we develop a simple traffic information dissemination application based on the data naming design from our previous work and evaluate its performance through simulations. Our simulation results show that data names can greatly facilitate the forwarding process for Interest and data packets. With adequate vehicle density, data can propagate over long distances robustly at tens of kilometers per second, and a requester can retrieve the desired traffic information 10km away in a matter of seconds. 2012 ACM.",,
Afanasyev,"Yi C., Afanasyev A., Wang L., Zhang B., Zhang L.","In Named Data Networking (NDN) architecture, packets carry data names rather than source or destination addresses. This change of paradigm leads to a new data plane: data consumers send out Interest packets, routers forward them and maintain the state of pending Interests, which is used to guide Data packets back to the consumers. NDN routers' forwarding process is able to detect network problems by observing the two-way traffic of Interest and Data packets, and explore multiple alternative paths without loops. This is in sharp contrast to today's IP forwarding process which follows a single path chosen by the routing process, with no adaptability of its own. In this paper we outline the design of NDN's adaptive forwarding, articulate its potential benefits, and identify open research issues.",,
Afanasyev,"Kline E., Afanasyev A., Reiher P.","Denial-of-service (DoS) attacks continue to be a major problem on the Internet. While many defense mechanisms have been created, they all have significant deployment issues. This paper introduces a novel method that overcomes these issues, allowing a small number of deployed DoS defenses to act as secure on-demand shields for any node on the Internet. The proposed method is based on rerouting any packet addressed to a protected autonomous system (AS) through an intermediate filtering node - a shield. In this way, all potentially harmful traffic could be discarded before reaching the destination. The mechanisms for packet rerouting use existing routing techniques and do not require any kind of modification to the deployed protocols or routers. To make the proposed system feasible, from both deployment and usage points of view, traffic rerouting and outsourced filtering could be provided as an insurance-style on-demand service. 2011 IEEE.",,
Afanasyev,"Afanasyev A., Wang J., Peng C., Zhang L.","This paper tries to estimate redundancy level on the Web by employing information collected from existent search engines. To make measurements feasible, a representative set of Internet sites was collected using a random sampling of the Internet catalogs DMOZ and Delicious. Each page in the set was identified using a random 32-word phrase extracted from the content of the page. These phrases were used to perform search engine queries and infer the number of pages with the same content. Though the presented method is far from being perfectly accurate, it provides an approximation of a lower-bound for visible redundancy of the web-long phrases will likely belong to duplicate pages, and only the pages indexed by search engines are really visible to users. Obtained results showed a surprisingly low level of duplication averaged over all content types, with less then ten duplicates for most of the pages. This indicates that besides well-known classes of high-redundant content (news, mailing list archives, etc.), content duplication and plagiarism are not globally widespread across all types of webpages. 2011 ACM.",,
Afanasyev,"Afanasyev A., Tilley N., Reiher P., Kleinrock L.","The Transmission Control Protocol (TCP) carries most Internet traffic, so performance of the Internet depends to a great extent on how well TCP works. Performance characteristics of a particular version of TCP are defined by the congestion control algorithm it employs. This paper presents a survey of various congestion control proposals that preserve the original host-to-host idea of TCPnamely, that neither sender nor receiver relies on any explicit notification from the network. The proposed solutions focus on a variety of problems, starting with the basic problem of eliminating the phenomenon of congestion collapse, and also include the problems of effectively using the available network resources in different types of environments (wired, wireless, high-speed, long-delay, etc.). In a shared, highly distributed, and heterogeneous environment such as the Internet, effective network use depends not only on how well a single TCP-based application can utilize the network capacity, but also on how well it cooperates with other applications transmitting data through the same network. Our survey shows that over the last 20 years many host-to-host techniques have been developed that address several problems with different levels of reliability and precision. There have been enhancements allowing senders to detect fast packet losses and route changes. Other techniques have the ability to estimate the loss rate, the bottleneck buffer size, and level of congestion. The survey describes each congestion control alternative, its strengths and its weaknesses. Additionally, techniques that are in common use or available for testing are described. 2005 IEEE.",,
Bobadilla,"Inyim P., Batouli M., Reyes M.P., Carmenate T., Bobadilla L., Mostafavi A.","Occupant behavior is a significant contributor to energy waste in buildings. This research introduces an advanced smartphone application, developed based on the theoretical underpinnings of situational awareness theory, to effectively implement multi-method and personalized intervention to encourage energy conservation behaviors of building occupants. The new smart application provides several innovative features, such as energy saving points, customized feedback, and visualized user interface, which are implemented in the application to support multi-method interventions. The application was created using the Java language for Android devices. With the use of the Android platform, the app takes advantage of hardware technology from the user's mobile device. Measurement of occupancy behavior is accomplished by making use of the device's positional sensors. Orientation and geomagnetic field sensors serve to provide an accurate location of an occupant inside the building. The application can determine energy waste in a zone by using occupancy behavior. Moreover, the application offers real-time and projected future energy consumption based on occupants' behaviors. This novel feature can significantly improve communication that can lead to prompt action for building energy reduction. Results show how the app can compile raw data on energy behavior and make it easy to understand for the user through the use of visuals and statistical algorithms. 2018 by the authors.",,
Bobadilla,"Alam T., Reis G.M., Bobadilla L., Smith R.N.","Processes of scientific interest in the aquatic environment occur across multiple spatio-temporal time scales. To properly assess and understand these processes, we must observe aquatic ecosystems over long time periods. This requires examination of the problem of deploying multiple, inexpensive, and minimally-actuated drifting vehicles. We aim to utilize these persistent assets to explore all locations on the water surface, and examine the entirety an underwater environment through the visibility of downward-facing cameras. In this work, we propose a data-driven approach for the deployment of drifters that creates a stochastic model, finds the generalized flow pattern of the water, and studies the long-term behavior of an aquatic environment from a flow point-of-view. Given the long-term behavior of the environment, our approach finds attractors and their transient groups as the domains of attractions. We then determine a minimum number of deployment locations for the drifters using these attractors and their transient groups. Our simulation results based on actual ocean model prediction data demonstrate the applicability of our approach. 2018 IEEE.",,
Bobadilla,"Zanlongo S., Abodo F., Long P., Padir T., Bobadilla L.","There is a growing need for robots to perform complex tasks autonomously. However, there remain certain tasks that cannot - or should not - be completely automated. While these tasks may require one or several operators, we can oftentimes schedule when an operator should assist. We build on our previous work to present a methodology for allocating operator attention across multiple robots while attempting to minimize the execution time of the robots involved. In this paper, we: 1) Analyze of the complexity of this problem, 2) Provide a scalable methodology for designing robot policies so that few operators can oversee many robots, 3) Describe a methodology for designing both policies and robot trajectories to permit operators to assist many robots, and 4) Present simulation and hardware experiments demonstrating our methodologies. 2018 IEEE.",,
Bobadilla,"Rahman M.M., Bobadilla L., Mostafavi A., Carmenate T., Zanlongo S.A.","Collisions between automated moving equipment and human workers in job sites are one of the main sources of fatalities and accidents during the execution of construction projects. In this paper, we present a methodology to identify and assess project plans in terms of hazards before their execution. Our methodology has the following steps: 1) several potential plans are extracted from an initial activity graph; 2) plans are translated from a high-level activity graph to a discrete-event simulation model; 3) trajectories and safety policies are generated that avoid static and moving obstacles using existing motion planning algorithms; 4) safety scores and risk-based heatmaps are calculated based on the trajectories of moving equipment; and 5) managerial implications are provided to select an acceptable plan with the aid of a sensitivity analysis of different factors (cost, resources, and deadlines) that affect the safety of a plan. Finally, we present illustrative case study examples to demonstrate the usefulness of our model.Note to Practitioners-Currently, construction project planning does not explicitly consider safety due to a lack of automated tools that can identify a plan's safety level before its execution. This paper proposes an automated construction safety assessment tool which is able to evaluate the alternate construction plans and help to choose considering safety, cost, and deadlines. Our methodology uses discrete-event modeling along with motion planning to simulate the motions of workers and equipment, which account for most of the hazards in construction sites. Our method is capable of generating safe motion trajectories and coordination policies for both humans and machines to minimize the number of collisions. We also provide safety heatmaps as a spatiotemporal visual display of construction site to identify risky zones inside the environment throughout the entire timeline of the project. Additionally, a detailed sensitivity analysis helps to choose among plans in terms of safety, cost, and deadlines. 2004-2012 IEEE.",,
Bobadilla,"Rahman M.M., Bobadilla L., Abodo F., Rapp B.","In this paper, we solve the problem of relay robot placement in multi-robot missions to establish or enhance communication between a static operator and a number of remote units in an environment with known obstacles. We study the hardness of two different relay placement problems: 1) a chain formation of multiple relay robots to transmit information from an operator to a single unit; and 2) a spanning tree of relays connecting multiple remote units to the operator. We first build a communication map data structure from a layered graph that contains the positions of the relays as the unit moves. This structure is computed once and reused throughout the mission, significantly reducing plan re-computation time when compared to the best-known solution in the literature. Second, we create a min-arborescence tree that forms a connected component among the operator, relays, and units, and that has an optimal communication cost. Finally, we validate our ideas through software simulations, hardware experiments, and a comparison of our approach to state-of-the-art methods. 2017 IEEE.",,
Bobadilla,"Alam T., Rahman M.M., Bobadilla L., Rapp B.","In a communication-constrained and adversarial environment, it is an interesting and challenging problem for multiple vehicles to patrol the whole environment by sensing with their limited visibility. Early works based on deterministic paths are predictable to the adversary, and recent non-deterministic works are limited to only a circular environment and require synchronization. So, we propose a method of finding patrolling policies for multiple vehicles that monitor any polygonal environment using limited visibility regions and non-deterministic patrolling paths. First, limited visibility regions are calculated for a subset of locations that cover the whole environment or a part of the environment. Then, we find distributed patrolling policies in the form of Markov chains, using convex optimization to minimize the average expected commute time for the subset of the locations permitting each vehicle to cover the whole environment independently. We also find coordinated and Markov chain based patrolling policies that minimize the average commute time for the subset of locations permitting each vehicle to cover a part of the environment and additionally, communicate with a centralized base station. We present multiple simulation results to show the effectiveness of our visibility-based non-deterministic patrolling method. 2017 IEEE.",,
Bobadilla,"Reis G.M., Fitzpatrick M., Anderson J., Kelly J., Bobadilla L., Smith R.N.","Accurate and energy-efficient navigation and localization methods for autonomous underwater vehicles continues to be an active area of research. As interesting as they are important, ocean processes are spatiotemporally dynamic and their study requires vehicles that can maneuver and sample intelligently while underwater for extended durations. In this paper, we present a new technique for augmenting terrain-based navigation with physical water data to enhance the utility of traditional methods for navigation and localization. We examine the construct of this augmentation method over a range of deployment regions, e.g., ocean and freshwater lake. Data from field trials are presented and analyzed for multiple deployments of an autonomous underwater vehicle. 2017 IEEE.",,
Bobadilla,"Zanlongo S.A., Rahman M., Abodo F., Bobadilla L.","As society increases its reliance on remotely-operated and partially autonomous robots, we face the problem of how to effectively interact with large numbers of robots given a limited number of operators. In this paper, we present a methodology to allocate operator attention across multiple robots before plan execution while minimizing the duration of the plan. Our methodology has the following steps: 1) accept trajectories for robots and analyze where they will require supervision, 2) create a coordination space representing operator attention conflicts, 3) apply algorithms to attempt to reduce the time-span of robot activity by devising control policies, or assigning alternate trajectories, and 4) return a policy for each robot representing a feasible solution where operator attention is not overextended. Finally, we present software simulations and a physical implementation to demonstrate the usefulness of the methodology. 2017 IEEE.",,
Bobadilla,"Alam T., Bobadilla L., Shell D.A.","Equipped only with a clock and a contact sensor, a mobile robot can be programmed to bounce off walls in a predictable way: The robot drives forward until meeting an obstacle, then rotates in place and proceeds forward again. Though this behavior is easily modeled and trivially implemented, is it useful?We present an approach for solving both navigation and coverage problems using such a bouncing robot. The former entails finding a path from one pose to another, while the latter combines different paths over desired locations. Our approach has the following steps: 1) A directed graph is constructed from the environment geometry using the simple bouncing policies, 2) The shortest path on the graph, for navigation, is generated between either one given pair of initial and goal poses or all possible pairs of initial and goal poses, 3) The optimal distribution of bouncing policies is computed so that the actual coverage distribution is as close as possible to the target coverage distribution. Finally, we present experimental results from multiple simulations and hardware experiments to demonstrate the practical utility of our approach. 2017 IEEE.",,
Bobadilla,"Reis G.M., Fitzpatrick M., Anderson J., Bobadilla L., Smith R.N.","To effectively examine ocean processes, sampling campaigns require persistent autonomous underwater vehicles that are able to spend a majority of their deployment time maneuvering and gathering data underwater. Current navigation techniques rely either on high-powered sensors (e.g. Doppler Velocity Loggers) resulting in decreased deployment time, or dead reckoning (compass and IMU) with motion models resulting in poor navigational accuracy due to unbounded sensor drift. Recent work has shown that terrain-based navigation can augment existing navigation methods to bound sensor drift and reduce error in an energy-efficient manner. In this paper, we investigate the augmentation of terrain-based navigation with in situ science data to further increase navigation and localization accuracy. The motivation for this arises from the need for underwater vehicles to navigate within a spatiotemporally dynamic environment and gather data of high scientific value. We investigate a method to create a terrain map with maximum variability across the range of data available. These data combined with local bathymetry create a terrain that enables underwater vehicles to navigate and localize 1) relative to interesting water properties, and 2) globally based on the terrain and traditional methods. We examine a dataset of bathymetry and multiple science parameters gathered at the ocean surface at Big Fisherman's Cove on Santa Catalina Island and present a weighting for each parameter. We present efficient algorithms to obtain a convex combination of science and bathymetry parameters for unique trajectories generation. 2017 IEEE.",,
Bobadilla,"Zanlongo S.A., Wilson A.C., Bobadilla L., Sookoor T.","Mobile devices are expected to perform complex tasks exceeding their current hardware capabilities. Traditional cloud-based solutions are often useful, but fail in adversarial environments with limited communication connectivity. This can be exemplified in battlefield scenarios where soldiers may require access to large processing capabilities, without sacrificing their own mobility. In this paper, we extend previous work on computational ferrying, where Mobile High Performance Computers (MHPCs) physically move the necessary hardware into the proximity of mobile devices. Our extension proposes several improvements: 1) Path planning, which is used for reliable a priori estimation of distances between locations, yielding more accurate scheduling; 2) Collision checking, which permits robots to carry out the role of MHPCs; and 3) Prioritization, allowing operators to assign weights to tasks. In this paper, our algorithms are implemented and tested, with a comparison to previous work. 2016 IEEE.",,
Bobadilla,"Rahman M.M., Bobadilla L., Rapp B.","In communication-denied scenarios or contested environments, Line-of-sight(LoS) communication (by infrared or visible light) becomes one of the most reliable ways to send information between mobile units. In this paper, we consider the problem of verifying and repairing the visibility-based communication network formed among a number of servicing vehicles and military units that are dispersed in an environment with obstacles. We first formulate the problem in terms of algebraic graph theory to verify its connectivity. If any disconnection occurs due to a unit going out of the visibility polygons of all the vehicles, we propose a solution to repair it by dispatching a single vehicle that is optimal in terms of visibility and relocation cost. We tested our ideas through computer simulations in several case studies based on different mobility models. 2016 IEEE.","Pouyanfar S., Chen S.-C., Shyu M.-L.","Deep learning, particularly Convolutional Neural Networks (CNNs), has significantly improved visual data processing. In recent years, video classification has attracted significant attention in the multimedia and deep learning community. It is one of the most challenging tasks since both visual and temporal information should be processed effectively. Existing techniques either disregard temporal information between video sequences or generate very complex and computationally expensive models to integrate the spatiotemporal data. In addition, most deep learning techniques do not automatically consider the data imbalance problem. This paper presents an effective deep learning framework for imbalanced video classification by utilizing both spatial and temporal information. This framework includes a spatiotemporal synthetic oversampling to handle data with a skewed distribution, a pre-trained CNN model for spatial sequence feature extraction, followed by a residual bidirectional Long Short Term Memory (LSTM) to capture temporal knowledge in video datasets. Experimental results on two imbalanced video datasets demonstrate the superiority of the proposed framework compared to the state-of-the-art approaches. 2018 IEEE."
Bobadilla,"Rahman M.M., Bobadilla L., Rapp B.","Multiple objective navigation is commonly found in practice, where a path for an autonomous vehicle needs to be generated to simultaneously optimize a number of different objectives such as distance, safety, and visibility. Objectives can be weighted to solve a single objective optimization problem but appropriate weights may not be known a priori. In this paper, we formulate a series of missions for a group of vehicles that need to keep connectivity among themselves, surveil a group of targets, and minimize path lengths. These problems are solved by extending optimal sampling-based algorithms (RRT?) to support multiple objectives, non-additive costs and cooperative conditions. We present several increasingly complicated missions in obstacle-filled environments to illustrate and compare our ideas with existing methods. 2016 IEEE.",,
Bobadilla,"Inyim P., Carmenate T., Hidalgo D., Reyes M., Leante D., Bobadilla L., Mostafavi A.","Occupant behaviors contribute significantly to energy consumption and waste in buildings. Effective implementation of intervention strategies is the key to minimize energy wastes derived by occupant behaviors. In particular, intervention strategies should be personalized, compatible with the attributes of occupants, and based on real-time and predictive patterns of occupants' behaviors. This paper presents a smart system for integrated sensing, simulation, and feedback of occupant behaviors that enables implementation of personalized interventions. The proposed system includes a sensor network composed of non-invasive and inexpensive distance, light, temperature, and plug-load sensors to capture the presence of occupants and energy expenditures in different regions of a building. The data is stored for real-time evaluation of energy use and waste analysis. The data is also used for calibration and parameter estimation of an occupant energy simulation model used to predict occupants' behaviors and energy use projections. Both real-time and predicted data are stored in a database of a Smart Android Application. The application utilizes both antecedent and consequence intervention strategies to promote energy saving behaviors. Using robust machine learning algorithms, the application enables implementation of personalized intervention strategies based on the attributes and behaviors of each occupant. For example, the application enables providing feedback regarding the current and projected energy consumption by each occupant, personalized information-graphs of energy consumption, rating of energy performance for each occupant compared to other occupants, and personalized energy saving tips. The implementation of the smart system developed in this research introduces great opportunities for moving beyond the conventional one-size-fits-all approaches toward a more personalized approach for effective implementation of intervention strategies to reduce energy waste in buildings. ASCE.",,
Bobadilla,"Carmenate T., Inyim P., Pachekar N., Chauhan G., Bobadilla L., Batouli M., Mostafavi A.","The objective of this paper is to discover the emergent energy performance and determinants of energy waste in buildings. Electricity consumption in the U.S. attributes to 73% of energy waste in buildings and much of this waste is due to improper design, operation, and use of appliances. In particular, the operation or use phase of buildings and the way occupants behave significantly contribute to energy waste. Understanding the determinants of energy waste during the operation phase of buildings is a challenging task due to the complex interactions between the occupants, building units, and appliances. To decode these complex interactions and facilitate a better understanding of the determinants of energy waste, a simulation approach is used in this study. An agent-based simulation model was developed to capture the diverse attributes and dynamic behaviors of building occupants at the interface of human-building-appliance interactions. The application of the proposed model is demonstrated in a case study. Using simulation experiments, the interactions between occupant, building unit and appliance on energy consumption were investigated. The simulation model also was used for estimating determinants of energy waste. In addition, the simulation model includes a visualization interface that facilitates communication of strategies between the buildings users and facility managers. The results will highlight the significant attributes and effective strategies for energy waste reduction at the interface of human-building-appliance interactions. This information has potentially significant implications for building designers, facility managers, and users through a better understanding of emergent energy performance of buildings. 2016 The Authors.",,
Bobadilla,"Carmenate T., Rahman M.M., Leante D., Bobadilla L., Mostafavi A.","Buildings account for a majority of energy consumption in the United States. One of the major factors affecting the energy performance of buildings is occupant behaviors. Decoding occupant behaviors is a key to identifying energy waste and to discovering strategies to curtail energy consumption in buildings. We propose an information space approach for automated detection and proactive monitoring of energy waste due to occupant behaviors. In this paper we present a set of filtering algorithms to capture the minimum amount of information necessary to detect wasteful states and trajectories that occupants may have, in order to pro-actively modify occupant behaviors. We also describe and implement a sensor network consisting of inexpensive distance, light, temperature sensors and electricity consumption monitors utilized in order capture data related to occupancy behaviors. By keeping count of the number occupants and energy expenditures in different regions of a building, we accurately estimate how occupancy behavior is affecting energy use, in a non-invasive way. Furthermore, we present a methodology to pro-actively eliminate energy expenditure by calculating a score associated with occupants in different regions. This score will be used to suggest policies to users or facility managers to help reduce energy costs related to occupancy behaviors. 2015 IEEE.",,
Bobadilla,"Bobadilla L., Johnson T.T., Laviers A., Huzaifa U.","Movement primitives and formal methods have been proposed for many robotic applications. In this paper, we discuss work-in-progress utilizing formal methods for synthesizing high-level specifications written in linear temporal logic (LTL) realized with low-level prim- itives that ensure these specifications produce physically feasible swarm behaviors. The methodology synthesizes higher-level, choreographed behaviors for virtual kinematic chains of planar mobile robots that are realized with primitives consisting of (a) a one-dimensional distributed flocking algorithm with verified properties and (b) planar homogeneous trans- formations (rotations and translations). We show how to use the methodology to construct verified distributed algorithms for higher-dimensional (planar) shape formation. The existing one-dimensional algorithm has two main properties: (safety) avoidance of collisions between swarm members, and (progress) eventual flock (platoon) formation, which in one dimension is a roughly equal spacing between adjacent robots. By combining this one- dimensional flocking algorithm with other simple local operations, namely rotations and other distributed consensus (averaging) algorithms, we show how to create planar formations with planar safety and progress properties. 2015, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",,
Bobadilla,"Bobadilla L., Johnson T.T., LaViers A., Huzaifa U.","Movement primitives and formal methods have been proposed for many robotic applications. In this paper, we discuss work-in-progress utilizing formal methods for synthesizing high-level specifications written in linear temporal logic (LTL) realized with low-level primitives that ensure these specifications produce physically feasible swarm behaviors. The methodology synthesizes higher-level, choreographed behaviors for virtual kinematic chains of planar mobile robots that are realized with primitives consisting of (a) a one-dimensional distributed flocking algorithm with verified properties and (b) planar homogeneous transformations (rotations and translations). We show how to use the methodology to construct verified distributed algorithms for higher-dimensional (planar) shape formation. The existing one-dimensional algorithm has two main properties: (safety) avoidance of collisions between swarm members, and (progress) eventual flock (platoon) formation, which in one dimension is a roughly equal spacing between adjacent robots. By combining this one-dimensional flocking algorithm with other simple local operations, namely rotations and other distributed consensus (averaging) algorithms, we show how to create planar formations with planar safety and progress properties. 2015, E-flow American Institute of Aeronautics and Astronautics (AIAA). All rights reserved.",,
Bobadilla,"Carmenate T.S., Leante D., Zanlongo S.A., Bobadilla L., Mostafavi A.","Occupancy behaviors have been recognized as a major source of uncertainty in assessment of energy performance in buildings. The limitations in collecting information regarding occupancy behaviors have hindered the ability to fully decode the emergent occupant behaviors affecting the energy performance of buildings. In this paper, we demonstrate our preliminary results related to a novel system for non-invasive tracking of occupancy behaviors. Our analysis will consist of the deployment of a system, consisting of a non-invasive sensor network, to capture the movements of occupants and their behaviors within a residence while recording the information related to the indoor temperature of the location. Utilizing small Infrared sensors, the building's occupant's movements are detected and the occupancy levels are recorded without having to uniquely identify individuals. The proposed system is demonstrated in a case study of a residential home. Data collected from the case study shows the capability of the proposed system in capturing occupancy behaviors using minimal information in a non-invasive and inexpensive fashion. The methodology used an serve as a basis for detecting behaviors that lead to energy waste in buildings. The system implemented has the ability to capture information required to integrate building performance and occupant behaviors towards the realization of smart buildings. 2015 ASCE.",,
Bobadilla,"Rahman M.M., Carmenate T., Bobadilla L., Zanlongo S., Mostafavi A.","Collisions between moving machinery and human workers in construction job sites are one of the main sources of fatalities and accidents during the execution of construction projects. In this paper, we present a methodology to identify and assess construction project plan dangers before their execution. Our methodology has the following steps: 1) Plans are translated from a high-level activity graph to a discrete event simulation model; 2) Trajectories are simulated using sampling based and combinatorial motion planning algorithms; and 3) Safety scores and risk-based heatmaps are calculated based on the trajectories of moving equipment. Finally, we present an illustrative case study to demonstrate the usability of our model. 2015 IEEE.",,
Bobadilla,"Terry I.M., Wu A., Ramirez S., Makki A.P., Bobadilla L., Pissinou N., Iyengar S.S., Carbunar B.","The tight integration of mobile devices and apps into our daily routine impacts our approach to health. While existing solutions propose to take advantage of recent developments in mobile and sensor technologies to encourage users to lead healthier lives, we still lack a viable approach that motivates user participation. In this paper we aim to integrate fitness challenges into the daily routine of users, in order to motivate them to participate more frequently. We develop GeoFit, a mobile app that enables users to discover and add novel fitness challenges in their proximity. In addition to achievement badges, GeoFit relies on top-k lists to motivate users to become top performers. Since participation incentives may also raise cheating concerns, GeoFit leverages GPS and accelerometer sensors of the mobile device to verify the authenticity of fitness challenges performed by users. We have implemented and tested GeoFit on Android devices. Our experimental results show that the GeoFit client imposes only little overhead on the user device, while the server side can support hundreds of client interactions per second. 2014 IEEE.",,
Bobadilla,"Rahman M.M., Carmenate T., Bobadilla L., Mostafavi A.","Struck-by accidents are one of the major causes of fatalities and injuries in construction projects. While the likelihood of struck-by hazards is significantly affected by the layout of a jobsite, sequence of activities, and movement patterns of equipment and workers, there is currently no existing methodology for ex-ante investigation of struck-by safety hazards during the construction planning phase. In this paper, we propose a preliminary methodology for evaluation of struck-by safety hazards using a motion planning approach. We solve the problem of finding collision states and obstacle free paths for working in the middle of moving machinery and in the presence of various obstacles. We present, propose, and implement efficient algorithms to create safe routes for workers that avoid static and moving obstacles. We present a detailed case study of a common construction task involving struck-by hazards. We discuss several avenues for future work that could transform into robust methodologies for safety planning in complex construction projects. 2014 IEEE.",,
Bobadilla,"Gierl D.E., Bobadilla L., Sanchez O., Lavalle S.M.","This paper presents strategies for controlling the distribution of large numbers of minimalist robots (ones containing no sensors or computers). The strategies are implemented by varying area, speed, gate length, or gate configuration in environments composed of regions connected by gates and modelled by Continuous Time Markov chains. We demonstrate the effectiveness and practical feasibility of our strategies through physical experiments and simulation. We use Continuous Stochastic Logic to verify high level properties of our system and to evaluate the accuracy of our model. Also, we prove that our model is accurate and that our algorithms are efficient with respect to the number of regions and number of bodies. 2014 IEEE.",,
Bobadilla,"Bobadilla L., Mostafavi A., Carmenate T., Bista S.","Struck-by accidents are one of the major causes of fatalities in construction projects. The dynamic and complex nature of construction sites leads to layouts and motions that increase the vulnerability of workers and equipment to struck-by accidents. In this paper, we present our initial efforts towards a methodology based on information spaces for predictive assessment and proactive monitoring of struck-by safety hazards in construction sites. Our working hypothesis is that struck-by accidents occur due to undesirable states and trajectories in the physical state space that can be partially predicted and monitored in real time through efficient and robust algorithms. Our methodology includes multiple steps related to defining the physical state space and information spaces to compute filters for detecting hazardous states. In order to illustrate our methodology, we present three case studies of construction safety monitoring tasks related to: (1) worker-equipment collision avoidance; (2) falling/swinging load collision avoidance; and (3) distribution of workers in hazardous regions. ASCE 2014.",,
Bobadilla,"Tovar B., Cohen F., Bobadilla L., Czarnowski J., Lavalle S.M.","A problem is introduced in which a moving body (robot, human, animal, vehicle, and so on) travels among obstacles and binary detection beams that connect between obstacles or barriers. Each beam can be viewed as a virtual sensor that may have many possible alternative implementations. The task is to determine the possible body paths based only on sensor observations that each simply report that a beam crossing occurred. This is a basic filtering problem encountered in many settings, under a variety of sensingmodalities. Filtering methods are presented that reconstruct the set of possible paths at three levels of resolution: (1) the possible sequences of regions (bounded by beams and obstacles) visited, (2) equivalence classes of homo-topic paths, and (3) the possible numbers of times the path winds around obstacles. In the simplest case, all beams are disjoint, distinguishable, and directed. More complex cases are then considered, allowing for any amount of beams overlapping, indistinguishability, and lack of directional information. The method was implemented in simulation. An inexpensive, low-energy, easily deployable architecture was also created which implements the beam model and validates the methods of the article with experiments. 2014 ACM 1550-4859/2014/04-ART47 $15.00.",,
Bobadilla,"Bobadilla L., Martinez F., Gobst E., Gossman K., Lavalle S.M.","We present an approach to controlling multiple mobile robots without requiring system identification, geometric map building, localization, or state estimation. Instead, we purposely design them to execute wild motions, which means each will strike every open set infinitely often along the boundary of any connected region in which it is placed. We then divide the environment into a discrete set of regions, with borders delineated with simple markers, such as colored tape. Using simple sensor feedback, we show that complex tasks can be solved, such as patrolling, disentanglement, and basic navigation. The method is implemented in simulation and on real robots, which for many tasks are fully distributed without any mutual communication. 2012 AACC American Automatic Control Council).",,
Bobadilla,"Bobadilla L., Gossman K., Lavalle S.M.","This paper proposes methods for achieving basic tasks such as navigation, patrolling, herding, and coverage by exploiting the wild motions of very simple bodies in the environment. Bodies move within regions that are connected by gates that enforce specific rules of passage. Common issues such as dynamical system modeling, precise state estimation, and state feedback are avoided. The method is demonstrated in a series of experiments that manipulate the flow of weasel balls (without the weasels) and Hexbug Nano vibrating bugs. 2012 Springer London.",,
Bobadilla,"Bobadilla L., Sanchez O., Czarnowski J., Gossman K., LaValle S.M.","There is substantial interest in controlling a group of bodies from specifications of tasks given in a high-level, human-like language. This paper proposes a methodology that creates low-level hybrid controllers that guarantee that a group of bodies execute a high-level specified task without dynamical system modeling, precise state estimation or state feedback. We do this by exploiting the wild motions of very simple bodies in an environment connected by gates which serve as the system inputs, as opposed to motors on the bodies.We present experiments using inexpensive hardware demonstrating the practical feasibility of our approach to solving tasks such as navigation, patrolling, and coverage.",,
Bobadilla,"Bobadilla L., Sanchez O., Czarnowski J., LaValle S.M.","We consider the problem of determining the paths of multiple, unpredictable moving bodies in a cluttered environment using weak detection sensors that provide simple crossing information. Each sensor is a beam that, when broken, provides the direction of the crossing (one bit) and nothing else. Using a simple network of beams, the individual paths are separated and reconstructed as well as possible, up to combinatorial information about the route taken. In this setup, simple filtering algorithms are introduced, and a low-cost hardware implementation that demonstrates the practicality of the approach is shown. The results may apply in settings such as verification of multirobot system execution, surveillance and security, and unobtrusive behavioral monitoring for wildlife and the elderly. 2011 IEEE.",,
Bobadilla,"Bobadilla L., Niño F., Cepeda E., Patarroyo M.A.","Since there is a strong need for computational methods to predict and characterize functional sites for initial annotations of protein structures, a new methodology that relies on descriptions of the functional sites based on local properties is proposed in this paper. This new approach is independent of conserved residues and conserved residue geometry and takes advantage of the large number of protein structures available to construct models using a machine learning approach. Particularly, the proposed method performed feature extraction, clustering and classification on a protein structure data set, and it was validated on metal-binding sites (Ca2+, Zn2+, Na+,K+, Mg2+, Mn2+, Cu2+, Fe3+, Hg2+, Cl-) present in a non-redundant PDB (a total of 11,959 metal-binding sites in 3,609 proteins). Feature extraction provided a description of critical features for each metal-binding site, which were consistent with prior knowledge about them. Furthermore, new insights about metal-binding site microenvironments could be provided by the descriptors thus obtained. Results using k-fold cross-validation for classification showed accuracy above 90%. Complete proteins were scanned using these classifiers to locate metal-binding sites. 2007 IEEE.",,
Bobadilla,"Bobadilla L., Niño F., Cepeda E., Patarroyo M.A.","Developing computational methods for assigning protein function from tertiary structure is a very important problem, predicting a catalytic mechanism based only on structural information being a particularly challenging task. This work focuses on helping to understand the molecular basis of catalysis by exploring the nature of catalytic residues, their environment and characteristic properties in a large data set of enzyme structures and using this information to predict enzyme structures' active sites. A machine learning approach that performsfeature extraction, clustering and classification on a protein structure data set is proposed. 6,376 residues directly involved in enzyme catalysis, present in more than 800 proteins structures in the PDB were analyzed. Feature extraction provided a description of critical features for each catalytic residue, which were consistent with prior knowledge about them. Results from k-fold-cross-validation for classification showed more than 80% accuracy. Complete enzymes were scanned using these classifiers to locate catalytic residues. ©2007 IEEE.",,
Bobadilla,"Hernandez G., Bobadilla L., Sanchez O.","In this work, a genetic word clustering algorithm, that classifies words present in the phrases of a linguistic corpus, is proposed. The underlying goal of word classification is to build a good probabilistic model of the language defined by the phrases in the corpus. Some experiments comparing the performance of the proposed algorithm with a classical word clustering algorithm were carried out. 2005 IEEE.",,
Bobadilla,"Novaes A.C., Schaiquevich P., Nasswetter G., Marqués J., Uchoa M., Villanueva L., Aguilera S., Herrera Méndez J., Hernández G., Cotasio G., Mantilla Hernández R., Straka R., Rojas L., Zurita L., Alanis M., Gutiérrez M., Ayala J., Morales T., Taylor C., Garza R., De La Garza Castro S., Rodríguez N.M., Lemus O., Almada J., Muñoz I., Ramírez H., Colado G., Ureña S., Bobadilla L., Aldana A., Larias V., Ordóñez O., Leonori R., Jiménez R., Romero J., Acevedo R., López H., Galván R., Venzes G., Medélez J., Barrera E., Zepeda J., Díaz L., Aguilera J., Borga Pérez A., Lecona E., Ble Campos R., Herrera D., Rivera J., El Kouri E., Vilchez F., Peña F., Crespo V., Crespo A., Chacón G., Liendo V., Iciarte J., Ma?riquez F., Jelenkovich Z., Marín E., Rondón C.","Viscosuppplementation with intra-articular hyaluronic acid (hyaluronan [HA]) is a relatively new option for improving pain and articular function in patients with symptomatic knee osteoarthritis. An open multicenter study was performed in 365 patients with definite and symptomatic knee osteoarthritis from seven Latin American countries. Five doses of HA were administered once a week. The parameters studied were pain (six items), stiffness (two items) and functional capacity (17 items). The parameters were evaluated 1 week after the corresponding injection. Statistical differences were found when basal determinations of the three parameters were compared with the results of the first and fourth administration (p < 0.05). Intra-articular HA administration was well tolerated. Treatment-related nonserious adverse events were registered in 2.5% of administrations. Based on the results obtained, HA is a useful and well-tolerated symptomatic treatment for knee osteoarthritis with a rapid onset of action. 2005 Bioscience Ediprint Inc.",,
Carbunar,"Hernandez N., Rahman M., Recabarren R., Carbunar B.","The persistence of search rank fraud in online, peer-opinion systems, made possible by crowdsourcing sites and specialized fraud workers, shows that the current approach of detecting and filtering fraud is inefficient. We introduce a fraud de-anonymization approach to disincentivize search rank fraud: attribute user accounts flagged by fraud detection algorithms in online peer-opinion systems, to the human workers in crowdsourcing sites, who control them. We model fraud de-anonymization as a maximum likelihood estimation problem, and introduce UODA, an unconstrained optimization solution. We develop a graph based deep learning approach to predict ownership of account pairs by the same fraudster and use it to build discriminative fraud de-anonymization (DDA) and pseudonymous fraudster discovery algorithms (PFD). To address the lack of ground truth fraud data and its pernicious impacts on online systems that employ fraud detection, we propose the first cheating-resistant fraud de-anonymization validation protocol, that transforms human fraud workers into ground truth, performance evaluation oracles. In a user study with 16 human fraud workers, UODA achieved a precision of 91%. On ground truth data that we collected starting from other 23 fraud workers, our co-ownership predictor significantly outperformed a state-of-the-art competitor, and enabled DDA and PFD to discover tens of new fraud workers, and attribute thousands of suspicious user accounts to existing and newly discovered fraudsters. 2018 Association for Computing Machinery.",,
Carbunar,"Rahman M., Hernandez N., Carbunar B., Chau D.H.","We introduce the fraud de-anonymization problem, that goes beyond fraud detection, to unmask the human masterminds responsible for posting search rank fraud in online systems. We collect and study search rank fraud data from Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters recruited from 6 crowdsourcing sites.We propose Dolos, a fraud de-anonymization system that leverages traits and behaviors extracted from these studies, to attribute detected fraud to crowdsourcing site fraudsters, thus to real identities and bank accounts. We introduce MCDense, a mincut dense component detection algorithm to uncover groups of user accounts controlled by different fraudsters, and leverage stylometry and deep learning to attribute them to crowdsourcing site profiles. Dolos correctly identified the owners of 95% of fraudster-controlled communities, and uncovered fraudsters who promoted as many as 97.5% of fraud apps we collected from Google Play. When evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6 months, Dolos identified 1,056 apps with suspicious reviewer groups. We report orthogonal evidence of their fraud, including fraud duplicates and fraud re-posts. 2018 Association for Computing Machinery.",,
Carbunar,"Talukder S., Carbunar B.","Adversaries leverage social network friend relationships to collect sensitive data from users and target them with abuse that includes fake news, cyberbullying, malware, and propaganda. Case in point, 71 out of 80 user study participants had at least 1 Facebook friend with whom they never interact, either in Facebook or in real life, or whom they believe is likely to abuse their posted photos or status updates, or post offensive, false or malicious content. We introduce AbuSniff, a system that identifies Facebook friends perceived as strangers or abusive, and protects the user by unfriending, unfollowing, or restricting the access to information for such friends. We develop a questionnaire to detect perceived strangers and friend abuse. We introduce mutual Facebook activity features and show that they can train supervised learning algorithms to predict questionnaire responses. We have evaluated AbuSniff through several user studies with a total of 263 participants from 25 countries. After answering the questionnaire, participants agreed to unfollow and restrict abusers in 91.6% and 90.9% of the cases respectively, and sandbox or unfriend non-abusive strangers in 92.45% of the cases. Without answering the questionnaire, participants agreed to take the AbuSniff suggested action against friends predicted to be strangers or abusive, in 78.2% of the cases. AbuSniff increased the participant self-reported willingness to reject invitations from strangers and abusers, their awareness of friend abuse implications and their perceived protection from friend abuse. Copyright 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,
Carbunar,"Azimpourkivi M., Topkara U., Carbunar B.","Biometrics are widely used for authentication in consumer devices and business settings as they provide sufficiently strong security, instant veri.cation and convenience for users. However, biometrics are hard to keep secret, stolen biometrics pose lifelong security risks to users as they cannot be reset and re-issued, and transactions authenticated by biometrics across di.erent systems are linkable and traceable back to the individual identity. In addition, their cost-bene.t analysis does not include personal implications to users, who are least prepared for the imminent negative outcomes, and are not o.en given equally convenient alternative authentication options. We introduce ai.lock, a secret image based authentication method for mobile devices which uses an imaging sensor to reliably extract authentication credentials similar to biometrics. Despite lacking the regularities of biometric image features, we show that ai.lock consistently extracts features across authentication a.empts from general user captured images, to reconstruct credentials that can match and exceed the security of biometrics (EER = 0.71%). ai.lock only stores a ""hash"" of the object's image. We measure the security of ai.lock against brute force a.acks on more than 3.5 billion authentication instances built from more than 250,000 images of real objects, and 100,000 synthetically generated images using a generative adversarial network trained on object images. We show that the ai.lock Shannon entropy is superior to a .ngerprint based authentication built into popular mobile devices.",,
Carbunar,"Rahman M., Azimpourkivi M., Topkara U., Carbunar B.","The impact of citizen journalism raises important video integrity and credibility issues. In this article, we introduce Vamos, the first user transparent video ""liveness"" verification solution based on video motion, that accommodates the full range of camera movements, and supports videos of arbitrary length. Vamos uses the agreement between video motion and camera movement to corroborate the video authenticity. Vamos can be integrated into any mobile video capture application without requiring special user training. We develop novel attacks that target liveness verification solutions. The attacks leverage both fully automated algorithms and trained human experts. We introduce the concept of video motion categories to annotate the camera and user motion characteristics of arbitrary videos. We show that the performance of Vamos depends on the video motion category. Even though Vamos uses motion as a basis for verification, we observe a surprising and seemingly counter-intuitive resilience against attacks performed on relatively 'stationary' video chunks, which turn out to contain hard-to-imitate involuntary movements. We show that overall the accuracy of Vamos on the task of verifying whole length videos exceeds 93 percent against the new attacks. 2017 IEEE.",,
Carbunar,"Potharaju R., Rahman M., Carbunar B.","The difficulty of large-scale monitoring of app markets affects our understanding of their dynamics. This is particularly true for dimensions such as app update frequency, control and pricing, the impact of developer actions on app popularity, as well as coveted membership in top app lists. In this paper, we perform a detailed temporal analysis on two datasets we have collected from the Google Play Store, one consisting of 160 000 apps and the other of 87 223 newly released apps. We have monitored and collected data about these apps over more than six months. Our results show that a high number of these apps have not been updated over the monitoring interval. Moreover, these apps are controlled by a few developers that dominate the total number of app downloads. We observe that infrequently updated apps significantly impact the median app price. However, a changing app price does not correlate with the download count. Furthermore, we show that apps that attain higher ranks have better stability in top app lists. We show that app market analytics can help detect emerging threat vectors, and identify search rank fraud and even malware. Further, we discuss the research implications of app market analytics on improving developer and user experiences. 2014 IEEE.",,
Carbunar,"Sargolzaei A., Yen K.K., Abdelghani M.N., Sargolzaei S., Carbunar B.","Industrial control systems are distributed hierarchical networks that share information via an assortment of communication protocols. Such systems are vulnerable to attacks that can cause disastrous consequences. This article focuses on time delay switch (TDS) attacks and shows that cryptographic solutions are ineffective in recovering from the denial of service component of TDS attacks. Therefore, a cryptography-free TDS recovery (CF-TDSR) communication protocol enhancement is developed that leverages adaptive channel redundancy techniques and a novel state estimator, to detect and recover from the destabilizing effects of TDS attacks. Simulation results are conducted to prove that CF-TDSR ensures the control stability of linear time invariant systems and to show the efficacy of CF-TDSR against attacks deployed on multi-area load frequency control components of a distributed power grid. 2013 IEEE.",,
Carbunar,"Talukder S., Carbunar B.","We show through 2 user studies (n = 80), that a high number of participants have at least 1 Facebook friend whom they believe is likely to abuse their posted photos or status updates, or post offensive, false or malicious content, or with whom they never interact in Facebook and in real life. Our results reveal the importance of developing tools that automatically detect and defend against friend abuse in social networks. 2017 Copyright held by the owner/author(s).",,
Carbunar,"Rahman M., Carbunar B., Recabarren R., Lee D.","The profitability of fraud in online systems such as app markets and social networks marks the failure of existing defense mechanisms. In this paper, we propose FraudSys, a real-time fraud preemption approach that imposes Bitcoin-inspired computational puzzles on the devices that post online system activities, such as reviews and likes. We introduce and leverage several novel concepts that include (i) stateless, verifiable computational puzzles, that impose minimal performance overhead, but enable the efficient verification of their authenticity, (ii) a real-time, graph based solution to assign fraud scores to user activities, and (iii) mechanisms to dynamically adjust puzzle difficulty levels based on fraud scores and the computational capabilities of devices. FraudSys does not alter the experience of users in online systems, but delays fraudulent actions and consumes significant computational resources of the fraudsters. Using real datasets from Google Play and Facebook, we demonstrate the feasibility of FraudSys by showing that the devices of honest users are minimally impacted, while fraudster controlled devices receive daily computational penalties of up to 3,079 hours. In addition, we show that with FraudSys, fraud does not pay off, as a user equipped with mining hardware (e.g., AntMiner S7) will earn less than half through fraud than from honest Bitcoin mining. 2017 ACM.",,
Carbunar,"Rahman M., Rahman M., Carbunar B., Chau D.H.","Fraudulent behaviors in Google Play, the most popular Android app market, fuel search rank abuse and malware proliferation. To identify malware, previous work has focused on app executable and permission analysis. In this paper, we introduce FairPlay, a novel system that discovers and leverages traces left behind by fraudsters, to detect both malware and apps subjected to search rank fraud. FairPlay correlates review activities and uniquely combines detected review relations with linguistic and behavioral signals gleaned from Google Play app data (87 K apps, 2.9 M reviews, and 2.4M reviewers, collected over half a year), in order to identify suspicious apps. FairPlay achieves over 95 percent accuracy in classifying gold standard datasets of malware, fraudulent and legitimate apps. We show that 75 percent of the identified malware apps engage in search rank fraud. FairPlay discovers hundreds of fraudulent apps that currently evade Google Bouncer's detection technology. FairPlay also helped the discovery of more than 1,000 reviews, reported for 193 apps, that reveal a new type of 'coercive' review campaign: users are harassed into writing positive reviews, and install and review other apps. 1989-2012 IEEE.",,
Carbunar,"Guo M., Jin X., Pissinou N., Zanlongo S., Carbunar B., Iyengar S.S.","Recent advances in mobile device, wireless networking, and positional technologies have helped locationaware applications become pervasive. However, location trajectory privacy concerns hinder the adoptability of such applications. In this article, we survey existing trajectory privacy work in the context of wireless sensor networks, location-based services, and geosocial networks. In each context, we categorize and summarize the main techniques according to their own feathers. Furthermore, we discuss future trajectory privacy research challenges and directions. 2015 ACM.",,
Carbunar,"Carbunar B., Potharaju R.","Recently emerged app markets provide a centralized paradigm for software distribution in smartphones. The difficulty of massively collecting app data has led to a lack a good understanding of app market dynamics. In this paper we seek to address this problem, through a detailed temporal analysis of Google Play, Google's app market. We perform the analysis on data that we collected daily from 160,000 apps, over a period of six months in 2012. We report often surprising results. For instance, at most 50% of the apps are updated in all categories, which significantly impacts the median price. The average price does not exhibit seasonal monthly trends and a changing price does not show any observable correlation with the download count. In addition, productive developers are not creating many popular apps, but a few developers control apps which dominate the total number of downloads. We discuss the research implications of such analytics on improving developer and user experiences, and detecting emerging threat vectors. 2015 ACM.",,
Carbunar,"Rahman M., Azimpourkivi M., Topkara U., Carbunar B.","Citizen journalism videos increasingly complement or even replace the professional news coverage through direct reporting by event witnesses. This raises questions of the integrity and credibility of such videos. We introduce Vamos, the first user transparent video ""liveness"" verification solution based on video motion, that can be integrated into any mobile video capture application without requiring special user training. Vamos' algorithm not only accommodates the full range of camera movements, but also supports videos of arbitrary length. We develop strong attacks both by utilizing fully automated attackers and by employing trained human experts for creating fraudulent videos to thwart mobile video verification systems. We introduce the concept of video motion categories to annotate the camera and user motion characteristics of arbitrary videos. We share motion annotations of YouTube citizen journalism videos and of free-form video samples that we collected through a user study. We observe that the performance of Vamos differs across video motion categories. We report the expected performance of Vamos on the real citizen journalism video chunks, by projecting on the distribution of categories. Even though Vamos is based on motion, we observe a surprising and seemingly counterintuitive resilience against attacks performed on relatively ""static"" video chunks, which turn out to contain hard-to-imitate involuntary movements. We show that the accuracy of Vamos on the task of verifying whole length videos exceeds 93% against the new attacks. Copyright 2015 ACM.",,
Carbunar,"Rahman M., Carbunar B., Ballesteros J., Chau D.H.P.","The popularity and influence of reviews, make sites like Yelp ideal targets for malicious behaviors. We present Marco, a novel system that exploits the unique combination of social, spatial and temporal signals gleaned from Yelp, to detect venues whose ratings are impacted by fraudulent reviews. Marco increases the cost and complexity of attacks, by imposing a tradeoff on fraudsters, between their ability to impact venue ratings and their ability to remain undetected. We contribute a new dataset to the community, which consists of both ground truth and gold standard data. We show that Marco significantly outperforms state-of-the-art approaches, by achieving 94% accuracy in classifying reviews as fraudulent or genuine, and 95.8% accuracy in classifying venues as deceptive or legitimate. Marco successfully flagged 244 deceptive venues from our large dataset with 7,435 venues, 270,121 reviews and 195,417 users. Furthermore, we use Marco to evaluate the impact of Yelp events, organized for elite reviewers, on the hosting venues. We collect data from 149 Yelp elite events throughout the US. We show that two weeks after an event, twice as many hosting venues experience a significant rating boost rather than a negative impact. 2015 Wiley Periodicals, Inc.",,
Carbunar,"Terry I.M., Wu A., Ramirez S., Makki A.P., Bobadilla L., Pissinou N., Iyengar S.S., Carbunar B.","The tight integration of mobile devices and apps into our daily routine impacts our approach to health. While existing solutions propose to take advantage of recent developments in mobile and sensor technologies to encourage users to lead healthier lives, we still lack a viable approach that motivates user participation. In this paper we aim to integrate fitness challenges into the daily routine of users, in order to motivate them to participate more frequently. We develop GeoFit, a mobile app that enables users to discover and add novel fitness challenges in their proximity. In addition to achievement badges, GeoFit relies on top-k lists to motivate users to become top performers. Since participation incentives may also raise cheating concerns, GeoFit leverages GPS and accelerometer sensors of the mobile device to verify the authenticity of fitness challenges performed by users. We have implemented and tested GeoFit on Android devices. Our experimental results show that the GeoFit client imposes only little overhead on the user device, while the server side can support hundreds of client interactions per second. 2014 IEEE.",,
Carbunar,"Rahman M., Carbunar B., Topkara U.","The increasing interest in personal telemetry has induced a popularity surge for wearable personal fitness trackers. Such trackers automatically collect sensor data about the user throughout the day, and integrate it into social network accounts. Solution providers have to strike a balance between many constraints, leading to a design process that often puts security in the back seat. Case in point, we reverse engineered and identified security vulnerabilities in Fit bit Ultra and Gammon Forerunner 610, two popular and representative fitness tracker products. We introduce Fit Bite and GarMax, tools to launch efficient attacks against Fit bit and Garmin. We devise SensCrypt, a protocol for secure data storage and communication, for use by makers of affordable and lightweight personal trackers. SensCrypt thwarts not only the attacks we introduced, but also defends against powerful JTAG Read attacks. We have built Sens.io, an Arduino Uno based tracker platform, of similar capabilities but at a fraction of the cost of current solutions. On Sens.io, SensCrypt imposes a negligible write overhead and significantly reduces the end-to-end sync overhead of Fit bit and Garmin. 2014 IEEE.",,
Carbunar,"Carbunar B., Sion R., Potharaju R., Ehsan M.","Geosocial networks (GSNs) extend classic online social networks with the concept of location. Users can report their presence at venues through 'check-ins' and, when certain check-in sequences are satisfied, users acquire special status in the form of 'badges'. We first show that this innovative functionality is popular in Foursquare, a prominent GSN. Furthermore, we address the apparent tension between privacy and correctness, where users are unable to prove having satisfied badge conditions without revealing the corresponding time and location of their check-in sequences. To this end, we propose several privacy preserving protocols that enable users to prove having satisfied the conditions of several badge types. Specifically, we introduce (i) GeoBadge and T-Badge, solutions for acquiring location badges, (ii) FreqBadge, for mayorship badges, (iii) e-Badge, for proving various expertise levels and (iv) MPBadge, for accumulating multi-player badges. We show that a Google Nexus One smartphone is able to perform tens of badge proofs per minute while a provider can support hundreds of million of check-ins and badge verifications per day. 2012 IEEE.",,
Carbunar,"Carbunar B., Rahman M., Ballesteros J., Rishe N., Vasilakos A.V.","Profit is the main participation incentive for social network providers. Its reliance on user profiles, built from a wealth of voluntarily revealed personal information, exposes users to a variety of privacy vulnerabilities. In this paper, we propose to take first steps toward addressing the conflict between profit and privacy in geosocial networks. We introduce PROFIL R, a framework for constructing location centric profiles (LCPs), aggregates built over the profiles of users that have visited discrete locations (i.e., venues). PROFILR endows users with strong privacy guarantees and providers with correctness assurances. In addition to a venue centric approach, we propose a decentralized solution for computing real time LCP snapshots over the profiles of colocated users. An Android implementation shows that PROFILR is efficient; the end-to-end overhead is small even under strong privacy and correctness assurances. 2014 IEEE.",,
Carbunar,"Rahman M., Carbunar B., Ballesteros J., Burri G., Chau D.H.","The popularity and influence of reviews, make sites like Yelp ideal targets for malicious behaviors. We present Marco, a novel system that exploits the unique combination of social, spatial and temporal signals gleaned from Yelp, to detect venues whose ratings are impacted by fraudulent reviews. Marco increases the cost and complexity of attacks, by imposing a tradeoff on fraudsters, between their ability to impact venue ratings and their ability to remain undetected. We contribute a new dataset to the community, which consists of both ground truth and gold standard data. We show' that Marco significantly outperforms state-of-the-art approaches, by achieving 94% accuracy in classifying reviews as fraudulent or genuine, and 95.8% accuracy in classifying venues as deceptive or legitimate. Marco successfully flagged 244 deceptive venues from our large dataset with 7,435 venues, 270,121 reviews and 195,417 users. Among the San Francisco car repair and moving companies that we analyzed, almost 10% exhibit fraudulent behaviors. Copyright SIAM.",,
Carbunar,"Ballesteros J., Carbunar B., Rahman M., Rishe N., Iyengar S.S.","Population density and natural and man-made disasters make public safety a concern of growing importance. In this paper we aim to enable the vision of smart and safe cities by exploiting mobile and social networking technologies to securely and privately extract, model and embed real-time public safety information into quotidian user experiences. We first propose novel approaches to define location-and user-based safety metrics. We evaluate the ability of existing forecasting techniques to predict future safety values. We introduce iSafe, a privacy-preserving algorithm for computing safety snapshots of co-located mobile devices as well as geosocial network users. We present implementation details of iSafe as both an Android application and a browser plugin that visualizes safety levels of visited locations and browsed geosocial venues. We evaluate iSafe using crime and census data from the Miami-Dade (FL) county as well as data we collected from Yelp, a popular geosocial network. 2014 IEEE.",,
Carbunar,"Carbunar B., Potharaju R.","GeoSocial Networks (GSNs) are online social networks centered on the location information of their users. Users ""check-in"" their location and use it to acquire location-based special status (e.g., badges, mayorships) and receive venue dependent rewards. The strategy of rewarding user participation however makes cheating a profitable behavior. In this paper we introduce XACT, a suite of venue-oriented secure location verification mechanisms that enable venues and GSN providers to certify the locations claimed by users. We prove that XACT is correct, secure and easy to use. We validate the need for secure location verification mechanisms by collecting and analyzing data from the most popular GSNs today: 780,000 Foursquare users and 143,000 Gowalla users. Through a proof-of-concept implementation on a Revision C4 BeagleBoard embedded system we show that XACT is easy to deploy and economically viable. We analytically and empirically prove that XACT detects location cheating attacks. 2012 IEEE.",,
Carbunar,"Ballesteros J., Rahman M., Carbunar B., Rishe N.","Smart cities combine technology and human resources to improve the quality of life and reduce expenditures. Ensuring the safety of city residents remains one of the open problems, as standard budgetary investments fail to decrease crime levels. This work takes steps toward implementing smart, safe cities, by combining the use of personal mobile devices and social networks to make users aware of the safety of their surroundings. We propose novel metrics to define location and user based safety values. We evaluate the ability of forecasting techniques including autoregressive integrated moving average (ARIMA) and artificial neural networks (ANN) to predict future safety values. We devise iSafe, a privacy preserving algorithm for computing safety snapshots of co-located mobile device users and integrate our approach into an Android application for visualizing safety levels. We further investigate relationships between location dependent social network activity and crime levels. We evaluate our contributions using data we collected from Yelp as well as crime and census data. 2012 IEEE.",,
Carbunar,"Carbunar B., Rahman M., Rishe N., Ballesteros J.","Providing input to targeted advertising, profiling social network users is an important source of revenue for geosocial networks. Since profiles contain personal information, their construction introduces a trade-off between user privacy and incentives of participation for businesses and geosocial network providers. In this paper we introduce location centric profiles (LCPs), aggregates built over the profiles of users present at a given location. We introduce ProfilR, a suite of mechanisms that construct LCPs in a private and correct manner. Our Android implementation shows that Profil R is efficient: the end-to-end overhead is small even under strong correctness assurances. 2012 Authors.",,
Carbunar,"Feng T., Liu Z., Carbunar B., Boumber D., Shi W.","While enriching the user experiences, the development of mobile devices and applications introduces new security and privacy vulnerabilities for the remote services accessed by mobile device users. A trusted and usable authentication architecture for mobile devices is thus in high demand. In this paper, we leverage a unified structure, consisting of transparent TFTbased fingerprint sensors, touchscreen, and display, to propose a novel identity management mechanism that authenticates users of touch based mobile devices for accessing the local devices and remote services. Our solution differs from the previous onetime and enforced authentication approaches through two novel features: (i) user transparent authentication process, requiring neither password nor extra login steps and (ii) continuous identity management based on fingerprint biometric, where each user-to-device touch interaction is used toward authentication. Moreover, we introduce two different security scenarios, one for local identity management, and the second extended solution for remote identity management. Finally we employ TRUST (Trust Reinforcement based on Unified Structural Touch-display) to solve the identity challenge in cyber space. 2012 IEEE.",,
Carbunar,"Feng T., Liu Z., Kwon K.-A., Shi W., Carbunar B., Jiang Y., Nguyen N.","Securing the sensitive data stored and accessed from mobile devices makes user authentication a problem of paramount importance. The tension between security and usability renders however the task of user authentication on mobile devices a challenging task. This paper introduces FAST (Fingergestures Authentication System using Touchscreen), a novel touchscreen based authentication approach on mobile devices. Besides extracting touch data from touchscreen equipped smartphones, FAST complements and validates this data using a digital sensor glove that we have built using off-the-shelf components. FAST leverages state-of-the-art classification algorithms to provide transparent and continuous mobile system protection. A notable feature is FAST 's continuous, user transparent post-login authentication. We use touch data collected from 40 users to show that FAST achieves a False Accept Rate (FAR) of 4.66% and False Reject Rate of 0.13% for the continuous post-login user authentication. The low FAR and FRR values indicate that FAST provides excellent post-login access security, without disturbing the honest mobile users. 2012 IEEE.",,
Carbunar,"Carbunar B., Potharaju R., Pearce M., Vasudevan V.",Video on Demand (VoD) services allow users to select and locally consume remotely stored content. We investigate the use of caching to solve the scalability issues of several existing VoD providers. We propose metrics and goals that define the requirements of a caching framework for CDNs of VoD systems. Using data logs collected from Motorola equipment from Comcast VoD deployments we show that several classic caching solutions do not satisfy the proposed goals. We address this issue by developing a novel technique for predicting future values of several metrics of interest. We use these predictions to evaluate the penalty imposed on the system (network and caches) when not caching individual items. We use item penalties to propose novel caching and static placement strategies. We use the mentioned data logs to validate our solutions and show that they satisfy all the defined goals. 2012 IEEE.,,
Carbunar,"Carbunar B., Chen Y., Sion R.","We design and analyze the first practical anonymous payment mechanisms for network services. We start by reporting on our experience with the implementation of a routing micropayment solution for Tor. We then propose micropayment protocols of increasingly complex requirements for networked services, such as P2P or cloud-hosted services. The solutions are efficient, with bandwidth and latency overheads of under 4% and 0.9 ms, respectively, in the ORPay implementation, provide full anonymity (for both payers and payees), and support thousands of transactions per second. 2005-2012 IEEE.",,
Carbunar,"Carbunar B., Sion R., Potharaju R., Ehsan M.","Location based social or geosocial networks (GSNs) have recently emerged as a natural combination of location based services with online social networks: users register their location and activities, share it with friends and achieve special status (e.g., ""mayorship"" badges) based on aggregate location predicates. Boasting millions of users and tens of millions of daily check-ins, such services pose significant privacy threats: user location information may be tracked and leaked to third parties. Conversely, a solution enabling location privacy may provide cheating capabilities to users wanting to claim special location status. In this paper we introduce new mechanisms that allow users to (inter)act privately in today's geosocial networks while simultaneously ensuring honest behavior. An Android implementation is provided. The Google Nexus One smartphone is shown to be able to perform tens of badge proofs per minute. Providers can support hundreds of million of check-ins and status verifications per day. 2012 Springer-Verlag.","Clarke P.J., Davis D.L., Chang-Lau R., King T.M.","Software continues to affect a major part of our daily lives, including the way we use our phones, home appliances, medical devices, and cars. The pervasiveness of software has led to a growing demand for software developers over the next decade. To ensure the high quality of software developed in industry, students being trained in software engineering also need to be trained on how to use testing techniques and supporting tools effectively at all levels of development. In this article, we investigate how testing tools are used in the software project of an undergraduate testing course.We also investigate howa cyberlearning environment-TheWeb-Based Repository of Software Testing Tutorials (WReSTT)-is used to supplement the learning materials presented in class, particularly the tutorials on different software testing tools. The results of a study spanning three semesters of the undergraduate course suggest that (1) the use of code coverage tools motivates students to improve their test suites; (2) the number of bugs found when using coverage tools slightly increased, which is similar to the results found in the research literature; and (3) students find WReSTT to be a useful resource for learning about software testing techniques and the use of code coverage tools. 2017 ACM."
Carbunar,"Franz M., Williams P., Carbunar B., Katzenbeisser S., Peter A., Sion R., Sotakova M.","In the past few years, outsourcing private data to untrusted servers has become an important challenge. This raises severe questions concerning the security and privacy of the data on the external storage. In this paper we consider a scenario where multiple clients want to share data on a server, while hiding all access patterns. We propose here a first solution to this problem based on Oblivious RAM (ORAM) techniques. Data owners can delegate rights to external new clients enabling them to privately access portions of the outsourced data served by a curious server. Our solution is as efficient as the underlying ORAM constructs and allows for delegated read or write access while ensuring strong guarantees for the privacy of the outsourced data. The server does not learn anything about client access patterns while clients do not learn anything more than what their delegated rights permit. 2012 Springer-Verlag.",,
Carbunar,"Carbunar B., Tripunitara M.V.","With the recent advent of cloud computing, the concept of outsourcing computations, initiated by volunteer computing efforts, is being revamped. While the two paradigms differ in several dimensions, they also share challenges, stemming from the lack of trust between outsourcers and workers. In this work, we propose a unifying trust framework, where correct participation is financially rewarded: neither participant is trusted, yet outsourced computations are efficiently verified {and} validly remunerated. We propose three solutions for this problem, relying on an offline bank to generate and redeem payments; the bank is oblivious to interactions between outsourcers and workers. We propose several attacks that can be launched against our framework and study the effectiveness of our solutions. We implemented our most secure solution and our experiments show that it is efficient: the bank can perform hundreds of payment transactions per second and the overheads imposed on outsourcers and workers are negligible. 2006 IEEE.",,
Carbunar,"Carbunar B., Sion R.","We introduce WORM-ORAM, a first mechanism that combines Oblivious RAM (ORAM) access privacy and data confidentiality with Write-Once Read-Many (WORM) regulatory data retention guarantees. Clients can outsource their database to a server with full confidentiality and data access privacy, and, for data retention, the server ensures client access WORM semantics. In general simple confidentiality and WORM assurances are easily achievable, e.g., via an encrypted outsourced data repository with server-enforced read-only access to existing records (albeit encrypted). However, this becomes hard when also access privacy is to be ensuredwhen client access patterns are necessarily hidden and the server cannot enforce access control directly. WORM-ORAM overcomes this by deploying a set of zero-knowledge proofs to convince the server that all stages of the protocol are WORM-compliant. 2006 IEEE.",,
Carbunar,"Carbunar B., Sion R.","Location based social or geosocial networks (GSNs) have recently emerged as a natural combination of location based services with online social networks: users register their location and activities, share it with friends and achieve special status (e.g., ""mayorship"" badges) based on aggregate location predicates. Boasting millions of users and tens of daily check-ins, such services pose significant privacy threats: user location information may be tracked and leaked to third parties. Conversely, a solution enabling location privacy may provide cheating capabilities to users wanting to claim special location status. In this paper we introduce new mechanisms that allow users to (inter)act privately in today's geosocial networks while simultaneously ensuring honest behaviors. We show that our solutions are efficient both on the provider and the client side. 2011 Authors.",,
Carbunar,"Carbunar B., Pearce M., Vasudevan V., Needham M.","Video on Demand (VoD) services provide a wide range of content options and enable subscribers to select, retrieve and locally consume desired content. In this work we propose caching solutions to improve the scalability of the content distribution networks (CDNs) of existing VoD architectures. We first investigate metrics relevant to this caching framework and subsequently define goals that should be satisfied by an efficient solution. We propose novel techniques for predicting future values of metrics of interest. We use our prediction mechanisms to define the cost imposed on the system (network and caches) by items that are not cached. We use this cost to develop novel caching and static placement strategies. We validate our solutions using log data collected from Motorola equipment from several Comcast VoD deployments. 2011 IEEE.",,
Carbunar,"Carbunar B., Shi W., Sion R.","We introduce a novel conditional e-cash protocol allowing future anonymous cashing of bank-issued e-money only upon the satisfaction of an agreed-upon public condition. Payers are able to remunerate payees for services that depend on future, yet to be determined outcomes of events. Moreover, payees are able to further transfer payments to third parties. Once the payment is complete, any double-spending attempt by the payer will reveal its identity; no double spending by any of payees in the payee transfer chain is possible. Payers cannot be linked to payees or to ongoing or past transactions. The flow of cash within the system is thus both correct and anonymous. We discuss several applications of conditional e-cash including online trading of financial securities, prediction markets, and betting systems. 2010 Elsevier Inc. All rights reserved.",,
Carbunar,"Chen Y., Sion R., Carbunar B.","We design and analyze the first practical anonymous payment mechanisms for network services. We start by reporting on our experience with the implementation of a routing micropayment solution for Tor. We then propose micropayment protocols of increasingly complex requirements for networked services, such as P2P or cloud-hosted services. The solutions are efficient, with bandwidth and latency overheads of under 4% and 0.9 ms respectively (in ORPay for Tor), provide full anonymity (both for payers and payees), and support thousands of transactions per second. Copyright 2009 ACM.",,
Carbunar,"Tripunitara M.V., Carbunar B.","We address the distributed setting for enforcement of a centralized Role-Based Access Control (RBAC) protection state. We present a new approach for time- and space-efficient access enforcement. Underlying our approach is a data structure that we call a cascade Bloom filter. We describe our approach, provide details about the cascade Bloom filter, its associated algorithms, soundness and completeness properties for those algorithms, and provide an empirical validation for distributed access enforcement of RBAC. We demonstrate that even in low-capability devices such as WiFi network access points, we can perform thousands of access checks in a second. Copyright 2009 ACM.",,
Carbunar,"Nita-Rotaru C., Carbunar B., Ioannis I.","Hybrid networks consisting of cellular and Wi-Fi networks were proposed as a high-throughput architecture for cellular services. In such networks, devices equipped with cellular and Wi-Fi network cards access Internet services through the cellular base station. The Wi-Fi interface is used to provide a better service to clients that are far away from the base station, via multihop ad hoc paths. The modified trust model of hybrid networks generates a set of new security challenges as clients rely on intermediate nodes to participate effectively in the resource reservation process and data forwarding. In this paper, we introduce JANUS, a framework for scalable, secure, and efficient routing for hybrid cellular and Wi-Fi networks. JANUS uses a scalable routing algorithm with multiple channel access, for improved network throughput. In addition, it provides protection against selfish nodes through a secure crediting protocol and protection against malicious nodes through secure route establishment and data forwarding mechanisms. We evaluate JANUS experimentally and show that its performance is 85 percent of the optimum algorithm, improving with a factor greater than 50 percent over previous work. We evaluate the security overhead of JANUS against two types of attacks: less aggressive, but sufficient for some applications, selfish attacks and purely malicious attacks. 2009 IEEE.",,
Carbunar,"Bhatt S., Sion R., Carbunar B.","In this paper we report on our experience in building the experimental Personal Digital Rights Manager for Motorola smartphones, an industry first. Digital Rights Management allows producers or owners of digital content to control the manner in which the content is consumed. This may range from simply preventing duplication to finer access policies such as restricting who can use the content, on what devices, and for how long. In most commercial DRM systems, the average end user plays the role of content consumer, using DRM protected content made available by a service. Here we present a personal digital rights system for mobile devices where the end user has the ability to place DRM protection and controls on his or her own personal content. We designed the personal DRM system to allow users of a mobile device to transparently define controls and generate licenses on custom content and securely transfer them to other mobile devices. A user is able to define and restrict the intended audience and ensure expiration of the content as desired. Compatible devices automatically detect each other and exchange credentials. The personal DRM system on each device safely enforces the content usage rules and also handles moving licenses between devices while preventing leakage of content. We implemented a prototype of our system on Motorola E680i smartphones. 2009 Elsevier Ltd. All rights reserved.",,
Carbunar,"Carbunar B., Ramanathan M.K., Koyutôrk M., Jagannathan S., Grama A.","Recent technological advances have motivated large-scale deployment of RFID systems. However, a number of critical design issues relating to efficient detection of tags remain unresolved. In this paper, we address three important problems associated with tag detection in RFID systems: (i) accurately detecting RFID tags in the presence of reader interference (reader collision avoidance problem); (ii) eliminating redundant tag reports by multiple readers (optimal tag reporting problem); and (iii) minimizing redundant reports from multiple readers by identifying a minimal set of readers that cover all tags present in the system (optimal tag coverage problem). The underlying difficulties associated with these problems arise from the lack of collision detection mechanisms, the potential inability of RFID readers to relay packets generated by other readers, and severe resource constraints on RFID tags. In this paper we present a randomized, distributed and localized Reader Collision Avoidance (RCA) algorithm and provide detailed probabilistic analysis to establish the accuracy and the efficiency of this algorithm. Then, we prove that the optimal tag coverage problem is NP-hard even with global knowledge of reader and tag locations. We develop a distributed and localized Redundant Reader Elimination (RRE) algorithm, that efficiently identifies redundant readers and avoids redundant reporting by multiple readers. In addition to rigorous analysis of performance and accuracy, we provide results from elaborate simulations for a wide range of system parameters, demonstrating the correctness and efficiency of the proposed algorithms under various scenarios. 2008 Elsevier Inc. All rights reserved.",,
Carbunar,"Bhatt S., Carbunar B., Sion R., Vasudevan V.","Driven by an unparalleled advance in network infrastructure support as well as a boom in the number of interconnected personal communication, computation and storage devices, the modern mobile customer experience has become increasingly compelling. Traditional barriers between the roles of information consumer and producer have disappeared. Users increasingly produce and distribute valuable and often personal content such as pictures and free or purchased copyrighted media. It becomes essential to enable user-level DRM controls for content access, data integrity and rights management. In this presentation we will overview the design and implementation of a a personal digital rights management system for mobile devices. The Personal DRM Manager enables user-defined ORCON-type controls for personal content originating in a cell phone or other mobile device. Users can transparently define, generate, package and migrate content licenses between mobile devices on-demand. Networked cellular devices cooperate in the enforcement mechanisms. IFCA/Springer-Verlag Berlin Heidelberg 2007.",,
Carbunar,"Shi L., Carbunar B., Sion R.","We introduce a novel conditional e-cash protocol allowing future anonymous cashing of bank-issued e-money only upon the satisfaction of an agreed-upon public condition. Payers are able to remunerate payees for services that depend on future, yet to be determined outcomes of events. Once payment complete, any double-spending attempt by the payer will reveal its identity; no double-spending by the payee is possible. Payers can not be linked to payees or to ongoing or past transactions. The flow of cash within the system is thus both correct and anonymous. We discuss several applications of conditional e-cash including online trading of financial securities, prediction markets, and betting systems. IFCA/Springer-Verlag Berlin Heidelberg 2007.",,
Carbunar,"Carbunar B., Yu Y., Shi L., Pearce M., Vasudevan V.","Existing mechanisms for querying wireless sensor networks leak client interests to the servers performing the queries. The leaks are not only in terms of specific regions but also of client access patterns. In this paper we introduce the problem of preserving the privacy of clients querying a wireless sensor network owned by untrusted organizations. We investigate two architectures and their corresponding trust models. For the first model, consisting of multiple, mutually distrusting servers governing the network, we devise an efficient protocol, SPYC, and show that it provides full query privacy. For the second model, where all queries are performed through a single server, we introduce two metrics for quantifying the privacy achieved by a client's query sequence. We propose a suite of practical algorithms, then analyze the privacy and efficiency levels they provide. Our TOSSIM simulations show that the proposed query mechanisms are communication efficient while significantly improving client query privacy levels. 2007 IEEE.",,
Carbunar,"Carbunar B., Lindsley B., Pearce M., Vasudevan V.","Encouraging cooperation between users of mobile devices operating in ad hoc mode is a difficult task mostly because the scarce battery and bandwidth resources of devices suggest that selfish behavior may be most beneficial. The insecure usage of credits to reward cooperation can easily provide an incentive for cheating, thus, on the long term only leading to selfishness. In this paper we propose several secure credit based mechanisms enforcing fairness in a hybrid wireless content retrieval system operating both in cellular and ad hoc connectivity modes. Our solution consists of mechanisms for securely and privately discovering desired content on neighboring devices, simultaneously exchanging credit and content shares in a verifiable manner and for generating and expiring non-forgeable credits. We present experimental results of a partial prototype of our system implemented on MPx and E680i cellular phones and HP iPaq hx4700 PDAs, along with extensive simulation results showing that our solution significantly reduces the effectiveness of selfish behavior, making it an unattractive strategy. 2007 IEEE.",,
Carbunar,"Sion R., Bajaj S., Carbunar B., Katzenbeisser S.","In an outsourced data framework, we introduce and demonstrate mechanisms for securely storing a set of data items (documents) on an un-trusted server, while allowing for subsequent conjunctive keyword searches for matching documents. The protocols provide full computational privacy, query correctness assurances and no leaks: the server either correctly executes client queries or (if it behaves maliciously) is immediately detected. The client is then provided with strong assurances proving the authenticity and completeness of results. This is different from existing secure keyword search research efforts where a cooperating, non-malicious server behavior is assumed. Additionally, not only does the oblivious search protocol conceal the outsourced data (from the un-trusted server) but it also does not leak client access patterns, the queries themselves, the association between different queries or between newly added documents and their corresponding keywords (not even in encrypted form). These assurances come at the expense of additional computation costs which we analyze in the context of today's hardware. Copyright 2007 VLDB Endowment, ACM.",,
Carbunar,"Ioannidis I., Carbunar B.","As wireless ad-hoc networking approaches its maturity, an architecture that can support the massive deployment of such networks has not been established. Hybrid networks are a promising architecture that builds ad hoc, wireless networks around the existing cellular telephony infrastructure. In this paper we present a routing protocol (DST) for hybrid networks that maintains a close to optimal spanning tree of the network with the use of distributed topology trees. DST is fully dynamic and generates only O(log n) messages per update operation. We demonstrate experimentally that the performance of DST scales excellently with the network size and activity, making it ideal for the metropolitan environment hybrid networks are expected to operate in. 2004 IEEE.",,
Carbunar,"Carbunar B., Grama A., Vitek J., C?rbunar O.","In this paper we study the problem of detecting and eliminating redundancy in a sensor network with a view to improving energy efficiency, while preserving the network's coverage. We also examine the impact of redundancy elimination on the related problem of coverage-boundary detection. We reduce both problems to the computation of Voronoi diagrams, prove and achieve lower bounds on the solution of these problems, and present efficient distributed algorithms for computing and maintaining solutions in cases of sensor failures or insertion of new sensors. We prove the correctness and termination properties of our distributed algorithms, and analytically characterize the time complexity and the traffic generated by our algorithms. Our simulations show that the traffic generated per sensor insertion or removal (failure) experiences a dramatic decrease with increase in sensor density, (up to 300% when the number of sensors deployed in the same 1000 _ 1000m2 area increases from 150 to 800), and with increase in radio transmission range (up to 200% when the sensor's transmission range increases from 70m to 200m). 2004 IEEE.",,
Carbunar,"Carbunar B., Ioannidis I., Nita-Rotaru C.","In this paper we investigate and provide solutions for security threats in the context of hybrid networks consisting of a cellular base station and mobile devices equipped with dual cellular and ad-hoc (802.1 Ib) cards. The cellular connection is used for receiving services (i.e. Internet access) from the base station, while the ad-hoc links are used to improve the quality of the connection. We provide detailed descriptions of several attacks that arbitrarily powerful adversaries, whether outsiders or insiders, can mount against well-behaved members of the network. We introduce a secure routing protocol called JANUS, that focuses on the establishment of secure routes between the base station and mobile devices, and the secure routing of the data. We show that our protocol is secure against the attacks described and experimentally compare the message over-head introduced by JANUS and UCAN.",,
Carbunar,"Carbunar B., Grama A., Vitek J.","In this paper we study two important problems - coverage-boundary detection and implementing distributed hash tables in ad-hoc wireless networks. These problems frequently arise in service location and relocation in wireless networks. For the centralized coverage-boundary problem we prove a ?(n log n) lower bound for n devices. We show that both problems can be effectively reduced to the problem of computing Voronoi overlays, and maintaining these overlays dynamically. Since the computation of Voronoi diagrams requires O(n log n) time, our solution is optimal for the computation of the coverage-boundary. We present efficient distributed algorithms for computing and dynamically maintaining Voronoi overlays, and prove the stability properties for the latter - i.e., if the nodes stop moving, the overlay stabilizes to the correct Voronoi overlay. Finally, we present experimental results in the context of the two selected applications, which validate the performance of our distributed and dynamic algorithms.",,
Carbunar,"Carbunar B., Grama A., Vitek J.","Security is an important consideration for many applications of ad-hoc networks. While security aspects of the routing layer have been addressed extensively, there is relatively lesser work on establishing a viable public key infrastructure, which is the basis for most security protocols. We present a distributed algorithm for validating the association between the network identifier of a host and its public key without relying on a priori shared secrets or a trusted certification authority.",,
Carbunar,"Carbunar B., Valente M.T., Vitek J.","The choice of suitable high-level communication primitives for wide area network programming languages remains an open problem. This paper is driven by the practical consideration of providing an efficient and secure communication infrastructure for mobile agent systems. This has led us to formalise the Lime coordination middleware and propose a simplified model, which we call CoreLime, that addresses some of the main shortcomings of Lime while retaining its distinguishing feature, namely transient sharing of tuple spaces. We further discuss a prototype implementation along with security extensions. Our contribution is thus an exploration of the language design space rather than a theoretical investigation of properties of these models. 2004, Cambridge University Press. All rights reserved.Chen",,
Chen,"Sadiq S., Zmieva M., Shyu M.-L., Chen S.-C.","The evolution of information science has seen an immense growth in multimedia data, specially in the case of CCTV live stream capture. The tremendously large volumes of multimedia data give rise to a particularly challenging problem called the outlier events of interest detection. In the wake of growing school shootings in the United States, there needs to be a rethinking of our security strategies regarding the safety of children at school utilizing multimedia data mining research. This paper proposes a novel method to identify faces of interest using live stream CCTV data. By integrating the adversarial information, the proposed framework can help imbalance facial recognition and enhance rare class mining even with trivial scores from the minority class. Experimental results on the Faces in the Wile (FIW) dataset demonstrate the effectiveness of the proposed framework with promising performance. The proposed method was implemented on a low powered NVIDIA TX2 for real-time face recognition. The proposed framework was benchmarked against several existing state-of-the-art methods for accuracy, computational complexity, and real-time power measurement. The proposed method performs very well under the power and complexity constraints. 2018 IEEE.",,
Chen,"Tian H., Pouyanfar S., Chen J., Chen S.-C., Iyengar S.S.","Deep neural networks such as Convolutional Neural Networks (CNNs) have achieved several significant milestones in visual data analytics. Benefited from transfer learning, many researchers use pre-trained CNN models to accelerate the training process. However, there is still uncertainty about the deep learning models, structures, and applications. For instance, the diversity of the datasets may affect the performance of each pre-trained model. Therefore, in this paper, we proposed a new approach based on genetic algorithms to select or regenerate the best pre-trained CNN models for different visual datasets. A new genetic encoding model is presented which denotes different pre-trained models in our population. During the evolutionary process, the optimal genetic code that represents the best model is selected, or new competitive individuals are generated using the genetic operations. The experimental results illustrate the effectiveness of the proposed framework which outperforms several existing approaches in visual data classification. 2018 IEEE.",,
Chen,"Banisakher D.M., Reyes M.E.P., Allen J., Eisenberg J.D., Finlayson M.A., Price R., Chen S.-C.","Academic literature search is a vital step of every research project, especially in the face of the increasingly rapid growth of scientific knowledge. Semantic academic literature search is an approach to scientific article retrieval and ranking using concepts in an attempt to address well-known deficiencies of keyword-based search. The difficulty of semantic search, however, is that it requires significant knowledge engineering, often in the form of conceptual ontologies tailored to a particular scientific domain. It also requires non-trivial tuning, in the form of domain-specific term and concepts weights. As part of an ongoing project seeking to build a domain-specific semantic search system, we present an ontology-based supervised concept learning approach for the biogeochemical scientific literature. We first discuss the creation of a dataset of scientific articles in the biogeochemical domain annotated using the Environment Ontology (ENVO). Next we present a supervised machine learning classifier-a random decision forest-that uses a distinctive set of features to learn ENVO concepts and then label and index scientific articles at the sentence level. Finally, we evaluate our approach against two baseline methods, keyword-based and bag-of-words, achieving an overall performance of 0.76 F1 measure, an improvement of approximately 50%. 2018 IEEE.",,
Chen,"Tian H., Cen Zheng H., Chen S.-C.","Videos serve to convey complex semantic information and ease the understanding of new knowledge. However, when mixed semantic meanings from different modalities (i.e., image, video, text) are involved, it is more difficult for a computer model to detect and classify the concepts (such as flood, storm, and animals). This paper presents a multimodal deep learning framework to improve video concept classification by leveraging recent advances in transfer learning and sequential deep learning models. Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) models are then used to obtain the sequential semantics for both audio and textual models. The proposed framework is applied to a disaster-related video dataset that includes not only disaster scenes, but also the activities that took place during the disaster event. The experimental results show the effectiveness of the proposed framework. 2018 IEEE.",,
Chen,"Pouyanfar S., Tao Y., Mohan A., Tian H., Kaseb A.S., Gauen K., Dailey R., Aghajanzadeh S., Lu Y.-H., Chen S.-C., Shyu M.-L.","Many multimedia systems stream real-time visual data continuously for a wide variety of applications. These systems can produce vast amounts of data, but few studies take advantage of the versatile and real-time data. This paper presents a novel model based on the Convolutional Neural Networks (CNNs) to handle such imbalanced and heterogeneous data and successfully identifies the semantic concepts in these multimedia systems. The proposed model can discover the semantic concepts from the data with a skewed distribution using a dynamic sampling technique. The paper also presents a system that can retrieve real-time visual data from heterogeneous cameras, and the run-time environment allows the analysis programs to process the data from thousands of cameras simultaneously. The evaluation results in comparison with several state-of-the-art methods demonstrate the ability and effectiveness of the proposed model on visual data captured by public network cameras. 2018 IEEE.",,
Chen,"Yang Y., Pouyanfar S., Tian H., Chen M., Chen S.-C., Shyu M.-L.","Multimedia concept detection is a challenging topic due to the well-known class imbalance issue, where the data instances are distributed unevenly across different classes. This problem becomes even more prominent when the minority class that contains an extremely small proportion of the data represents the concept of interest as has occurred in many real-world applications such as frauds in banking transactions and goal events in soccer videos. Traditional data mining approaches often have difficulty handling largely skewed data distributions. To address this issue, in this paper, an importance-factor (IF)-based multiple correspondence analysis (MCA) framework is proposed to deal with the imbalanced datasets. Specifically, a hierarchical information gain analysis method, which is inspired by the decision tree algorithm, is presented for critical feature selection and IF assignment. Then, the derived IF is incorporated with the MCA algorithm for effective concept detection and retrieval. The comparison results in video concept detection using the disaster dataset and the soccer dataset demonstrate the effectiveness of the proposed framework. 2017 IEEE.",,
Chen,"Pan L., Pouyanfar S., Chen H., Qin J., Chen S.-C.","Deep learning has brought a series of breakthroughs in image processing. Specifically, there are significant improvements in the application of food image classification using deep learning techniques. However, very little work has been studied for the classification of food ingredients. Therefore, this paper proposes a new framework, called DeepFood which not only extracts rich and effective features from a dataset of food ingredient images using deep learning but also improves the average accuracy of multi-class classification by applying advanced machine learning techniques. First, a set of transfer learning algorithms based on Convolutional Neural Networks (CNNs) are leveraged for deep feature extraction. Then, a multi-class classification algorithm is exploited based on the performance of the classifiers on each deep feature set. The DeepFood framework is evaluated on a multi-class dataset that includes 41 classes of food ingredients and 100 images for each class. Experimental results illustrate the effectiveness of the DeepFood framework for multi-class classification of food ingredients. This model that integrates ResNet deep feature sets, Information Gain (IG) feature selection, and the SMO classifier has shown its supremacy for foodingredients recognition compared to several existing work in this area. 2017 IEEE.",,
Chen,"Tian H., Chen S.-C., Rubin S.H., Grefe W.K.","Multimedia semantic concept detection is one of the major research topics in multimedia data analysis in recent years. Disaster information management needs the assistance of multimedia data analysis to better utilize those disaster-related information, which has been widely shared by people through the Internet. In this paper, a Feature Affinity based Multiple Correspondence Analysis and Decision Fusion (FA-MCADF) framework is proposed to extract useful semantics from a disaster dataset. By utilizing the selected features and their affinities/ranks in each of the feature groups, the proposed framework is able to improve the concept detection results. Moreover, the decision fusion scheme further improves the accuracy performance. The experimental results demonstrate the effectiveness of the proposed framework and prove that the fusion of the decisions of the basic classifiers could make the framework outperform several existing approaches in the comparison. 2017 IEEE",,
Chen,"Sadiq S., Yan Y., Taylor A., Shyu M.-L., Chen S.-C., Feaster D.","The rise in popularity of social interacting websites such as Facebook, Twitter, and Snapchat has been challenged by the upsurge of unwelcomed and troubling bodies on these systems. This includes spam senders, malware systems, and other content contaminators. It is noted that highly automated accounts with 450 tweets per day produced almost 18% of entire Twitter circulation in the 2016 U.S. Presidential election. It is also observed that those disruptive systems called bots are inclined more towards circulating negative news than positive information. This paper introduces a novel framework named Associative Affinity Factor Analysis (AAFA) designed for stance detection and bot identification. Using AAFA, the proposed framework identifies real people from bots and detects the stance in bipolar affinities. The 2016 U.S. Presidential election campaign was used as a test use case because of its significant and unique counter-factual properties. The results show that our proposed AAFA framework achieves high accuracy when compared to several existing state-of-theart methods. 2017 IEEE.",,
Chen,"Eisenberg J.D., Banisakher D., Presa M., Unthank K., Finlayson M.A., Price R., Chen S.-C.","Literature search is a vital step of every research project. Semantic literature search is an approach to article retrieval and ranking using concepts rather than keywords, in an attempt to address the well-known deficiencies of keyword-based search, namely, (1) retrieval of an overwhelming number of results, (2) rankings that do not precisely reflect true relevance, and (3) the omission of relevant results because they do not contain the idiosyncratic keywords of the query. The difficulty of semantic search, however, is that it requires significant knowledge engineering, often in the form of conceptual ontologies tailored to a particular scientific domain. It also requires non-trivial tuning, in the form of domain-specific term and concepts weights. Here we present preliminary, work-in-progress results in the development of a semantic search system for the biogeochemical scientific literature. We report the following initial steps: first, one of the co-authors-a biogeochemistry expert-wrote a sample search query, and ranked the five most relevant articles that were returned for that query from a popular keyword-based search engine. We then hand annotated the five articles and the query with the Environmental Ontology (ENVO), an existing ontology for the domain. Critically, this pilot annotation revealed a number of missing concepts that we will add in future work. We then showed that a straightforward ontology distance metric between concepts in the search query and the five articles was sufficient to produce the expected ranking. We discuss the implications of these results, and outline next steps required produce a full-fledged semantic search system for the biogeochemistry scientific literature. 2017 IEEE.",,
Chen,"Rubin S.H., Grefe W.K., Bouabana-Tebibel T., Chen S.-C., Shyu M.-L., Simonsen K.S.","This paper initially describes how an inferred context-free (stochastic) grammar can be used to verify command transmissions and serve as a hedge against a successful cyber-attack. The remainder of the paper addresses a computational problem not amenable to closed-form solution; namely, the hard real-time (~57 usec) synthesis of a desired waveform through the adaptive modification of a carrier wave. This effectively increases the signal to noise ratio - ensuring better UAV communications. Here, the modulation of the primary waveform is under user control and is of strictly positive amplitude. The primary waveform induces a secondary waveform having delayed leading and trailing edges and expanded rise and fall times. There is, in general, a direct relation between the period of the primary waveform and the amplitude of the secondary waveform. The relation between the primary and secondary waveforms may be characterized by trigonometric functions or even interpolating polynomials. However, response time will be minimized where the primary waveforms are discretized and stored in the form of array-based cases. The tertiary (target) wave may be any periodic trigonometric function, but is taken to be a simple sine wave without loss of generality. The task of the adaptive program is to minimize ||s(t) - g(t)||2, where f(t) ¬ g(t) and f(t) is the primary waveform at time t, g(t) is the secondary waveform at time t, and s(t) is the tertiary waveform at time t. A computationally efficient algorithm is provided for solving this task in real time. Moreover, an evolutionary program (EP) is provided for automatic case acquisition. Primary waveforms are mutated in accordance with a normal distribution. 2017 Optical Society of America.",,
Chen,"Gauen K., Dailey R., Laiman J., Zi Y., Asokan N., Lu Y.-H., Thiruvathukal G.K., Shyu M.-L., Chen S.-C.","One of the greatest technological improvements in recent years is the rapid progress using machine learning for processing visual data. Among all factors that contribute to this development, datasets with labels play crucial roles. Several datasets are widely reused for investigating and analyzing different solutions in machine learning. Many systems, such as autonomous vehicles, rely on components using machine learning for recognizing objects. This paper compares different visual datasets and frameworks for machine learning. The comparison is both qualitative and quantitative and investigates object detection labels with respect to size, location, and contextual information. This paper also presents a new approach creating datasets using real-time, geo-tagged visual data, greatly improving the contextual information of the data. The data could be automatically labeled by cross-referencing information from other sources (such as weather). 2017 IEEE.",,
Chen,"Pouyanfar S., Chen S.-C., Shyu M.-L.","Deep learning has led to many breakthroughs in machine perception and data mining. Although there are many substantial advances of deep learning in the applications of image recognition and natural language processing, very few work has been done in video analysis and semantic event detection. Very deep inception and residual networks have yielded promising results in the 2014 and 2015 ILSVRC challenges, respectively. Now the question is whether these architectures are applicable to and computationally reasonable in a variety of multimedia datasets. To answer this question, an efficient and lightweight deep convolutional network is proposed in this paper. This network is carefully designed to decrease the depth and width of the state-of-the-art networks while maintaining the high-performance. The proposed deep network includes the traditional convolutional architecture in conjunction with residual connections and very light inception modules. Experimental results demonstrate that the proposed network not only accelerates the training procedure, but also improves the performance in different multimedia classification tasks. 2017 IEEE.",,
Chen,"Tian H., Chen S.-C.","We present a novel web-based system and a video-aided mobile application that allows emergency management personnel to access to prompt and relevant disaster situation information. The system is able to semantically integrate text-based disaster situation reports with related disaster videos taken in the field. The system is adapted to the video concept detection model and automates the procedure of file deployment and data manipulation. In addition, through an intuitive and seamless Apple iPad application, the users are able to interact with the system in various places and conditions and thus provide a more effective response. The system is demonstrated via its iPad application, which aims to provide relevant and feasible information for a disaster situation of interest. 2017 IEEE.",,
Chen,"Pouyanfar S., Chen S.-C.","As deep learning has been widespread in a wide range of applications, its training speed and convergence have become crucial. Among different hyperparameters existed in the gradient descent algorithm, the learning rate has an essential role in the learning procedure. This paper presents a new statistical algorithm for adapting the learning rate during the training process. The proposed T-LRA (trend-based learning rate annealing) algorithm is calculated based on the statistical trends seen in the previous training iterations. The proposed algorithm is computationally very cheap and applicable to online training for very deep networks and large datasets. This efficient, simple, and well-principled algorithm not only improves the deep learning results, but also speeds up the training convergence. Experimental results on a multimedia dataset and deep learning networks demonstrate the effectiveness and efficiency of the proposed algorithm. 2017 IEEE.",,
Chen,"Tian H., Chen S.-C.","This paper proposes a semantic content analysis framework for reliable video event detection. In this work, we target to improve the concept detection results by feeding the learnt results from individual shallow learning models into a generic model to dig out of the similarities in deeper layers. Compared to the deep learning models, the shallow learning models are memorizing rather than understanding the features. The proposed framework tackles the issue in shallow learning by integrating the strength of Multiple Correspondence Analysis (MCA) and Multilayer Perceptron (MLP) neural network. The low-level features are taken as the initial inputs for MCA-based models to abstract higher-level feature values. The output values further involve interaction in the neural network for better understanding. It earns the ability to put forward the arguments. The framework provides final decisions of video classifications by analyzing the decisions of every single frame from the network outputs. 2017 IEEE.",,
Chen,"Reyes M.E.P., Chen S.-C.","In this paper, we present a storm surge flooding animation system using Three-Dimensional (3D) visualization of real life Geographic Information System (GIS) data. Putting together ground elevation with building information provided by Open Street Maps (OSM), we can recreate real life cities (e.g., South Miami Beach in this paper) in a 3D environment. The 3D terrain development and visualization are done with the aid of the game engine Unity. With this tool, learning about storm surge and hurricanes can be an interactive experience. Moreover, since the system more closely portrays a real life environment, visualizing the effects of storm surge can help users study past hurricane disasters as well as possible forecasted hurricane events. For an immersive experience, we connect the system with an Integrated Computer Augmented Virtual Environment (I-CAVE) to give users the capability of navigation through the terrain in a human-scale view. 2017 IEEE.",,
Chen,"Li T., Xie N., Zeng C., Zhou W., Zheng L., Jiang Y., Yang Y., Ha H.-Y., Xue W., Huang Y., Chen S.-C., Navlakha J., Iyengar S.S.","Improving disaster management and recovery techniques is one of national priorities given the huge toll caused by man-made and nature calamities. Data-driven disaster management aims at applying advanced data collection and analysis technologies to achieve more effective and responsive disaster management, and has undergone considerable progress in the last decade. However, to the best of our knowledge, there is currently no work that both summarizes recent progress and suggests future directions for this emerging research area. To remedy this situation, we provide a systematic treatment of the recent developments in data-driven disaster management. Specifically, we first present a general overview of the requirements and system architectures of disaster management systems and then summarize state-of-the-art data-driven techniques that have been applied on improving situation awareness as well as in addressing users' information needs in disaster management. We also discuss and categorize general data-mining and machine-learning techniques in disaster management. Finally, we recommend several research directions for further investigations. 2017 ACM.",,
Chen,"Pouyanfar S., Chen S.-C.","Numerous deep learning architectures have been designed for a variety of tasks in the past few years. However, it is almost impossible for one model to work well for all kinds of scenarios and datasets. Therefore, we present an ensemble deep learning framework in this paper, which not only decreases the information loss and over-fitting problems caused by single models, but also overcomes the imbalanced data issue in multimedia big data. First, a suite of deep learning algorithms are utilized for deep feature selection. Thereafter, an enhanced ensemble algorithm is developed based on the performance of each single Support Vector Machine classifier on each deep feature set. We evaluate our proposed ensemble deep learning framework on a large and highly imbalanced video dataset containing natural disaster events. Experimental results demonstrate the effectiveness of the proposed framework for semantic event detection, and show how it outperforms several state-of-the-art deep learning architectures, as well as handcrafted features integrated with ensemble and non-ensemble algorithms. 2016 IEEE.",,
Chen,"Zhu Y., Emre Bayraktar M., Chen S.-C.","Disagreements in construction projects often result in litigation that is both time-consuming and expensive. A dispute review board (DRB) provides a valuable and proven alternative method of dispute resolution. Currently, the Florida Department of Transportation (FDOT) stores DRB reports in portable document format (PDF) with limited search capability. Improving information retrieval of DRB documents and providing a certain level of integration of DRB reports with relevant but heterogeneous data and documents is the key to enhancing the current FDOT DRB system. This paper presents a web-based data management framework to improve information management processes of the FDOT DRB system by providing key features such as metadata generation, an integrated review process, a simple issue description, member information management, and versatile information search. The new system not only allows DRB members and FDOT construction engineers to store and retrieve DRB reports but also provides more functionality to process those reports. New functionalities include a structured search based on the metadata of DRB reports, an unstructured search using advanced computer technology, and the integration of DRB reports with other related information for analysis. This type of functionality improves the efficiency and effectiveness of the DRB system.",,
Chen,"Zhu Q., Lin L., Shyu M.-L., Chen S.-C.","Content-based multimedia retrieval faces many challenges such as semantic gap, imbalanced data, and varied qualities of the media. Feature selection as a component of the retrieval process plays an important role. The aim of feature selection is to identify a subset of features by removing irrelevant or redundant features. An effective subset of features can not only improve model performance and reduce computational complexity, but also enhance semantic interpretability. To achieve these objectives, in this paper, a novel metric that integrates the correlation and reliability information between each feature and each class obtained from Multiple Correspondence Analysis (MCA) is proposed to score the features for feature selection. Based on these scores, a ranked list of features can be generated and different selection criteria can be adopted to select a subset of features. To evaluate the proposed framework, four other wellknown feature selection methods, namely information gain, chisquare measure, correlation-based feature selection, and relief are compared with the proposed method over five popular classifiers using the benchmark data from TRECVID 2009 highlevel feature extraction task. The results show that the proposed method outperforms the other methods in terms of classification accuracy, the size of feature subspace, and the ability to capture the semantic information. 2010 IEEE.",,
Chen,"Chatterjee K., Sadjadi S.M., Chen S.-C.","In this chapter, we propose a distributed multimedia data management architecture, which can efficiently store and retrieve multimedia data across several nodes of a Grid environment. The main components of the proposed system comprises of a distributed multidimensional index structure, a distributed query manager handling content-based information retrievals and a load balancing technology. The proposed distributed query manager embeds the high-level semantic relationships among the multimedia data objects into the k-NN based similarity search, thus bridging the semantic gap and increasing the relevance of query results manifold. This research has two major usabilities. First, it models a web environment where each node of the Grid can be considered as the nodes or sources of data in the world-wide-web. This should help to investigate and understand the challenges and requirements of future search paradigms based on content of multimedia data rather than on text annotations, as used currently. Second, it provides the foundation to develop content-based information retrievals as a possible Grid service. Extensive experiments were conducted with varied data sizes and different number of distribution nodes. Encouraging results are obtained that makes this endeavor a potential architecture to manage complex multimedia data over a distributed environment. Springer-Verlag Berlin Heidelberg 2010.",,
Chen,"Meng T., Lin L., Shyu M.-L., Chen S.-C.","The fast development of microscopy imaging techniques nowadays promotes the generation of a large amount of data. These data are very crucial not only for theoretical biomedical research but also for clinical usage. In order to decrease the inter-intra observer variability and save the human effort on labeling and classifying these images, a lot of research efforts have been devoted to the development of algorithms for biomedical images. Among such efforts, histology image classification is one of the most important areas due to its broad applications in pathological diagnosis such as cancer diagnosis. To improve classification accuracy, most of the previous work focuses on extracting more features and building algorithms for a specific task. This paper proposes a framework based on the novel and robust Collateral Representative Subspace Projection Modeling (C-RSPM) supervised classification model for general histology image classification. In the proposed framework, a cell image is first divided into 25 blocks to reduce the spatial complexity of computation, and one C-RSPM model is built on each block set which contains blocks in the same location from different images. For each testing image, our proposed framework first classifies each of its blocks using the C-RSPM classification model built for that block set, and then applies a multimodal late fusion algorithm with a weighted majority voting strategy to decide the final class label of the whole image. Experimenting using three-fold cross validation with three benchmark histology data sets shows that the proposed framework outperforms other well-known classifiers in the comparison and gives better results than the highest accuracy reported previously. 2010 IEEE.",,
Chen,"Liu D., Shyu M.-L., Chen C., Chen S.-C.","Key frame extraction methods aim to obtain a set of frames that can efficiently represent and summarize video contents and be reused in many video retrieval-related applications. An effective set of key frames, viewed as a high-quality summary of the video, should include the major objects and events of the video, and contain little redundancy and overlapped content. In this paper, a new key frame extraction method is presented, which not only is based on the traditional idea of clustering in the feature extraction phase but also effectively reduces redundant frames using the integration of local and global information in videos. Experimental results on the TRECVid 2007 test video dataset have demonstrated the effectiveness of our proposed key frame extraction method in terms of the compression rate and retrieval precision. 2010 IEEE.",,
Chen,"Pava J., Fleites F., Ruan F., Chatterjee K., Chen S.-C., Zhang K.","The rise of offshore water caused by the high winds of a low pressure weather system, or storm surge, is a hurricane's greatest threat to human life. As weather forecasters struggle to enable coastal residents to make timely evacuation decisions, the need arises for more visually compelling and interactive storm surge visualization tools. This paper presents an interactive and three-dimensional storm surge visualization system. It integrates road, topographic, and building data to construct accurate three-dimensional models of major cities in the State of Florida. Storm surge data are then used to construct a three-dimensional ocean positioned over the terrain models. Ambient details such as wind, vegetation, ocean waves, and traffic are animated based on up-to-date wind and storm surge data. Videos of the storm surge visualizations are recorded and made available to coastal residents through a web-interface. The three-dimensional visualization of geographic and storm surge data provides a more visually compelling representation of the potential effects of storm surge than traditional two-dimensional models and is more capable to enable coastal residents to make potentially life-saving evacuation decisions. 2010 IEEE.",,
Chen,"Zheng L., Shen C., Tang L., Li T., Luis S., Chen S.-C., Hristidis V.","Crisis Management and Disaster Recovery have gained immense importance in the wake of recent man and nature inflicted calamities. A critical problem in a crisis situation is how to efficiently discover, collect, organize, search and disseminate realtime disaster information. In this paper, we address several key problems which inhibit better information sharing and collaboration between both private and public sector participants for disaster management and recovery. We design and implement a web based prototype of a Business Continuity Information Network (BCIN) system utilizing the latest advances in data mining technologies to create a user-friendly, Internet-based, information-rich service and acting as a vital part of a company's business continuity process. Specifically, information extraction is used to integrate the input data from different sources; the content recommendation engine and the report summarization module provide users personalized and brief views of the disaster information; the community generation module develops spatial clustering techniques to help users build dynamic community in disasters. Currently, BCIN has been exercised at Miami-Dade County Emergency Management. 2010 ACM.",,
Chen,"Hamid S., Golam Kibria B.M., Gulati S., Powell M., Annane B., Cocke S., Pinelli J.-P., Gurley K., Chen S.-C.","Hurricanes threaten the Florida coast line every year and are capable of causing catastrophic losses. The Public Hurricane Loss Model was developed in response to the need for having an open transparent model to predict the above losses. The results were summarized in the paper ""Predicting Losses of Residential Structures in the State of Florida by the Public Hurricane Loss Model"" which was subsequently a subject of discussion by experts in Meteorology, Engineering, Actuarial Sciences and Statistics. The present paper presents a response to the discussions on the above mentioned article. 2010.","Alam D.M.M., He X.","High-level Petri nets (HLPNs) are a formal method for studying concurrent and distributed systems and have been widely used in many application domains. However, their strong expressive power hinders their analyzability. In this paper, we present a new transformational analysis method for analyzing a special class of HLPNs - predicate transition (PrT) nets. This method extends and improves our prior results by covering more PrT net features including full first-order logic formulas and exploring additional alternative translation schemes. This new analysis method is supported by a tool chain - front-end PIPE+ for creating and simulating PrT nets and back-end SPIN for model checking safety and liveness properties. We have applied this method to two benchmark systems used in annual Petri net model checking contest 2015. We discuss several properties and show the detailed model checking results in one system. 2017 World Scientific Publishing Company."
Chen,"Hamid S., Golam Kibria B.M., Gulati S., Powell M., Annane B., Cocke S., Pinelli J.-P., Gurley K., Chen S.-C.","As an environmental phenomenon, hurricanes cause significant property damage and loss of life in coastal areas almost every year. Although a number of commercial loss projection models have been developed to predict the property losses, only a handful of studies are available in the public domain to predict damage for hurricane prone areas. The state of Florida has developed an open, public model for the purpose of probabilistic assessment of risk to insured residential property associated with wind damage from hurricanes. The model comprises three components; viz. the atmospheric science component, the engineering component and the actuarial science component. The atmospheric component includes modeling the track and intensity life cycle of each simulated hurricane within the Florida threat area. Based on historical hurricane statistics, thousands of storms are simulated allowing determination of the wind risk for all residential Zip Code locations in Florida. The wind risk information is then provided to the engineering and actuarial components to model damage and average annual loss, respectively. The actuarial team finds the county-wise loss and the total loss for the entire state of Florida. The computer team then compiles all information from atmospheric science, engineering and actuarial components, processes all hurricane related data and completes the project. The model was submitted to the Florida Commission on Hurricane Loss Projection Methodology for approval and went through a rigorous review and was revised as per the suggestions of the commission. The final model was approved for use by the insurance companies in Florida by the commission. At every stage of the process, statistical procedures were used to model various parameters and validate the model. This paper presents a brief summary of the main components of the model (meteorology, vulnerability and actuarial) and then focuses on the statistical validation of the same. 2010 Elsevier B.V.",,
Chen,"Chatterjee K., Chen S.-C.","In this paper, we propose a multimedia data management framework using GeM-Tree. GeM-Tree is a multidimensional tree-based index structure which provides a generalized framework to organize and retrieve images and videos seamlessly. In addition to supporting different multimedia data types and diverse representations, the proposed data management framework supports varied multimedia retrieval strategies like content-based image and video retrieval, mixed multimedia data retrieval where crosssimilarity between images and videos is considered, regionbased retrieval, etc. The framework embeds a high-level semantic relationship between multimedia data objects with a construct called Hierarchical Markov Model Mediator via a novel affinity promotion technique to improve query result relevance manifold. Extensive experiments were conducted with different multimedia data types possessing varied representations, different retrieval approaches, and different data sizes. The encouraging results demonstrate a potential solution to the important and complex issue of managing a large volume of multimedia data. 2010 Academy Publisher.",,
Chen,"Chatterjee K., Chen S.-C.","This paper proposes a multidimensional distance-based index structure for video data which supports the three important video modelling approaches namely hierarchical unit-based modelling, feature-based modelling and video semantics modelling seamlessly within one single framework. These three modelling techniques collectively capture and contain the important aspects of the usersê information need during content-based video retrieval. The index is built based on the low-level features of the video data, and the hierarchical containment relationships as well as the video semantics are introduced into the index space with an efficient data signature and a stochastic model, respectively. Efficient fc-NN algorithms are proposed to emulate popular content-based video retrieval approaches in a multidimensional distance-based index structure. Extensive experimental results demonstrate the capability of the index structure to generate relevant query results with low computational overhead. 2010 Inderscience Enterprises Ltd.",,
Chen,"Chen C., Zhu Q., Liu D., Meng T., Lin L., Shyu M.-L., Yang Y., Ha H.Y., Fleites F., Chen S.-C.","This paper presents the framework and results of team Florida International University - University of Miami (FIU-UM) for the semantic indexing task of TRECVID 2010. In this task, we submitted four runs of results: « F_A_FIU-UM-1_1: KF+RERANK - apply subspace learning and classification on the key framebased low-level features (KF) and use co-occurrence probability re-ranking method (RERANK) to generate the final ranked results. « F_A_FIU-UM-2_2: LF+KF+SF+RERANK - apply subspace learning and classification on the key frame-based low-level features (KF) and shot-based low-level features (SF) separately. Then co-occurrence probability re-ranking method (RERANK) is used for both key frame based model and shot based model. Finally, a Late Fusion (LF) step combines ranking scores from each model and generates the final ranked shots. « F_A_FIU-UM-3_3: EF+KF+SF+RERANK - apply subspace learning and classification on combined features from the selected key frame-based low-level features (KF) and shot based low-level features (SF) in the Early Fusion (EF) step. Then co-occurrence probability re-ranking method (RERANK) is used. « F_A_FIU-UM-4_4: SF+RERANK - learning and classification based on shot based low-level features (SF). Then co-occurrence probability re-ranking method (RERANK) is used. From the results of different runs, it can be observed that F_A_FIU-UM-1_1 and F_A_FIU-UM-3_3 have better performance than F_A_FIU-UM-2_2 and F_A_FIU-UM-4_4. It implies that adding features from different sources could enhance the effectiveness of the learning and classification model and also visual features seem to be more reliable than audio features for most semantics in TRECVID 2010. The framework aims to handle several challenges in semantic indexing. For the challenge of data imbalance, Multiple Correspondence Analysis (MCA) based pruning method is able to reduce the high ratio between the number of negative instances and the number of positive instances. Meanwhile, for the challenge of semantic gap, the proposed subspace learning and ranking method has adopted a new way to select Principal Components (PCs), which spans a subspace where all instances are projected and classification rules are generated. The scores from one-class positive and negative learning models are further used to rank the classified instances. Then the co-occurrence probability re-ranking approach is utilized to improve the relevance of the retrieved shots. Please note that there is one run that adopts late fusion to combine the scores from key frame-based model and shot-based model. Evaluation results show that more efforts still need to be done to refine each module within our framework and some future directions to be explored are discussed in the conclusion section.",,
Chen,"Xie Z., Quirino T., Shyu M.-L., Chen S.-C., Chang L.","In this paper, a novel agent-based distributed intrusion detection system (IDS) is proposed, which integrates the desirable features provided by the distributed agent-based design methodology with the high accuracy and speed response of the Principal Component Classifier (PCC). Experimental results have shown that the PCC lightweight anomaly detection classifier outperforms other existing anomaly detection algorithms such as the KNN and LOF classifiers. In order to assess the performance of the PCC classifier on a real network environment, the Relative Assumption Model together with feature extraction techniques are used to generate normal and anomalous traffic in a LAN testbed. Finally, scalability and response performance of the proposed system are investigated through the simulation of the proposed communication architecture. The simulation results demonstrate a satisfactory linear relationship between the degradation of response performance and the scalability of the system. 2006 IEEE.",,
Chen,"Chatterjee K., Chen S.-C.","A novel indexing and access method, called Affinity Hybrid Tree (AH-Tree), is proposed to organize large image data sets efficiently and to support popular image access mechanisms like Content-Based Image Retrieval (CBIR) by embedding the high-level semantic image-relationship in the access mechanism as it is. AH-Tree combines Space-Based and Distance-Based indexing techniques to form a hybrid structure which is efficient in terms of computational overhead and fairly accurate in producing query results close to human perception. Algorithms for similarity (range and k-nearest neighbor) queries are implemented. Results from elaborate experiments are reported which depict a low computational overhead in terms of the number of I/O and distance computations and a high relevance of query results. The proposed index structure solves the existing problems of introducing high-level image relationships in a retrieval mechanism without going through the pain of translating the content-similarity measurement into feature-level equivalence and yet maintaining an efficient structure to organize the large sets of images. 2006 IEEE.",,
Chen,"Chen S.-C., Shyu M.-L., Zhang C., Chen M.","In this paper, we propose a new multimedia data mining framework for the extraction of soccer goal events in soccer videos by utilising both multimodal analysis and decision tree logic. The extracted events can be used to index the soccer videos. We first adopt an advanced video shot detection method to produce shot boundaries and some important visual features. Then, the visual/audio features are extracted for each shot at different granularities. This rich multimodal feature set is then filtered by a pre-filtering step in order to clean the noise as well as to reduce the irrelevant data. A decision tree model is built upon the cleaned data set and is used to classify the goal shots. We also present the experimental results for the proposed framework, which indicate the performance of the framework for soccer goal extraction. Copyright 2006 Inderscience Enterprises Ltd.",,
Chen,"Haruechaiyasak C., Shyu M.-L., Chen S.-C.","Traditional search engines require users to form the keyword based query which can accurately depict the search topic. More importantly, search engines are generally unable to customise the results according to the users' preferences. Recently, an alternative approach of retrieving the information, known as the recommender system is proposed. A recommender system is an intermediary program that intelligently generates a list of information which matches the users' preferences. In this paper, a new recommender system framework based on data mining techniques and the Semantic Web concept is proposed. Two information filtering methods for providing the recommended information (i.e.. content-based and collaborative filtering) are considered. Both filtering techniques are based on data mining algorithms which provide efficiency in handling large data sets. In addition, the Semantic Web concept, in which the information is given well-defined meaning, is incorporated into the framework in order to provide the users with semantically-enhanced information. To demonstrate the potential use of the proposed framework, a system prototype for recommending the University of Miami's Web pages was implemented to enhance the performance of the traditional query-based information retrieval approach provided on the website. Copyright 2006 Inderscience Enterprises Ltd.",,
Chen,"Zhao N., Chen S.-C., Shyu M.-L., Rubin S.H.","In this research, we propose an integrated and interactive framework to manage and retrieve large scale video archives. The video data are modeled by a hierarchical learning mechanism called HMMM (Hierarchical Markov Model Mediator) and indexed by an innovative semantic video database clustering strategy. The cumulated user feedbacks are reused to update the affinity relationships of the video objects as well as their initial state probabilities. Correspondingly, both the high level semantics and user perceptions are employed in the video clustering strategy. The clustered video database is capable of providing appealing multimedia experience to the users because the modeled multimedia database system can learn the user's preferences and interests interactively. 2006 IEEE.",,
Chen,"Rubin S.H., Chen S.-C., Law J.B.","The transformational methodology described in this paper induces new knowledge, which may be open under any deductive process. The method of transposition is used to maintain a maximum size for the application as well as meta-rule bases. The ""move to head"" method is used by both the application and metarule bases for hypotheses formation. Whenever an application rule is fired, it is transposed on the transposition list and also moved to the head on the other list. If any meta-rule on a solution path individually leads to a contradiction on the application rule base, then the offending meta-rule is expunged. Then, when the system is idle enter dream mode, whereby rule i ? rule j is generated by the 3-2-1 skewed twister as a candidate most-specific meta-rule. Candidate most-specific meta-rules are ""cored"" to create one generalization per candidate. These candidate meta-rules are tested for application to each rule in the application domain rule base. In order to be saved in the meta base, they may not map any existing rule in the application domain rule base to one having the same antecedent as another in this base, but a different consequent (as found by hashing). In addition, all candidate meta-rules must map at least one rule in the application base to another distinct one there, or be symmetrically induced from meta-rules that so map. 2006 IEEE.",,
Chen,"Chen S.-C., Chen M., Zhang C., Shyu M.-L.","Event detection is of great importance in high-level semantic indexing and selective browsing of video clips. However, the use of low-level visual-audio feature descriptors alone generally fails to yield satisfactory results in event identification due to the semantic gap issue. In this paper, we propose an advanced approach for exciting event detection in soccer video with the aid of multi-level descriptors and classification algorithm. Specifically, a set of algorithms are developed for efficient extraction of meaningful mid-level descriptors to bridge the semantic gap and to facilitate the comprehensive video content analysis. The data classification algorithm is then performed upon the combination of multimodal mid-level descriptors and low-level feature descriptors for event detection. The effectiveness and efficiency of the proposed framework are demonstrated over a large collection of soccer video data with different styles produced by different broadcasters. ©2006 IEEE.",,
Chen,"Shyu M.-L., Chen S.-C., Sarinnapakorn K., Chang L.","In this chapter, a novel anomaly detection scheme that uses a robust principal component classifier (PCC) to handle computer network security problems is proposed. An intrusion predictive model is constructed from the major and minor principal components of the normal instances, where the difference of an anomaly from the normal instance is the distance in the principal component space. The screening of outliers prior to the principal component analysis adds the resistance property to the classifier which makes the method applicable to both the supervised and unsupervised training data. Several experiments using the KDD Cup 1999 data were conducted and the experimental results demonstrated that our proposed PCC method is superior to the k-nearest neighbor (KNN) method, density-based local outliers (LOF) approach, and the outlier detection algorithm based on the Canberra metric. 2006 Springer-Verlag Berlin/Heidelberg.",,
Chen,"Quirino T., Xie Z., Shyu M.-L., Chen S.-C., Chang L.","In this paper, a novel supervised classification approach called Collateral Representative Subspace Projection Modeling (C-RSPM) is presented. C-RSPM facilitates schemes for collateral class modeling, class-ambiguity solving, and classification, resulting a multi-class supervised classifier with high detection rate and various operational benefits including low training and classification times and low processing power and memory requirements. In addition, C-RSPM is capable of adaptively selecting nonconsecutive principal dimensions from the statistical information of the training data set to achieve an accurate modeling of a representative subspace. Experimental results have shown that the proposed C-RSPM approach outperforms other supervised classification methods such as SIMCA, C4.5 decision tree, Decision Table (DT), Nearest Neighbor (NN), KNN, Support Vector Machine (SVM), 1-NN Best Warping Window DTW, 1-NN DTW with no Warping Window, and the well-known classifier boosting method AdaBoost with SVM. 2006 IEEE.",,
Chen,"Xie Z., Quirino T., Shyu M.-L., Chen S.-C., Chang L.","The development of effective classification techniques, particularly unsupervised classification, is important for real-world applications since information about the training data before classification is relatively unknown. In this paper, a novel unsupervised classification algorithm is proposed to meet the increasing demand in the domain of network intrusion detection. Our proposed UNPCC (Unsu-pervised Principal Component Classifier) algorithm is a multi-class unsupervised classifier with absolutely no requirements for any a priori class related data information (e.g., the number of classes and the maximum number of instances belonging to each class), and an inherently natural supervised classification scheme, both which present high detection rates and several operational advantages (e.g., lower training time, lower classification time, lower processing power requirement, and lower memory requirement). Experiments have been conducted with the KDD Cup 99 data and network traffic data simulated from our private network testbed, and the promising results demonstrate that our UNPCC algorithm outperforms several well-known supervised and unsupervised classification algorithms. 2006 IEEE.",,
Chen,"Chatterjee K., Saleem K., Zhao N., Chen M., Chen S.-C., Hamid S.S.","Hurricanes are one of the deadliest and perilous natural calamities on the face of earth having a severe impact both on the lives of the people and economy of a nation. Attempts have been made to mitigate hurricane aftermath, by utilizing research and tools that can analyze hurricanes and estimate projected losses. The need for such research methodologies and tools stimulated the development of a multi-disciplinary cutting edge public hurricane model called Public Hurricane Risk and Insured Loss Projection Model (PHRLM). The complex and diverse nature of the application raises the need for module abstraction, seamless integration and effective reusability to create a uniform generic environment. Providing efficient interaction between these complex multi-disciplinary modules using different abstractions, formalisms, data formats and communications and making each module transparent enough to be reusable becomes a complicated task. This paper presents a UML based formal Modeling Methodology, enabling component reuse and integration of the application. 2006 IEEE.",,
Chen,"Shyu M.-L., Haruechaiyasak C., Chen S.-C.","The recent increase in HyperText Transfer Protocol (HTTP) traffic on the World Wide Web (WWW) has generated an enormous amount of log records on Web server databases. Applying Web mining techniques on these server log records can discover potentially useful patterns and reveal user access behaviors on the Web site. In this paper, we propose a new approach for mining user access patterns for predicting Web page requests, which consists of two steps. First, the Minimum Reaching Distance (MRD) algorithm is applied to find the distances between the Web pages. Second, the association rule mining technique is applied to form a set of predictive rules, and the MRD information is used to prune the results from the association rule mining process. Experimental results from a real Web data set show that our approach improved the performance over the existing Markov-model approach in precision, recall, and the reduction of user browsing time. Springer-Verlag London Limited 2006.",,
Chen,"Zhang K., Yan J., Chen S.-C.","This paper presents a framework that applies a series of algorithms to automatically extract building footprints from airborne light detection and ranging (LIDAR) measurements. In the proposed framework, the ground and nonground LIDAR measurements are first separated using a progressive morphological filter. Then, building measurements are identified from nonground measurements using a region-growing algorithm based on the plane-fitting technique. Finally, raw footprints for segmented building measurements are derived by connecting boundary points, and the raw footprints are further simplified and adjusted to remove noise caused by irregularly spaced LIDAR measurements. Data sets from urbanized areas including large institutional, commercial, and small residential buildings were employed to test the proposed framework. A quantitative analysis showed that the total of omission and commission errors for extracted footprints for both institutional and residential areas was about 12%. The results demonstrated that the proposed framework identified building footprints well. 2006 IEEE.",,
Chen,"Shyu M.-L., Chen S.-C., Chen M., Zhang C., Shu C.-M.","The performance of content-based image retrieval (CBIR) systems is largely limited by the gap between the low-level features and high-level semantic concepts. In this paper, a probabilistic semantic network-based image retrieval framework using relevance feedback is proposed to bridge this gap, which not only takes into consideration the low-level image content features, but also learns high-level concepts from a set of training data, such as access frequencies and access patterns of the images. One of the distinct properties of our framework is that it exploits the structured description of visual contents as well as the relative affinity measurements among the images. Consequently, it provides the capability to bridge the gap between the low-level features and high-level concepts. Moreover, such high-level concepts can be learned off-line, and can be utilized and refined based on the user's specific interest during the on-line retrieval process. Our experimental results demonstrate that the proposed framework can effectively assist in retrieving more accurate results for user queries.",,
Chen,"Luo H., Shyu M.-L., Chen S.-C.","In this paper, we propose an optimal resource utilization scheme with end-to-end congestion control for continuous media stream transmission. This scheme can achieve minimal allocation of bandwidth for each client and maximal utilization of the client buffers. By adjusting the server transmission rate in response to client buffer occupancy, playback requirements in the client, variations in network delays and the packet loss rate, an acceptable quality-of-service (QoS) level can be maintained at the end systems. Our proposed scheme can also achieve end-to-end TCP-friendly congestion control, which the multimedia flows can share fairly with the TCP flows instead of starving the TCP flows and the packet loss rate is reduced. Simulations to compare with different approaches under different network congestion levels and to show how our approach achieves end-to-end congestion control and inter-protocol fairness are conducted. The simulation results show that our proposed scheme can generally achieve high network utilization, low losses and end-to-end congestion control. 2005 Elsevier B.V. All rights reserved.",,
Chen,"Zhao N., Chen S.-C., Shyu M.-L.","The dream of pervasive multimedia retrieval and reuse will not be realized without incorporating semantics in the multimedia database. As video data is penetrating many information systems, the need for database support for video data evolves. Hence, we propose an innovative database modeling mechanism called Hierarchical Markov Model Mediator (HMMM) which integrates lowlevel features, semantic concepts, and high-level user perceptions for modeling and indexing multiple-level video objects to facilitate temporal pattern retrieval. Different from the existing database modeling methods, our approach carries a stochastic and dynamic process in both search and similarity calculation. In the retrieval of semantic event patterns, HMMM always tries to traverse the right path and therefore it can assist in retrieving more accurate patterns quickly with lower computational costs. Moreover, HMMM supports feedbacks and learning strategies, which can proficiently assure the continuous improvements of the overall performance. 2006 IEEE.",,
Chen,"Chen S.-C., Rubin S.H., Shyu M.-L., Zhang C.",A rapid increase in the amount of image data and the inefficiency of traditional text-based image retrieval systems have served to make content-based image retrieval an active research field. It is crucial to effectively discover users' concept patterns through an acquired understanding of the subjective role played by humans in the retrieval process for such systems. A learning and retrieval framework is used to achieve this. It seamlessly incorporates multiple instance learning for relevant feedback to discover users concept patterns - especially in the region of greatest user interest. It also maps the local feature vector of that region to the high-level concept pattern. This underlying mapping can be progressively discovered through feedback and learning. The user guides the retrieval systems learning process using his/her focus of attention. Retrieval performance is tested to establish the feasibility and effectiveness of the proposed learning and retrieval framework. 2006 IEEE.,,
Chen,"Zhang K., Chen S.-C., Singh P., Saleem K., Zhao N.","A prototype 3D visualization and animation system for storm-surge flooding includes three major components: a 3D synthetic visualization environment representing terrain, buildings, roads, vegetation, and the sea; a module for animating surge flooding and waves; and a high-resolution storm-surge model to serve as an engine to drive the animation of flooding. The storm-surge model used in this study is an improvement of National Oceanic and Atmospheric Administration's Slosh model, which enables us to refine the spatial resolution of surge modeling from the kilometer to the hundred-meter scale. 2006 IEEE.Clarke",,
Clarke,"Costa F.M., Morris K.A., Kon F., Clarke P.J.","Middleware was introduced to facilitate the development of sophisticated applications based on a uniform methodology and industry standards. However, early research and practice suggested that no one-size-fits-all approach was suitable for all application domains and scenarios. This gave rise to industry initiatives to standardize domain-specific middleware services and profiles, as well as research efforts on configurable, reflective, and adaptive middleware. The industry's approach led to easy deployment, although with a level of flexibility limited by the extent of existing profiles. The approach of the research community, on the other hand, enabled high flexibility, allowing any middleware configuration to be defined. Nevertheless, creating sound configurations using this approach is a challenging task, limiting the target audience to expert engineers. As a consequence, both initiatives do not scale with the current proliferation of specialized application domains. In this paper, we target this problem with an approach that leverages model-driven engineering for the construction of domain-specific middleware platforms. A set of high-level, yet expressive, building blocks is defined in the form of a metamodel, which is used to create models that specify the desired middleware configuration. We argue that this approach enables the rapid development of middleware platforms to match the proliferation of application domains, at the same time as it does not require per-application middleware construction or even highly skilled middleware engineers. We present the current state of our research and discuss research directions to fully realize the approach. 2017 IEEE.",,
Clarke,"Clarke P.J., Davis D.L., Lau R.C., Fu Y., Kiper J.D., Walia G.S.","The impact of developing high quality software in todays society cannot be under estimated. Testing continues to be the main approach to ensure the quality of software during development. However, many academic institutions that teach computer science do not teach any software testing courses in their undergraduate or graduate curricula. In this paper we describe a cyberlearning tool (WReSTT-CyLE Web-Based Repository of Software Testing Tutorials: a Cyberlearning Environment) that helps students and instructors to learn various software testing techniques and testing tools. We also report on the results of a four year project that aimed to (1) train instructors in the use of various software testing techniques and tools, and (2) support pedagogy by using WReSTT-CyLE to expose students to several software testing techniques and testing tool tutorials. WResTT-CyLE contains learning material on software testing, and uses a combination of learning and engagement strategies (LESs) to get students more involved in the learning process. American Society for Engineering Education, 2017.",,
Clarke,"Fu Y., Clarke P.J.","Gamification is a new emerging pedagogical technique in higher education to engage students into a non-game context during the last five years. A cyber enabled learning environment can provide a collaborative and sustainable platform for learners to continue studying outside the classroom and lecturing time. In this paper, we describe the design mechanisms of gamification used in a software testing cyber enabled learning environment, named WReSTT-CyLE (Web-Based Repository of Software Testing Tutorials - a Cyberlearning Environment) and the experimental results of class studies at Alabama A&M University. As a highly integrated online learning tool, other than gamification technology, WReSTT-CyLE synthesizes several key concepts including social interaction, collaborative learning, and learning objects to support student learning of software testing concepts effectively and efficiently. This work focuses on the design architecture of gamification, assessment, and analysis of the effectiveness of WReSTT-CyLE based on our study results. American Society for Engineering Education, 2016.",,
Clarke,"Fu Y., Clarke P.J., Barnes N., Jr.","A learning object repository is a digital library developed by a group of educators and researchers to store both context and/or content while sharing, managing and reusing this resource. This idea aims at making the knowledge units interchangeable with assessment forms in a standard way so that evaluating learning outcomes and teaching strategies results in greater educational benefits. WReSTT-CyLE is a cyber-enabled virtual learning environment that provides students and educators with information on software testing, supports various types of teaching materials in the form of learning objects (LOs), and facilitates social and media networking and peer study environments. Virtual Learning Environments has become a major field of interest in recent years, with the integration of multiple social and media networking applications and has dominated in current academic learning environments and peer learning support resources. In this paper, we present a computer science course study of multiple subjects using WReSTT-CyLE to teach software-testing concepts. Software testing is considered a highlevel concept and is not widely offered in many computer science programs. WReSTT-CyLE is a learning resource that can be used by students and instructors to improve their knowledge of software testing techniques and testing tools. A study was performed at Alabama A&M University to determine the impact using WReSTT-CyLE had on students' knowledge of software testing. American Society for Engineering Education, 2016.",,
Clarke,"Allen A.A., Costa F.M., Clarke P.J.","The pervasiveness of electronic devices coupled with increasing resources within these devices has aided the explosion of available software services. While reusable software services have become more accessible and the cost of using these services has become cheaper, the increased number of services and access methods also increases the complexity for the user of these services, demanding too much attention from the user. On the other hand, automation can alleviate some of the complexity issues but at the expense of user control. We propose a balance between smart reusable integration, automation and user controllability, through a dynamically adaptive user-centric approach that utilizes goal-oriented intent modeling. This work leverages the convergence of services and providers for the user through reuse, autonomic adaptation of services to relieve the userês of laborious and tedious tasks, while ensuring that the userês end-goal remains central at all times. We applied our approach to the domains of collaborative communication as demonstration of the feasibility of the approach. 2016, Springer-Verlag London.",,
Clarke,"Allison M., Clarke P.J., He X.",The prevalent application of domain-specific modeling languages (DSMLs) requires developers to initially specify the requirements for a software product as a domain-specific model then transform that model to a high-level language for subsequent execution. An alternative is to realize behavior directly by executing the models using a specialized interpreter. One category of interpreted domain-specific modeling languages (DSMLs) derives behavior from changes to models at runtime. These are termed interpreted DSMLs or simply i-DSMLs. Existing interpreters for i-DSMLs exhibit tight coupling between the implicit model of execution (MoE) and the semantics of the domain. The interweaving of these two concerns compounds the challenge of developing interpreters for new i-DSMLs without a significant investment in resources. This paper introduces a generalized approach to developing i-DSML interpreters by utilizing a generic framework that is loosely coupled to the domain-specific knowledge as swappable framework extensions. We present a prototype as validation of our approach implemented using a metamodel based architecture to instantiate the interpreter for two distinct cyber-physical domains. 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.,,
Clarke,"Morris K.A., Clarke P.J., He X., Costa F.M., Allison M.","The direct runtime interpretation and execution of domain-specific models through the use of a Domain Specific Virtual Machine (DSVM) is an area of emerging relevance in the model-driven engineering community. This is due in part to the increased efficiency and decreased complexity achieved through specialization of the architecture in disparate domains. An approach to the design of a DSVM is to include a middleware that is responsible for the delivery and management of domain-specific services. It is the job of this middleware to help realize user intent through the execution of received commands while ensuring adherence to system policies based on changing environmental context. To provide assurance of functionality, the DSVM middleware must be policy and context-aware and facilitate variability in its operations. It achieves this variability by dynamically generating behavioral models for execution in response to commands. The dynamic generation of models poses the challenge of ensuring their correctness at runtime. To guarantee the correctness of generated models, we adopted model validation techniques to ensure policy compliance and employed the Alloy Analyzer in our prototype to demonstrate the efficacy of this approach. This granted us use of the Alloy specification language, which, by utilizing first-order logic, enhanced our model validation process by allowing more expressive policies. We demonstrate the increased capabilities and assurance realized by this approach through a case study with a DSVM middleware instance for the communication domain. 2015 IEEE.",,
Clarke,"Morris K.A., Allison M., Costa F.M., Wei J., Clarke P.J.","Context: As the use of Domain-Specific Modeling Languages (DSMLs) continues to gain popularity, we have developed new ways to execute DSML models. The most popular approach is to execute code resulting from a model-to-code transformation. An alternative approach is to directly execute these models using a semantic-rich execution engine - Domain-Specific Virtual Machine (DSVM). The DSVM includes a middleware layer responsible for the delivery of services in a given domain. Objective: We will investigate an approach that performs the dynamic combination of constructs in the middleware layer of DSVMs to support the delivery of domain-specific services. This middleware should provide: (a) a model of execution (MoE) that dynamically integrates decoupled domain-specific knowledge (DSK) for service delivery, (b) runtime adaptability based on context and available resources, and (c) the same level of operational assurance as any DSVM middleware. Method: Our approach will involve (1) defining a framework that supports the dynamic combination of MoE and DSK and (2) demonstrating the applicability of our framework in the DSVM middleware for user-centric communication. We will measure the overhead of our approach and provide a cost-benefit analysis factoring in its runtime adaptability using appropriate experimentation. Results: Our experiments show that combining the DSK and MoE for a DSVM middleware allow us to realize efficient specialization while maintaining the required operability. We also show that the overhead introduced by adaptation is not necessarily deleterious to overall performance in a domain as it may result in more efficient operation selection. Conclusion: The approach defined for the DSVM middleware allows for greater flexibility in service delivery while reducing the complexity of application development for the user. These benefits are achieved at the expense of increased execution times, however this increase may be negligible depending on the domain. 2015 Elsevier B.V. All rights reserved.",,
Clarke,"Allison M., Morris K.A., Costa F.M., Clarke P.J.","The increase in prominence of model-driven software development (MDSD) has placed emphasis on the use of domain-specific modeling languages (DSMLs) during the development process. DSMLs allow for domain concepts to be conceptualized and represented at a high level of abstraction. Currently, most DSML models are converted into high-level languages (HLLs) through a series of model-to-model and/or model-to-text transformations before they are executed. An alternative approach for model execution is the interpretation of models directly without converting them into an HLL. These models are created using interpreted DSMLs (i-DSMLs) and realized using a semantic-rich execution engine or domain-specific virtual machine (DSVM). In this article we present an approach for model synthesis, the first stage of model interpretation, that separates the domain-specific knowledge (DSK) from the model of execution (MoE). Previous work on model synthesis tightly couples the DSK and MoE reducing the ability for implementations of the DSVM to be easily reused in other domains. To illustrate how our approach to model synthesis works for i-DSMLs, we have created MGridML, an i-DSML for energy management in smart microgrids, and an MGridVM prototype, the DSVM for MGridML. We evaluated our approach by performing experiments on the model synthesis aspect of MGridVM and comparing the results to a DSVM from the user-centric communication domain. 2014 Elsevier Inc.",,
Clarke,"Clarke P.J., Davis D.L., Lau R.C., King T.M.","Although practical training in software testing tools and methodologies are vital for ensuring software quality in industry, academic course curricula do not appear to be providing students with enough hands-on experience in software testing. Furthermore, there are few research studies that discuss how different pedagogical approaches to such training are helping students to improve their testing skills. In this paper we describe how testing tools are introduced and used in an undergraduate testing course at Florida International University. As part of a semester-long course project, students access self-study tutorials on black-box and white-box testing tools via WReSTT - A Web-Based Repository of Software Testing Tutorials. We have captured the results of their experience in a case study. Our findings suggest that code coverage tools and techniques are an effective motivator for students to improve the quality of their test cases. American Society for Engineering Education, 2014.",,
Clarke,"Pestaina N., Solis T., Clarke P.J.","Undergraduate program assessment is undertaken by many colleges world-wide in support of their continuous improvement processes. In addition to assuring stakeholders of program quality, assessment is required by major regional and national accrediting agencies. A critical part of the assessment process is the generation of useful data for analysis and evaluation yielding indicators for program improvement. Senior year capstone projects are a fertile source of such data. In this paper, we outline the Student Outcomes and Senior Project course of the BS-CS program at Florida International University (FIU). We describe and evaluate a methodology used to perform assessment of attainment of the BS-CS Student Outcomes using data from the Senior Project course. American Society for Engineering Education, 2014.",,
Clarke,"Clarke P.J., Davis D., King T.M., Pava J., Jones E.L.","As software becomesmore ubiquitous and complex, the cost of software bugs continues to grow at a staggering rate. To remedy this situation, there needs to be major improvement in the knowledge and application of software validation techniques. Although there are several software validation techniques, software testing continues to be one of the most widely used in industry. The high demand for software engineers in the next decade has resulted in more software engineering (SE) courses being offered in academic institutions. However, due to the number of topics to be covered in SE courses, little or no attention is given to software testing, resulting in students entering industry with little or no testing experience. We propose a minimally disruptive approach of integrating software testing into SE courses by providing students access to a collaborative learning environment containing learning materials on testing techniques and testing tools. In this article, we describe the learning environment and the studies conducted to measure the benefits accrued by students using the learning environment in the SE courses. 2014 ACM.",,
Clarke,"Junior A.R.S., Costa F.M., Clarke P.","Cyber-Physical Systems (CPS) integrate computing, networking, and physical processes to digitally execute tasks on or using the physical elements of a system. Power microgrids are a particular kind of CPS that enables management and autonomic control of local smart grids, aiming at reliability, fault tolerance and energy efficiency, among other goals. This paper explores a new approach based on MDE that uses models at runtime techniques to manage and control microgrids. The approach employs a model execution engine that manages a causally connected runtime model of the microgrid and interprets user-defined models in order to generate controls for the microgrid elements. We demonstrate the approach by building the lower layer of the model execution engine. Furthermore, we explore a model-driven technique to build the execution engine and use the resulting experience to argue that the approach can be extended to other kinds of cyber-physical systems.",,
Clarke,"Sousa G.C.M., Costa F.M., Clarke P.J., Allen A.A.","The combination of domain-specific modeling languages and model-driven engineering techniques hold the promise of a breakthrough in the way applications are developed. By raising the level of abstraction and specializing in building blocks that are familiar in a particular domain, it has the potential to turn domain experts into application developers. Applications are developed as models, which in turn are interpreted at runtime by a specialized execution engine in order to produce the intended behavior. This approach has been successfully applied in different domains, such as communication and smart grid management to execute applications described by models that can be created and changed at runtime. However, each time the approach has to be realized in a different domain, substantial re-implementation has to take place in order to put together an execution engine for the respective DSML. In this paper, we present our work towards a generalization of the approach in the form of a metamodel which captures the domain-independent aspects of runtime model interpretation and allow the definition of domain-specific execution engines. 2012 ACM.",,
Clarke,"Morris K.A., Wei J., Clarke P.J., Costa F.M.","A developing paradigm in the area of Software Engineering is that of Model Driven Development where models are used to express operations that are thereafter interpreted and executed through the use of an execution engine. The high level of abstraction within these models present inherent challenges in guaranteeing operation that respect policies and other constraints during execution. Additionally, the domain specificity necessarily present within these execution engines make them rigid and not suited for repurposing across different domains. We propose to address these issues through the use of a middleware architecture that is responsible for the service delivery aspect of the execution engine. Our architecture will provide a separation of domain specific and domain independent concerns, resulting in a set of domain specific artifacts which possess domain knowledge, and a generalized execution platform that inherits its operations from the domain artifacts. Our design facilitates the realization of user intent through the generation, validation and execution of adaptation models at runtime constrained by policies. We show the viability of this approach in the User-Centric Communication Middleware, a layer of the Communication Virtual Machine, which is responsible for enforcing communication requirements. 2012 IEEE.",,
Clarke,"Allison M., Morris K.A., Yang Z., Clarke P.J., Costa F.M.","The dominant paradigm of centralized power generation, characterized by heavy transmission losses, is being slowly replaced by the smart micro grid, which promises the proliferation of renewable and distributed energy sources. Micro grid reliability is a well established theme as assurance requirements are inherited from the larger smart grid. In this paper we describe how user defined domain-specific micro grid models can be synthesized using runtime model analysis thereby supporting stability in the micro grid plant. This analysis includes model reconciliation which produces a list of model changes that are then interpreted to control the plant via executable control scripts. To demonstrate the efficacy and applicability of our approach, we apply it to a typical scenario in the energy management domain and prove the concept utilizing a smart micro grid prototype test bed. 2012 IEEE.",,
Clarke,"Dìlaigh K.Ó., Power J.F., Clarke P.J.","This paper presents some preliminary results from an empirical study of 12 Java applications from the Qualitas corpus. We measure the quantity and distribution of exception-handling constructs, and study their change as the systems evolve through several versions. 2012 IEEE.",,
Clarke,"Clarke P.J., Power J.F., Babich D., King T.M.","One of the characteristics of the increasingly widespread use of object-oriented libraries and the resulting intensive use of inheritance is the proliferation of dependencies on abstract classes. Since abstract classes cannot be instantiated, they cannot be tested in isolation using standard execution-based testing strategies. A standard approach to testing abstract classes is to instantiate a concrete descendant class and test the features that are inherited. This paper presents a structured approach that supports the testing of features in abstract classes, paying particular attention to ensuring that the features tested are those defined in the abstract class. Two empirical studies are performed on a suite of large Java programs and the results presented. The first study analyses the role of abstract classes from a testing perspective. The second study investigates the impact of the testing strategy on the programs in this suite to demonstrate its feasibility and to comment on the pragmatics of its use. 2010 John Wiley & Sons, Ltd.",,
Clarke,"Clarke P.J., Pava J., Davis D., Hernandez F., King T.M.","There continues to be a lack of adequate training for students in software testing techniques and tools at most academic institutions. Several educators and researchers have investigated innovative approaches that integrate testing into programming and software engineering (SE) courses with some success. The main problems are getting other educators to adopt their approaches and ensuring students continue to use the techniques they learned in previous courses. In this paper we present a study that evaluates a non-intrusive approach to integrating software testing techniques and tools in SE courses. The study uses a Web-Based Repository of Software Testing Tools (WReSTT) that contains tutorials on software testing concepts and tools. The results of the study show that (1) students who use WReSTT in the classroom can improve their understanding and use of testing techniques and tools, (2) students find WReSTT a useful learning resource, and (3) the collaborative learning environment motivates students to complete assignments. 2012 ACM.",,
Clarke,"Wu Y., Allen A.A., Hernandez F., France R., Clarke P.J.","Advances in communication devices and technologies are dramatically expanding our communication capabilities and enabling a wide range of multimedia communication applications. The current approach to develop communication-intensive applications results in products that are fragmented, inflexible, and incapable of responding to changing end-users' communication needs. These limitations have resulted in the need for a new development approach of building communication applications that are driven by end-users and that support the dynamic nature of communication-based collaboration. To address this need, the Communication Virtual Machine (CVM) technology has been developed to support rapid specification and automatic realization of user-centric communication applications based on a domain-specific modeling approach. The CVM technology consists of a domain-specific modeling language (DSML), the Communication Modeling Language (CML), that is used to create communication models, and a semantic rich platform, the CVM, that realized the created communication models. In this paper, we report on our experiences of applying a systematic approach to engineering CML and the synthesis of CML models in CVM. Based on a feature model describing the taxonomy of the user-centric communication domain in a network independent manner, we develop the meta-model of CML and its different concrete syntaxes. We also present a behavioral specification (dynamic semantics) of CML that enables the dynamic synthesis of user-centric communication models into an executable form called communication control script. We validated the CML semantics using Kermeta, a meta-programming environment for engineering DSMLs, and evaluated the practicality of our approach using a CVM prototype and a set of experiments. Copyright 2011 John Wiley & Sons, Ltd.",,
Clarke,"Hernandez F., Clarke P.J.","As domain-specific modeling languages (DSMLs) become more widely used, it is important to develop approaches for creating DSMLs that allows different aspects of the language to be incrementally added. It is important, where possible, that the new aspects (or features) of the DSML be added using an automated or semi-automated approach. By creating such an approach the developers of a DSML can start with the constructs to describe the main functionality in the domain, then add those features to the DSML necessary to specify the non-functional constraints in the domain. In this paper we present a semi-automatic approach to integrate a policy language into an existing DSML. As proof of concept we apply the approach to a simple DSML from the bookstore domain. 2011 ACM.",,
Clarke,"Allison M., Allen A.A., Yang Z., Clarke P.J.","The smart grid has been proposed as the panacea to address systemic challenges of the over fifty year old legacy electrical grid, the single largest machine on the planet. A core component central to realizing the smart grid concept is the microgrid. The microgrid is a self-sustaining entity, capable of data interchange and real-time monitoring and control of its distributed generation, storage and load components. In this paper we introduce ongoing research that uses a software engineering approach to user-driven control of the microgrid. Our approach uses a domain-specific modeling language (DSML), MGridML. and a virtual machine, MGridVM, which interprets user-defined models representing domain-level abstractions of the microgrid. MGridML captures high-level representations of pertinent domain features, based on a centralized hierarchical model of demand side energy management. A metamodel for MGridML and a prototype of the MGridVM arc presented to show the feasibility and practicality of our approach.",,
Clarke,"Zeng R., Huang Y., Liu S., Clarke P.J., He X., Van Der Linden G.W., Ebert J.L.","As embedded systems become more widespread in industry it is important to investigate new ways of maintaining these systems that are less time consuming and conform to the industry standards for quality assurance. One of the key software components in many embedded systems is the controller logic whose responsibility is the coordination of sensors, actuators, user interfaces and machine interfaces. Making changes to the controller logic without having to recompile the entire system can offer major benefits to the development process. This paper introduces a lightweight scripting language for embedded systems, SC-xScript, and the environment that supports the compilation and interpretation of Sc-xScripl programs. Using Sc-xScript, application engineers can rapidly and safely test changes to the control logic, customize a product for different customers or applications, and add extra functionality to applications, all without requiring a new release of the underlying embedded software product. The strength of SC-xScript is its small size, simplicity and portability making it suitable for embedded systems. SC-xScript combines procedural constructs with matrix driven data structures.",,
Clarke,"Wu Y., Hernandez F., Clarke P.J., France R.","Rapid advances in electronic communication devices and technologies have resulted in a shift in the way communication applications are being developed. The emerging development strategies provide end-users with a greater ability to manipulate the underlying communication technologies by providing the appropriate level of abstraction, referred to as user-centric communication. In communication-intensive domains such as telemedicine and disaster management, the user-centric communication strategies still lack the ability to coordinate the various communication services in collaborative processes. In this paper, we present a domain-specific modeling language (DSML), Workflow Communication Modeling Language (WF-CML), that supports the rapid realization of collaborative user-centric communication applications. WF-CML is an extension of CML with communication specific abstractions of workflow concepts. To realize WF-CML models the dynamic synthesis process in the Communication Virtual Machine (CVM) prototype was extended to coordinate the negotiation and media transfer processes based on events generated during the collaboration. We also present a comparative study to show the advantage of using WF-CML over a general-purpose workflow language and execution environment. 2011 IEEE.",,
Clarke,"King T.M., Allen A.A., Cruz R., Clarke P.J.","Although runtime validation and verification are critical for ensuring reliability in autonomic software, research in these areas continues to lag behind other aspects of system development. Few researchers have tackled the problem of testing autonomic software at runtime, and the current state-of-the-art only addresses localized validation of self-adaptive changes. Such approaches fall short because they cannot reveal faults which may occur at different levels of the system. In this paper, we describe an approach that enables system-wide runtime testing of behavioral adaptations in autonomic software. Our approach applies a dependency-based test order strategy at runtime to facilitate integration and system-level regression testing in autonomic software. Since validation occurs on-line during system operations, we perform testing as part of a safe approach to adaptation. To investigate the feasibility of our approach, we apply it to an autonomic communication virtual machine. 2011 Springer-Verlag.",,
Clarke,"King T.M., Allen A.A., Wu Y., Clarke P.J., Ramirez A.E.","A survey on the landscape of self-adaptive systems identified testing and assurance as one of the most neglected areas in the engineering of autonomic software. However, since the structure and behavior of autonomic software can vary during its execution, runtime testing is critical to ensure that faults are not introduced into the system as a result of dynamic adaptation. Some researchers have developed approaches and supporting designs for integrating runtime testing into the workflow of autonomic software. In this paper, we describe a comparative case study performed on three autonomic applications that were engineered to include an implicit self-test characteristic. The findings of our study provide evidentiary insight into the benefits and software engineering challenges associated with developing these types of systems. 2011 IEEE.",,
Clarke,"Babich D., Clarke P.J., Power J.F., Kibria B.M.G.","In this paper, we propose an innovative suite of metrics based on a class abstraction that uses a taxonomy for OO classes (CAT) to capture aspects of software complexity through combinations of class characteristics. We empirically validate their ability to predict fault prone classes using fault data for six versions of the Java-based open-source Eclipse Integrated Development Environment. We conclude that this proposed CAT metric suite, even though it treats classes in groups rather than individually, is as effective as the traditional Chidamber and Kemerer metrics in identifying fault-prone classes. 2011 ACM.","Prathima E.G., Iyengar S.S., Venugopal K.R., Patnaik L.M.",Multiple events may occur simultaneously in heterogeneous sensor networks. It is necessary to aggregate event specific data. Event-Driven Data Aggregation (EDDA) is designed to efficiently gather aggregated data and route event specific data from the location of an event to the base station. The sensor nodes that have detected an event organize into an aggregation tree from the location of event to the base station. The nodes that do not detect the event relay the data to the base station. Such nodes integrate the data from different events into a single packet by prefixing a tag indicating the event type. The performance analysis shows that the EDDA algorithm incurs less energy consumption than existing state-of-the-art algorithms. 2017 IEEE.
Clarke,"Seidl M., Clarke P.J.","The Educators' Symposium (EduSymp) yields a major forum for software modeling education. Traditionally collocated with the ACM/IEEE International Conference on Model-Driven Engineering Languages and Systems (MODELS), EduSymp offers a unique opportunity for educators to present and discuss innovative pedagogical software modeling approaches. In this paper, a short retrospective on the 6th edition of EduSymp hosted in Oslo is presented. The program was a manifold of activities including interesting and thought-provoking oral presentations, an interactive breakout-session, and a panel discussion. 2011 Springer-Verlag Berlin Heidelberg.",,
Clarke,"Roach J., Chen L.-W., Clarke P., Dikshit A., Rotella F.M.","This work addresses the device modeling challenges of production-quality, state-of-the-art, silicon-on-sapphire (SOS) processes. Differences between SOS, silicon-on-insulator (SOI), and bulk CMOS are highlighted, with emphasis on the key differences in the modeling methodology. For RF and low-power applications, SOS has distinct advantages over SOI, such as reduced parasitics, better linearity, and enhanced electrical isolation. Yet little is reported in the literature about modeling of a commercially viable SOS process. Though originally developed for SOI, it is demonstrated that the BSIMSOI model can adequately represent SOS MOSFETs, including fully and partially depleted devices. For RF switch applications, RONand COFFare captured with reasonable accuracy. An additional RF figure of merit, f t, is also reasonably wellmodeled, yielding peak values in the 4050 GHz range. 2011 IEEE.",,
Clarke,"Bryant B.R., Gray J., Mernik M., Clarke P.J., France R.B., Karsai G.","Developing software from models is a growing practice and there exist many model-based tools (e.g., editors, interpreters, debuggers, and simulators) for supporting model-driven engineering. Even though these tools facilitate the automation of software engineering tasks and activities, such tools are typically engineered manually. However, many of these tools have a common semantic foundation centered around an underlying modeling language, which would make it possible to automate their development if the modeling language specification were formalized. Even though there has been much work in formalizing programming languages, with many successful tools constructed using such formalisms, there has been little work in formalizing modeling languages for the purpose of automation. This paper discusses possible semantics-based approaches for the formalization of modeling languages and describes how this formalism may be used to automate the construction of modeling tools.",,
Clarke,"Clarke P.J., Pava J., Wu Y., King T.M.","One of the main concerns in the software industry continues to be the development of high quality software. This concern will be exacerbated as software systems become more complex. The training of software developers continues to grow in academia since more institutions are offering software engineering (SE) courses. However, the list of topics that are expected to be covered in this course leaves little or no time for topics that focus on developing quality software, such as software testing and the use of testing tools. In this paper we describe an approach that non-intrusively integrates the use of software testing tools in SEcourses. The cornerstone of our approach is the interaction students have with a Web-Based Repository of Software Testing Tools (WReSTT) that contains tutorials on testing concepts and testing tools. WReSTT employs both collaborative learning and social networking features that are attractive to students. We present the results of preliminary study performed in two SEcourses that show how using the resources in WReSTT can potentially impact the students' understanding of software testing and the use of testing tools.",,
Clarke,"Seidl M., Clarke P.","Model-driven engineering (MDE) is a promising paradigm to deal with the ever increasing complexity of modern software systems. Its powerful abstraction mechanisms allow developers to focus on the essential challenges hiding away irrelevant aspects of the system under development. Within the last few years, noticable progress has been made in putting the vision of MDE into practice, where the activity of textual coding is substituted by modeling. With matured concepts and stable tools available, MDE becomes more and more ready to be applied in software engineering projects. Nevertheless, the best available technology is worthless, if it is not accepted and used by the developers. Also in MDE profound training is needed to fully exploit its power. In this paper, we discuss the efforts taken in educational environments to promote the application of modeling and MDE technologies for the software development process and discuss several challenges which still have to be faced. Software Modeling in Education 2011.",,
Clarke,"Clarke P.J., Allen A.A., King T.M., Jones E.L., Natesan P.","Improving the quality of software developed in the 21st century is one of the major challenges in the software industry. Addressing this problem will require that academic institutions play a key role in training developers to produce high quality software. Unfortunately, students and instructors continue to be frustrated by the lack of support provided when selecting appropriate testing tools and program analyzers to verify programs under development. In this paper we present an approach that integrates the use of software testing tools into programming and software engineering courses. The approach consists of three phases, developing an online repository with learning resources, training instructors in the area of testing techniques and tools, and integrating the use of testing tools into various programming courses. We also present the results of the first instructors' workshop and studies on integrating testing tools into two courses, CS2 and Software Engineering (SE). 2010 ACM.",,
Clarke,"Wu Y., Hernandez F., Ortega F., Clarke P.J., France R.","The use of domain-specific modeling languages (DSMLs) results in higher productivity during the development process. This is accomplished by raising the level of abstraction during design and focusing on domain concepts rather than low-level implementation details. Unlike other development paradigms, little work has been done in determining and measuring the claimed benefits of using DSMLs. In this paper, we propose a new approach to determine the effort involved in creating and using DSML models to develop applications and to manage the behavior of applications at runtime. The approach involves a classification of the effort involved, and definition of relevant metrics to measure the effort for each category. A case study is presented that shows how we applied the proposed metrics during the development and execution of an application using three different DSMLs. Copyright 2010 ACM.",,
Clarke,"Roach J., Chen L.-W., Clarke P., Rotella F.M.","The BSIMSOI model largely dominates the modeling of silicon-on-insulator (SOI) MOSFET technologies. Silicon-on-sapphire (SOS) technology has many of the advantages of SOI for RF and low-power applications, but with enhanced electrical isolation and heat dissipation, among others. We show that BSIMSOI can reasonably describe state-of-the-art SOS devices as well, including partial and full depletion, as long as differences between SOS and SOI technologies are accounted for in the parameter extraction methodology. For RF switch applications, RON and COFF are adequately represented. Also, a spot check at low currents shows that a modeled RF figure of merit, FT, is not unreasonable. 2010 IEEE.",,
Clarke,"Morales J.A., Clarke P.J., Deng Y.",This paper presents an approach to detecting known and unknown file infecting viruses based on their attempt to replicate. The approach does not require any prior knowledge about previously discovered viruses. Detection is accomplished at runtime by monitoring currently executing processes attempting to replicate. Replication is the fundamental characteristic of a virus and is consistently present in all viruses making this approach applicable to viruses belonging to many classes and executing under several conditions. An implementation prototype of our detection approach called SRRAT is created and tested on the Microsoft Windows operating systems focusing on the tracking of user mode Win32 API system calls and Kernel mode system services. 2008 Springer-Verlag France.,,
Clarke,"Clarke P., Muneer T., Cullinane K.","This paper presents an analysis of vehicle regenerative braking systems as a quick and relatively easy means of achieving higher overall fuel efficiency and lowering carbon emissions. The system involves the installation of an additional electric motor/generator in parallel to the vehicle's internal combustion engine and is used in conjunction with a DCDC converter and ultracapacitor. The system is used to recapture the energy lost in vehicle braking, significantly reducing a vehicle's overall energy consumption and lowering vehicle emissions. Experimentally-based evidence is collected and compared for two sample vehicles to deduce the potential fuel and emissions saving. 2010 Elsevier Ltd. All rights reserved.",,
Clarke,"Muneer T., Clarke P., Cullinane K.","In an effort to explore the potential reduction of local and global air pollution by using more sustainable forms of urban transport, Napier University presents this work on the results of using an electric scooter that is charged with the University's own solar photovoltaic facility. In this study, an electrical scooter urban driving cycle is compared with that of a conventional petrol automobile. The energy consumed for the scooter is 0.056kWh/km, respectively equating to monetary and environmental costs of 0.56 pence and 33.3g CO2 per km. In contrast, the CO2 emissions from a medium-sized automobile that was chosen for this study were found to be 208g/km IMechE, 2009.",,
Clarke,"Allen A.A., Wu Y., Clarke P.J., King T.M., Deng Y.","The diversity of communication media now available on IP networks presents opportunities to create elaborate collaborative communication applications. However, developing collaborative communication applications can be challenging when using the traditional stovepiped development approach with lengthy development cycle as well as limited utility. One proposed solution to this problem is the Communication Virtual Machine (CVM). CVM uses a user-centric communication (UCC) approach to reduce the complexity while offering operating simplicity to developers and users of collaborative communication services. CVM currently utilizes only one communication framework which limits the number, quality and types of services available. We extend the CVM to support multiple communication frameworks with a policy-driven approach for the selection and configuration of communication services. Users define policies that, through automation, can maintain high level goals by influencing the selection and configuration decisions. In this paper we provide a policy definition for UCC and a technique to evaluate these UCC policies. We also present our autonomic framework for UCC and experimental evaluation of the implementation. Copyright 2009 Andrew A. Allen, Yali Wu, Peter Clarke, Tariq M. King and Yi Deng.",,
Clarke,"Milani M., Sadjadi S.M., Rangaswami R., Clarke P.J., Li T.","According to Computing Research Association, during each year between 2003 and 2007, fewer than 3% of the US's Ph.D.s graduates in computer science and computer engineering were Hispanic or African American and fewer than 20% were women. Such an under-representation precludes the benefits of diversity in computer sciences research and industry and consequently compromises the competitiveness of the US economy. It is therefore imperative that undergraduate institutions introduce students from these groups to research at an early stage of their academic careers and to provide them with the tools necessary for success in graduate school. The School of Computing and Information Sciences (SCIS) at Florida International University (FIU) has been working to strengthen the pipeline of underrepresented students to graduate work in computer science by hosting an NSF sponsored Research Experiences for Undergraduates (REU) site for the past three years. Our REU site has hosted 30 undergraduate students, 23 of them were underrepresented including 8 females, 16 Hispanics, and 4 African Americans, who published 13 technical papers. Six of the ten students who have already graduated, have started their graduate studies. Copyright 2009 ACM.",,
Clarke,"Wang Y., Wu Y., Allen A., Espinoza B., Clarke P.J., Deng Y.",The pervasiveness of complex communication services and the need for end-users to play a greater role in developing communication services have resulted in the creation of the Communication Virtual Machine (CVM) technology. The CVM technology consists of a Communication Modeling Language (CML) and the CVM. CML is a declarative modeling language that can be used to specify domain-specific communication services and the CVM is the platform used to realize the CML models. In this paper we explicitly define the operational semantics of CML to support (1) the synthesis of CML models into executable control scripts and (2) the handling of negotiation and media transfer events during communication. We specify the semantics of CML using label transition systems and describe in detail an algorithm that is essential for the interpretation of CML models. A case study is presented showing how the semantics support the rapid realization of a scenario from the healthcare domain. 2009 IEEE.,,
Clarke,"Ding J., Clarke P.J., Argote-Garcia G., He X.","High level Petri nets have been extensively used for modeling concurrent systems; however, their strong expressive power reduces their ability to be easily analyzed. Currently there are few effective formal analysis techniques to support the validation of high level Petri nets. The executable nature of high level Petri nets means that during validation they can be analyzed using test criteria defined on the net model. Recently, theoretical test adequacy coverage criteria for concurrent systems using high level Petri nets have been proposed. However, determining the applicability of these test adequacy criteria has not yet been undertaken. In this paper, we present an approach for evaluating the proposed test adequacy criteria for high level Petri nets through experimentation. In our experiments we use the simulation functionality of the model checker SPIN to analyze various test coverage criteria on high level Petri nets. 2009 Elsevier B.V. All rights reserved.",,
Clarke,"Celik A.N., Muneer T., Clarke P.","This article analyses the energy statistics of 15 European Union countries (EU-15), giving special emphasis to the installed solar photovoltaic and thermal collector capacity. The installed capacities per capita are analysed in relation to the solar radiation income of respective countries with the view to explore the relationship between the solar income and its utilisation as of the year 2006. In terms of the installed solar thermal collector capacity, Austria leads the statistics amongst the countries studied with 223Wth collector capacity per capita, followed by Greece with 207Wth. Except for Greece, it is observed that the countries with high solar radiation income are lacking to realise their solar potential. Regarding the installed photovoltaic power per capita, Luxembourg leads the pack by a wide margin with 47Wp capacity, followed by Germany with 30Wp. Fiscal instruments to invigorate the deployment of solar energy have also been identified in this work. 2008 Elsevier Ltd. All rights reserved.",,
Clarke,"Clarke P.J., Babich D., King T.M., Golam Kibria B.M.","The transition from Java 1.4 to Java 1.5 has provided the programmer with more flexibility due to the inclusion of several new language constructs, such as parameterized types. This transition is expected to increase the number of class clusters exhibiting different combinations of class characteristics. In this paper we investigate how the number and distribution of clusters are expected to change during this transition. We present the results of an empirical study were we analyzed applications written in both Java 1.4 and 1.5. In addition, we show how the variability of the combinations of class characteristics may affect the testing of class members. 2008 Elsevier Inc. All rights reserved.",,
Clarke,"Ding J., Argote-Garcia G., Clarke P.J., He X.","How to ensure the quality of complex software systems is a grand challenge. Formal methods and software testing techniques are two major complementary approaches for software quality assurance. In this paper, we present a unique approach that uses the simulation capability of the Spin model checker to evaluate the test adequacy of high level Petri nets based on various coverage criteria. In our approach, a high level Petri net is expressed as a program in Promela, the input language for Spin, and the simulation capability of Spin is used to execute the program and evaluate different test coverage criteria for high level Petri nets. We use high level Petri nets as a concrete formal specification method to demonstrate our approach, and our results can be easily generalized to other formal models as well. Copyright 2008 ACM.",,
Clarke,"Hernandez Y., King T.M., Pava J., Clarke P.J.","As businesses strive to keep pace with the rapid evolution of web technologies, their efforts to maintain automated regression testing strategies are being hindered. Technological migration of a web application can lead to test scripts becoming incapable of validating the migrated application due to differences in the testing platform. Regression tests that are still applicable to the application would therefore have to be re-written to be compatible with the new technologies. In this paper, we apply a model-driven approach to the development of automated testing scripts for validating web applications from the client-side. We define a meta-model using UML 2.0 profiles, and describe the model transformations needed to automatically port regression tests to various platforms. A prototype of the test implementation for an e-commerce application is presented to demonstrate the feasibility of the approach.",,
Clarke,"King T.M., Ramirez A., Clarke P.J., Quinones-Morales B.","As the enabling technologies of autonomic computing continue to advance, it is imperative for researchers to exchange the details of their proposed techniques for designing, developing, and validating autonomic systems. Many of the software engineering issues related to building dependable autonomic systems can only be revealed by studying detailed designs and prototype implementations. In this paper we present a reusable object-oriented design for developing self-testable autonomic software. Our design aims to reduce the effort required to develop autonomic systems that are capable of runtime testing. Furthermore, we provide lowlevel implementation details of a case study, Autonomic Job Scheduler (AJS), developed using the proposed design. Copyright 2008 ACM.",,
Clarke,"Argote-Garcia G., Clarke P.J., He X., Fu Y., Shi L.","Quality assurance is recognized as a critical aspect in software construction. SAM is a formal software architecture description model that combines Petri Nets and Temporal Logic. PROMELA is the language used in the Spin model checker. This paper presents an approach to translate a restricted SAM model to a PROMELA program, enabling the model checking of the SAM model. We define the translation and show its correctness in terms of completeness and consistency. Completeness establishes that a SAM model maps all of its elements to PROMELA ones; whereas, consistency defines that an execution of a SAM model has a corresponding execution in a PROMELA program. The translation is also implemented as part of our software tool supporting SAM. Some aspects of the tool are discussed.",,
Clarke,"Wu Y., Allen A.A., Hernandez F., Wang Y., Clarke P.J.","The advances in communication frameworks, such as Skype and Google Talk facilitate the increasing needs of communication-intensive and collaborative applications. These communication frameworks also make it possible for end-users to be more involved in the development of such applications if the appropriate level of abstraction can be provided. In this paper, we propose the design of a user-centric communication middleware (UCM) that supports raising the level of abstraction appropriate for end-users to create and realize models using the communication virtual machine (CVM) technology. The CVM technology consists of the communication modeling language (CML) and CVM, and supports the rapid conception, construction and realization of new communication services using a model-driven approach. The UCM is a layer in CVM that provides operating simplicity to the end-user by masking the underlying technology. We present the design goals of UCM, high-level architecture, a description of the runtime environment and a case study showing how the communication needs of a medical scenario is realized in the UCM.He",,
Finlayson,"Banisakher D.M., Reyes M.E.P., Allen J., Eisenberg J.D., Finlayson M.A., Price R., Chen S.-C.","Academic literature search is a vital step of every research project, especially in the face of the increasingly rapid growth of scientific knowledge. Semantic academic literature search is an approach to scientific article retrieval and ranking using concepts in an attempt to address well-known deficiencies of keyword-based search. The difficulty of semantic search, however, is that it requires significant knowledge engineering, often in the form of conceptual ontologies tailored to a particular scientific domain. It also requires non-trivial tuning, in the form of domain-specific term and concepts weights. As part of an ongoing project seeking to build a domain-specific semantic search system, we present an ontology-based supervised concept learning approach for the biogeochemical scientific literature. We first discuss the creation of a dataset of scientific articles in the biogeochemical domain annotated using the Environment Ontology (ENVO). Next we present a supervised machine learning classifier-a random decision forest-that uses a distinctive set of features to learn ENVO concepts and then label and index scientific articles at the sentence level. Finally, we evaluate our approach against two baseline methods, keyword-based and bag-of-words, achieving an overall performance of 0.76 F1 measure, an improvement of approximately 50%. 2018 IEEE.",,
Finlayson,"Himmelstein P., Barb S., Finlayson M.A., Young K.D.","BACKGROUND: Major depressive disorder (MDD) is characterized by biases in memory, attention, and cognition. The present study utilized the Linguistic Inquiry and Word Count (LIWC) to examine the content of specific autobiographical memories (AMs) recalled by individuals with MDD during an autobiographical memory task. METHODS: We examined various features of the text (including use of affective, cognitive, and self-referential terms), as well as their associations with clinical and cognitive features of MDD (depression severity, autobiographical memory specificity, amygdala activity), in 45 unmedicated adults with MDD compared to 61 healthy controls. RESULTS: When recalling positive memories MDD individuals used the word ""I"" less, fewer positive words, more words indicating present focus (present tense verbs), and fewer words overall to describe memories compared to controls. When recalling negative memories, MDD individuals used ""I"" more, more words indicating present focus, and more words overall to describe memories relative to controls. Depression severity was correlated with word count, the use of ""I"", and words indicating present focus in negative memories and inversely correlated with word count and the use of ""I"" in positive memories. Autobiographical memory specificity was correlated with word count, the use of ""I"", and words indicating present focus for positive memories and inversely correlated with the use of ""I"" and words indicating present focus for negative memories. LIMITATIONS: Due to the nature of AM recall, we could not control for the number of memories which participants recalled in each mnemonic category. CONCLUSIONS: Results align with literature implicating rumination and intensive self-focus in depression and suggest that interventions targeting specific word use may be therapeutically beneficial in the treatment of MDD.",,
Finlayson,"Eisenberg J.D., Banisakher D., Presa M., Unthank K., Finlayson M.A., Price R., Chen S.-C.","Literature search is a vital step of every research project. Semantic literature search is an approach to article retrieval and ranking using concepts rather than keywords, in an attempt to address the well-known deficiencies of keyword-based search, namely, (1) retrieval of an overwhelming number of results, (2) rankings that do not precisely reflect true relevance, and (3) the omission of relevant results because they do not contain the idiosyncratic keywords of the query. The difficulty of semantic search, however, is that it requires significant knowledge engineering, often in the form of conceptual ontologies tailored to a particular scientific domain. It also requires non-trivial tuning, in the form of domain-specific term and concepts weights. Here we present preliminary, work-in-progress results in the development of a semantic search system for the biogeochemical scientific literature. We report the following initial steps: first, one of the co-authors-a biogeochemistry expert-wrote a sample search query, and ranked the five most relevant articles that were returned for that query from a popular keyword-based search engine. We then hand annotated the five articles and the query with the Environmental Ontology (ENVO), an existing ontology for the domain. Critically, this pilot annotation revealed a number of missing concepts that we will add in future work. We then showed that a straightforward ontology distance metric between concepts in the search query and the five articles was sufficient to produce the expected ranking. We discuss the implications of these results, and outline next steps required produce a full-fledged semantic search system for the biogeochemistry scientific literature. 2017 IEEE.",,
Finlayson,"Mealier A.-L., Pointeau G., Mirliaz S., Ogawa K., Finlayson M., Dominey P.F.","It has been proposed that starting from meaning that the child derives directly from shared experience with others, adult narrative enriches this meaning and its structure, providing causal links between unseen intentional states and actions. This would require a means for representing meaning from experience-a situation model-and a mechanism that allows information to be extracted from sentences and mapped onto the situation model that has been derived from experience, thus enriching that representation. We present a hypothesis and theory concerning how the language processing infrastructure for grammatical constructions can naturally be extended to narrative constructions to provide a mechanism for using language to enrich meaning derived from physical experience. Toward this aim, the grammatical construction models are augmented with additional structures for representing relations between events across sentences. Simulation results demonstrate proof of concept for how the narrative construction model supports multiple successive levels of meaning creation which allows the system to learn about the intentionality of mental states, and argument substitution which allows extensions to metaphorical language and analogical problem solving. Cross-linguistic validity of the system is demonstrated in Japanese. The narrative construction model is then integrated into the cognitive system of a humanoid robot that provides the memory systems and world-interaction required for representing meaning in a situation model. In this context proof of concept is demonstrated for how the system enriches meaning in the situation model that has been directly derived from experience. In terms of links to empirical data, the model predicts strong usage based effects: that is, that the narrative constructions used by children will be highly correlated with those that they experience. It also relies on the notion of narrative or discourse function words. Both of these are validated in the experimental literature. 2017 Mealier, Pointeau, Mirliaz, Ogawa, Finlayson and Dominey.",,
Finlayson,Finlayson M.A.,"I describe the collection and deep annotation of the semantics of a corpus of Russian folktales. This corpus, which I call the 'ProppLearner' corpus, was assembled to provide data for an algorithm designed to learn Vladimir Propp's morphology of Russian hero tales. The corpus is the most deeply annotated narrative corpus available at this time. The algorithm and learning results are described elsewhere; here, I provide detail on the layers of annotation and how they were chosen, novel layers of annotation required for successful learning, the selection of the texts for annotation, the annotation process itself, and the resulting inter-annotator agreement measures. In particular, the corpus comprised fifteen texts totaling 18,862 words. There were eighteen layers of annotation, five of which were developed specifically to support learning Propp's morphology: referent attributes, context relationships, event valences, Propp's 'dramatis personae', and Propp's functions. All annotations were created by trained annotators with the Story Workbench annotation tool, following a double-annotation paradigm. I discuss lessons learned from this effort and what they mean for future digital humanities efforts when working with the semantics of natural language text. The Author 2016. Published by Oxford University Press on behalf of EADH. All rights reserved.",,
Finlayson,"Dominey P.F., Mealier A.-L., Pointeau G., Mirliaz S., Finlayson M.","Dynamic construction grammar (DCG) is a neurcomputational framework for learning and generalizing sentence-to-meaning mappings. It is inspired by the cue competition hypothesis of Bates and MacWhinney, and learns regularities in the ordering of open and closed class words and the corresponding mapping to semantic roles for the open class words. The structure of meaning is a crucial aspect of these form to meaning mappings. Here we describe the DCG framework, and the evolution of meaning representations that have been addressed. The first and most basic meaning representation is a predicate-argument form indicating the predicate, agent, object and recipient. We developed an action recognition system, that detected simple actions and used naïve subjects' narration to train the model to understand. The DCG comprehension model was then extended to address sentence production. The resulting models were then integrated into a cooperative humanoid robotic platform. We then demonstrated how observed actions could be construed from different perspectives, and used the production model to generate corresponding sentences. In order to allow the system to represent and create meaning beyond the single sentence, we introduce the notions of narrative construction and narrative function word. In the same way that grammatical function words operate on relations between open class elements in the sentence, narrative function words operate on events across multiple sentences in a narrative. This motivates the need for an intermediate representation of meaning in the form of a situation model that represents multiple events and relations between their constituents. In this context we can now begin to address how narrative can enrich perceived meaning as suggested by Bruner. Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,
Finlayson,Finlayson M.A.,"Vladimir Propp's Morphology of the Folktale is a seminal work in folkloristics and a compelling subject of computational study. I demonstrate a technique for learning Propp's functions from semantically annotated text. Fifteen folktales from Propp's corpus were annotated for semantic roles, co-reference, temporal structure, event sentiment, and dramatis personae. I derived a set of merge rules from descriptions given by Propp. These rules, when coupled with a modified version of the model merging learning framework, reproduce Propp's functions well. Three important function groups-namely A/a (villainy/lack), H/I (struggle and victory), and W (reward)-are identified with high accuracies. This is the first demonstration of a computational system learning a real theory of narrative structure. ©2016 by the Board of Trustees of the University of Illinois.",,
Finlayson,"Yarlott W.V.H., Finlayson M.A.","We give a preliminary description of ProppML, an annotation scheme designed to capture all the components of a Proppian-style morphological analysis of narratives. This work represents the first fully complete annotation scheme for Proppian morphologies, going beyond previous annotation schemes such as PftML, ProppOnto, Bod et al., and our own prior work. Using ProppML we have annotated Propp's morphology on fifteen tales (18,862 words) drawn from his original corpus of Russian folktales. This is a significantly larger set of data than annotated in previous studies. This pilot corpus was constructed via double annotation by two highly trained annotators, whose annotations were then combined after discussion with a third highly trained adjudicator, resulting in gold standard data which is appropriate for training machine learning algorithms. Agreement measures calculated between both annotators show very good agreement (F1 &gt; 0.75, ? &gt; 0.9 for functions; F1 &gt; 0.6 for moves; and F1 &gt; 0.8, ? &gt; 0.6 for dramatis personae). This is the first robust demonstration of reliable annotation of Propp's system. W. Victor H. Yarlott and Mark A. Finlayson.",,
Finlayson,"Yarlott W.V.H., Finlayson M.A.","Motifs are distinctive recurring elements found in folklore, and are used by folklorists to categorize and find tales across cultures and track the genetic relationships of tales over time. Motifs have significance beyond folklore as communicative devices found in news, literature, press releases, and propaganda that concisely imply a large constellation of culturally-relevant information. Until now, folklorists have only extracted motifs from narratives manually, and the conceptual structure of motifs has not been formally laid out. In this short paper we propose that it is possible to automate the extraction of both existing and new motifs from narratives using supervised learning techniques and thereby possible to learn a computational model of how folklorists determine motifs. Automatic extraction would enable the construction of a truly comprehensive motif index, which does not yet exist, as well as the automatic detection of motifs in cultural materials, opening up a new world of narrative information for analysis by anyone interested in narrative and culture. We outline an experimental design, and report on our efforts to produce a structured form of Thompson's motif index, as well as a development annotation of motifs in a small collection of Russian folklore. We propose several initial computational, supervised approaches, and describe several possible metrics of success. We describe lessons learned and difficulties encountered so far, and outline our plan going forward. W. Victor H. Yarlott and Mark A. Finlayson.",,
Finlayson,"Eisenberg J.D., Yarlott W.V.H., Finlayson M.A.","Having access to a large set of stories is a necessary first step for robust and wide-ranging computational narrative modeling; happily, language data-including stories-are increasingly available in electronic form. Unhappily, the process of automatically separating stories from other forms of written discourse is not straightforward, and has resulted in a data collection bottleneck. Therefore researchers have sought to develop reliable, robust automatic algorithms for identifying story text mixed with other non-story text. In this paper we report on the reimplementation and experimental comparison of the two approaches to this task: Gordon's unigram classifier, and Corman's semantic triplet classifier. We cross-analyze their performance on both Gordon's and Corman's corpora, and discuss similarities, differences, and gaps in the performance of these classifiers, and point the way forward to improving their approaches. Joshua D. Eisenberg, W. Victor H. Yarlott, and Mark A. Finlayson.",,
Finlayson,Finlayson M.A.,"Java is a popular programming language for natural language processing. I compare and evaluate 12 Java libraries designed to access the information in the original Princeton Wordnet databases. From this comparison emerges a set of decision criteria that will enable a user to pick the library most suited to their purposes. I identify five deciding features: (1) availability of similarity metrics; (2) support for editing; (3) availability via Maven; (4) compatibility with retired Java versions; and (5) support for Enterprise Java. I also provide a comparison of other features of each library, the information exposed by each API, and the versions of Wordnet each library supports, and I evaluate each library for the speed of various retrieval operations. In the case that the user's application does not require one of the deciding features, I show that my library, JWI, the MIT Java Wordnet Interface, is the highest-performance, widest-coverage, easiest-to-use library available.",,
Finlayson,"Finlayson M.A., Halverson J.R., Corman S.R.","We describe the N2 (Narrative Networks) Corpus, a new language resource. The corpus is unique in three important ways. First, every text in the corpus is a story, which is in contrast to other language resources that may contain stories or story-like texts, but are not specifically curated to contain only stories. Second, the unifying theme of the corpus is material relevant to Islamist Extremists, having been produced by or often referenced by them. Third, every text in the corpus has been annotated for 14 layers of syntax and semantics, including: referring expressions and co-reference; events, time expressions, and temporal relationships; semantic roles; and word senses. In cases where analyzers were not available to do high-quality automatic annotations, layers were manually double-annotated and adjudicated by trained annotators. The corpus comprises 100 texts and 42, 480 words. Most of the texts were originally in Arabic but all are provided in English translation. We explain the motivation for constructing the corpus, the process for selecting the texts, the detailed contents of the corpus itself, the rationale behind the choice of annotation layers, and the annotation procedure.",,
Finlayson,Finlayson M.A.,"I apply Barwise and Seligman's theory of information flow to understand how sets of signals can carry information. More precisely I focus on the case where the information of interest is not present in any individual signal, but rather is carried by correlations between signals. This focus has the virtue of highlighting an oft-neglected process, viz., the different methods that apply categories to raw signals. Different methods result in different information, and the set of available methods provides a way of characterizing relative degrees of intensionality. I illustrate my points with the case of folktales and how they transmit cultural information. Certain sorts of cultural information, such as a grammar of hero stories, are not found in any individual tale but rather in a set of tales. Taken together, these considerations lead to some comments regarding the ""information unit"" of narratives and other complex signals. 2012 Springer-Verlag.",,
Finlayson,Finlayson M.A.,"Text annotations are of great use to researchers in the language sciences, and much effort has been invested in creating annotated corpora for an wide variety of purposes. Unfortunately, software support for these corpora tends to be quite limited: it is usually ad-hoc, poorly designed and documented, or not released for public use. I describe an annotation tool, the StoryWorkbench, which provides a generic platform for text annotation. It is free, open-source, cross-platform, and user friendly. It provides a number of common text annotation operations, including representations (e.g., tokens, sentences, parts of speech), functions (e.g., generation of initial annotations by algorithm, checking annotation validity by rule, fully manual manipulation of annotations) and tools (e.g., distributing texts to annotators via version control, merging doubly-annotated texts into a single file). The tool is extensible at many different levels, admitting new representations, algorithm, and tools. I enumerate ten important features and illustrate how they support the annotation process at three levels: (1) annotation of individual texts by a single annotator, (2) double-annotation of texts by two annotators and an adjudicator, and (3) annotation scheme development. The Story Workbench is scheduled for public release in March 2012. 2011, Association for the Advancement of Artificial.",,
Finlayson,Finlayson M.A.,"Annotated corpora have stimulated great advances in the language sciences. The time is ripe to bring that same stimulation, and consequent benefits, to computational approaches to narrative. I describe an effort to construct a corpus of semantically annotated stories. I outline the structure of the corpus, a structure which colloquially can be described as a ""handful of handfuls."" One handful of the corpus has already been constructed, viz., 18k words of Russian folktales. There are two handfuls under construction: legal cases focused on the area of probable cause, and stories from Islamist Extremist Jihadists. Four more handfuls are being planned: folktales from Chinese, English, and a West Asian culture, and stories of international conventional and cyber conflicts. There are numerous additional handfuls under discussion. The main focus of the corpus so far has been on textual materials that are annotated for their surface semantics using conventional annotation tools and techniques; nonetheless, there are numerous novel dimensions along which the corpus might grow and become useful for different communities. In particular I propose for discussion the outlines of a few novel sources, annotation schemes, and collection methodologies that could potentially make the corpus of great use to the interactive narrative or narrative generation communities. 2011, Association for the Advancement of Artificial.",,
Finlayson,"Azevedo R., Biswas G., Bohus D., Carmichael T., Finlayson M.A., Hadzikadic M., Havasi C., Horvitz E., Kanda T., Koyejo O., Lawless W.F., Lenat D., Meneguzzi F., Mutlu B., Oh J., Pirrone R., Raux A., Sofge D.A., Sukthankar G., Van Durme B., Yorke-Smith N.","The Association for the Advancement of Artificial Intelligence (AAAI) presented the 2010 Fall Symposium Series on November 11-13, 2010. The eight symposia included Cognitive and Metacognitive Educational Systems, Commonsense Knowledge, Complex Adaptive Systems: Resilience, Robustness, and Evolvability, Computational Models of Narrative, Dialog with Robots, Manifold Learning and Its Applications, Proactive Assistant Agents and Quantum Informatics for Cognitive, Social, and Semantic Processes. Cognitive and Metacognitive Educational Systems aimed to provide a comprehensive definition of metacognitive educational systems that is inclusive of the theoretical, architectural, and educational aspects of this field. The AAAI Commonsense Knowledge Fall Symposium had the goal of bringing together the diverse elements of this community whose work benefits from or contributes to the representation of general knowledge about the world. One of the specific goals of Proactive symposium was to gather the researchers from various projects in assistant agents to share their wisdom in retrospect.",,
Finlayson,"Hervás R., Finlayson M.A.","Generating referring expressions is a key step in Natural Language Generation. Researchers have focused almost exclusively on generating distinctive referring expressions, that is, referring expressions that uniquely identify their intended referent. While undoubtedly one of their most important functions, referring expressions can be more than distinctive. In particular, descriptive referring expressions - those that provide additional information not required for distinction - are critical to fluent, efficient, well-written text. We present a corpus analysis in which approximately one-fifth of 7,207 referring expressions in 24,422 words of news and narrative are descriptive. These data show that if we are ever to fully master natural language generation, especially for the genres of news and narrative, researchers will need to devote more attention to understanding how to generate descriptive, and not just distinctive, referring expressions. 2010 Association for Computational Linguistics.",,
Finlayson,Finlayson M.A.,"Analogical reasoning is crucial to robust and flexible high-level cognition. However, progress on computational models of analogy has been impeded by our inability to quickly and accurately collect large numbers (100+) of semantically annotated texts. The Story Workbench is a tool that facilitates such annotation by using natural language processing techniques to make a guess at the annotation, followed by approval, correction, and elaboration of that guess by a human annotator. Central to this approach is the use of a sophisticated graphical user interface that can guide even an untrained annotator through the annotation process. I describe five desiderata that govern the design of the Story Workbench, and demonstrate how each principle was fulfilled in the current implementation. The Story Workbench enables numerous experiments that previously were prohibitively laborious, of which I describe three currently underway in my lab. Copyright 2008, Association for the Advancement of Artificial Intelligence. All rights reserved.",,
Finlayson,"Hastings J.T., Zhang F., Finlayson M.A., Goodberlet J.G., Smith H.I.","Spatial-phase-locked electron-beam lithography (SPLEBL) developed to improve the pattern-placement accuracy of scanning-electron-beam lithography (SEBL) tools is presented.This mode of SPLEBL provides two dimensional sub-beam-step pattern placement, enen with the extremely poor signal-to-noise ratio (SNR). It is expected that the improvements to scintillator and light collection system will increase SNR and pattern-placement accuracy.",,
He,"He X., Dong Z., Yin H., Fu Y.","Cyber-physical systems (CPSs) are pervasive in our daily life from mobile phones to auto-driving cars. CPSs are inherently complex due to their sophisticated behaviors and thus difficult to build. In this paper, we propose a framework to develop CPSs based on a model-driven approach with quality assurance throughout the development process. An agent-oriented approach is used to model individual physical and computation processes using high-level Petri nets, and an aspect-oriented approach is used to integrate individual models. The Petri net models are systematically mapped to classes and threads in Java, which are enhanced and extended with domain-specific functionalities. Complementary quality assurance techniques are applied throughout system development and deployment, including simulation and model checking of design models, model checking of Java code, and runtime verification of Java executable. We demonstrate our framework using a car parking system. 2017 World Scientific Publishing Company.",,
He,He X.,"Android permission framework is a part of Android OS to enforce secure cross application communication. However the android permission framework is very complex, and its descriptions are scattered in dozens of webpages. It is very difficult to understand the relationships among multiple permission levels and their potential vulnerabilities. This paper presents a formal model of the Android permission framework using high level Petri nets. The model precisely defines the complex relationships among different levels of permissions. The model is constructed incrementally and thus is easily adaptable to future changes. The model building process is supported by our tool environment PIPE+, which further provides several analysis techniques. Simulation results for several scenarios that obey and violate the permission requirements are discussed. 2017 IEEE.",,
He,"Alam D.M.M., He X.","High level Petri nets (HLPNs) are a formal method for studying concurrent and distributed systems and have been widely used in many application domains. However, their strong expressive power hinds their analyzability. In this paper, we present a new transformational analysis method for analyzing a special class of HLPNs - predicate transition (PrT) nets. This method extends and improves our prior results by covering more PrT net features including full first order logic formulas and exploring additional alternative translation schemes. This new analysis method is supported by a tool chain - front end PIPE+ for creating and simulating PrT nets and back end SPIN for model checking safety and liveness properties. We have applied this method to two benchmark systems used in annual Petri net model checking contest 2015. We discuss several properties and show the detailed model checking results of two properties in one system.",,
He,"He X., Yin H., Dong Z., Fu Y.","Cyber physical systems (CPSs) are pervasive in our daily life from mobile phones to auto driving cars. CPSs are inherently complex due to their sophisticated behaviors and thus difficult to build. In this paper, we propose a framework to develop CPSs based on a model driven approach with quality assurance throughout the development process. An agent-oriented approach is used to model individual physical and computation processes using high level Petri nets, and an aspect-oriented approach is used to integrate individual models. The Petri net models are systematically mapped to classes and threads in Java, which are enhanced and extended with domain specific functionalities. Complementary quality assurance techniques are applied throughout system development and deployment, including simulation and model checking of design models, model checking of Java code, and run-Time verification of Java executable. We demonstrate our framework using a car parking system.",,
He,"He X., Zeng R., Liu S., Sun Z., Bae K.","High level Petri nets (HLPNs) have been widely applied to model concurrent and distributed systems in computer science and many other engineering disciplines. However, due to the expressive power of HLPNs, they are difficult to analyze. In recent years, a variety of new analysis techniques based on model checking have been proposed to analyze high level Petri nets in addition to the traditional analysis techniques such as simulation and reachability (coverability) tree. These new analysis techniques include (1) developing tailored model checkers for particular types of HLPNs or (2) leveraging existing general model checkers through model translation where a HLPN is transformed into an equivalent form suitable for the target model checker. In this paper, we present a term rewriting approach to analyze a particular type of HLPNs-predicate transition nets (PrT nets). Our approach is completely automatic and implemented in our tool environment, where the frontend is PIPE+, a general graphical editor for creating PrT net models, and the backend is Maude, a well-known term rewriting system. We have applied our approach to the Mondex system-the 1st pilot project of verified software repository in the worldwide software verification grand challenge, and several well-known problems used in the annual model checking contest of Petri net tools. Our initial experimental results are encouraging and demonstrate the usefulness of the approach. 2016 IEEE.",,
He,"He X., Fu Y.","Security has become an essential and critical nonfunctional requirement of modern software systems, especially cyber physical systems. Security patterns aim at capturing security expertise in the worked solutions to recurring security design problems. This paper presents an approach to formally model and analyze six security patterns to detect potential incompleteness, inconsistency, and ambiguity in the textual descriptions; and to prevent their incorrect implementation. These patterns are modeled using high level Petri nets in our tool environment PIPE+. Simulation is used to analyze various security relevant properties. The validated formal models of individual security patterns serve as the building blocks for system design involving the composition of multiple security patterns.",,
He,"Pereyra J., He X., Mostafavi A.","This study presents a novel multi-agent system (MAS) framework for conceptualizing, modeling, and analyzing interdependent critical infrastructure (ICI) as complex adaptive systems (CAS) to enable resilience analysis. Unlike conventional approaches that consider interdependent infrastructure as networks of physical assets, the MAS framework conceptualizes interdependent infrastructure as interacting networks of social and physical entities and processes at different levels of hierarchical organization. The interaction between physical and social entities is captured at different levels of interaction proposed in System-of-System taxonomy and modified for modeling critical infrastructure systems and social agents. The proposed MAS framework is capable of abstraction, realization, and implementation of interdependent critical infrastructure systems as complex adaptive systems. The implementation of the proposed MAS framework is conducted through a nested high-level Petri-net method capable of: (1) abstraction of dynamic behaviors and interactions between social actors and physical networks, and (2) multi-level aggregation of dependencies in modeling critical infrastructure systems. The contributions of the proposed framework are threefold: (1) abstraction of dynamic behaviors and interactions between social and technical entities; (2) multi-level aggregation and implementation of interdependent critical infrastructure (ICI) networks as complex and layered systems; and (3) formalized modeling and simulation of infrastructure's multi-agent systems (MAS) using high-level petri net. ASCE.",,
He,"Chang L., He X.","This paper presents a methodology for analyzing multi-agent systems modeled in nested predicate transition nets. The objective is to automate the model analysis for complex systems, and provide a foundation for tool development. We formally define the translation rules that translate the multi-agent model to an executable PROMELA model, and demonstrate the translation with an example. 2015 World Scientific Publishing Company.","Delamarre A., Buche C., Polceanu M., Lunn S., Ruiz G., Bolivar S., Shernoff E., Lisetti C.","Interactive Virtual Training (IVT) is a web-based interactive simulation system created to help early career teachers working in high-poverty schools learn effective classroom management skills. IVT simulates disruptive students' behavior (aggressive, off-task, non-compliant), as defined in vignettes developed by pedagogical experts on our team. These simulations enable early career teachers to hone their classroom management skills before they start teaching in real classrooms. In the simulation, users will interact with thirty (30) realistic diverse virtual three-dimensional (3D) ""practice"" students, which are situated in two (2) realistic virtual 3D classrooms. We describe IVT: (1) requirements and process mechanisms, (2) the IVT simulation of the vignettes depicting students' behaviors, (3) 3D virtual classrooms and characters, (4) website main functionalities, and (5) contributions and challenges. Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
He,"Liu S., He X.","High level Petri nets (HLPNs) have been widely used to model complex systems; however, their high expressive power costs their analyzability. Model checking techniques have been exploited in analyzing high level Petri nets, but have limited success due to either undecidability problem or state explosion problem. Bounded model checking (BMC) is a promising analysis method that explores state space within a predefined bound. BMC sacrifices the completeness of traditional model checking but becomes more practical and often effective to analyze large models. In our prior work, we have developed a method based on BMC and a supporting tool PIPE+Verifier to analyze high level Petri nets using a state of the art satisfiability modulo theories (SMT) solver Z3 as the backend engine. Our experiment results have been very encouraging. In this paper, we present the design, implementation, and use of PIPE+Verifier, as well as show additional improvements to make PIPE+Verifier more efficient.",,
He,"Zeng R., Sun Z., Liu S., He X.","Atomicity violations are the most common non-deadlock concurrency bugs, which have been extensively studied in recent years. Since detecting the actual occurrences of atomicity violations is extremely hard and exhaustive testing of a multi-threaded program is in general impossible, many predictive methods have been proposed, which make error predictions based on a small number of instrumented interleaved executions. Predictive methods often make tradeoffs between precision and coverage. An over-approximate predictive method ensures coverage but lacks precision and thus may report a large number of false bugs. An underapproximate predictive method ensures precision but lacks coverage and thus can miss significant real bugs. This paper presents a post-prediction analysis method for improving the precision of the prediction results obtained through over-approximation while achieving better coverage than that obtained through under-approximation. Our method analyzes and filters the prediction results of over-approximation by evaluating a subset of read-after-write relationships without enforcing all of them as in existing under-approximation methods. Our post-prediction method is a static analysis method on the predicted traces from dynamic instrumentation of C/C++ executable, and is faster than dynamic replaying methods for ensuring precision. Springer-Verlag Berlin Heidelberg 2015.",,
He,"Xiao F., Min H., Bo X., He X.","Service-oriented computing has emerged as a new software development paradigm that enables implementation of web accessible software systems composed of distributed services interacting with each other via exchanging messages. Modeling and analysis of interactions among services is a crucial problem in this paradigm. Many formal methods including process algebras have been used to address functional modeling and verification of web service composition; however they often ignore the modeling and analysis of the cost of the service composition. In this paper, we propose priced probabilistic process algebra (PPPA) by extending existing probabilistic process algebra with cost modeling and analysis capability. We show how to use PPPA to model service composition and how to control the cost of service composition using Markov decision process with target function of optimal cost.",,
He,"Allison M., Clarke P.J., He X.",The prevalent application of domain-specific modeling languages (DSMLs) requires developers to initially specify the requirements for a software product as a domain-specific model then transform that model to a high-level language for subsequent execution. An alternative is to realize behavior directly by executing the models using a specialized interpreter. One category of interpreted domain-specific modeling languages (DSMLs) derives behavior from changes to models at runtime. These are termed interpreted DSMLs or simply i-DSMLs. Existing interpreters for i-DSMLs exhibit tight coupling between the implicit model of execution (MoE) and the semantics of the domain. The interweaving of these two concerns compounds the challenge of developing interpreters for new i-DSMLs without a significant investment in resources. This paper introduces a generalized approach to developing i-DSML interpreters by utilizing a generic framework that is loosely coupled to the domain-specific knowledge as swappable framework extensions. We present a prototype as validation of our approach implemented using a metamodel based architecture to instantiate the interpreter for two distinct cyber-physical domains. 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.,,
He,"Morris K.A., Clarke P.J., He X., Costa F.M., Allison M.","The direct runtime interpretation and execution of domain-specific models through the use of a Domain Specific Virtual Machine (DSVM) is an area of emerging relevance in the model-driven engineering community. This is due in part to the increased efficiency and decreased complexity achieved through specialization of the architecture in disparate domains. An approach to the design of a DSVM is to include a middleware that is responsible for the delivery and management of domain-specific services. It is the job of this middleware to help realize user intent through the execution of received commands while ensuring adherence to system policies based on changing environmental context. To provide assurance of functionality, the DSVM middleware must be policy and context-aware and facilitate variability in its operations. It achieves this variability by dynamically generating behavioral models for execution in response to commands. The dynamic generation of models poses the challenge of ensuring their correctness at runtime. To guarantee the correctness of generated models, we adopted model validation techniques to ensure policy compliance and employed the Alloy Analyzer in our prototype to demonstrate the efficacy of this approach. This granted us use of the Alloy specification language, which, by utilizing first-order logic, enhanced our model validation process by allowing more expressive policies. We demonstrate the increased capabilities and assurance realized by this approach through a case study with a DSVM middleware instance for the communication domain. 2015 IEEE.",,
He,"Liu S., Zeng R., Sun Z., He X.","High level Petri nets (HLPNs) have been widely applied to model concurrent and distributed systems in computer science and many other engineering disciplines. However, due to the expressive power of HLPNs, they are more difficult to analyze. Exhaustive analysis methods such as traditional model checking based on fixed point calculation of state space may not work for HLPNs due to the state explosion problem. Bounded model checking (BMC) using satisfiability solvers is a promising analysis method that can handle a much larger state space than traditional model checking method. In this paper, we present an analysis method for HLPNs by leveraging the BMC technique with a state-of-theart satisfiability modulo theories (SMT) solver Z3. A HLPN model and some safety properties are translated into a first order logic formula that is checked by Z3. This analysis method has been implemented in a tool called PIPE+Verifier and is completely automatic. We show our results of applying PIPE+Verifier to several models from the Model Checking Contest @ Petri Nets and a few other sources. Springer International Publishing Switzerland 2014.",,
He,He X.,"Petri nets, a formal model for concurrent and distributed systems, have been widely applied in system modeling and analysis in almost every branch of computer science and many other scientific and engineering disciplines in the past half century. In this comprehensive survey, we review some major developments of Petri nets that have enhanced their modeling capabilities and in particular the methods to incorporate well-known software engineering development paradigms in Petri nets to support general software system modeling. World Scientific Publishing Company.",,
He,"Chang L., He X., Shatz S.M.","In the past two decades, multi-agent systems have emerged as a new paradigm for conceptualizing large and complex distributed software systems. Even though there are many conceptual frameworks for using multi-agent systems, there is no well established and widely accepted method for the representation of multi-agent systems. We adapt a well-known formal model, predicate transition nets, to include the notions of dynamic structure, agent communication and coordination to address the representation problems. This paper presents a comprehensive methodology for modeling multi-agents based on the extensions. We demonstrate our modeling approach with an example. Several case studies on different application domains from our previous works are also discussed. 2012 World Scientific Publishing Company.",,
He,"Zeng R., Sun Z., Liu S., He X.","Multi-thread programs are prone to bugs due to concurrency. Concurrency bugs are hard to find and reproduce because of the large number of interleavings. Most non-deadlock concurrency bugs are atomicity violation bugs due to unprotected accesses of shared variables by multiple threads. This paper presents a dynamic prediction tool named McPatom for predicting atomicity violation bugs involving a pair of threads accessing a shared variable using model checking. McPatom uses model checking to ensure the completeness in predicting any possible atomicity violation captured in the abstract thread model extracted from an interleaved execution. McPatom can predict atomicity violations involving more than three accesses and multiple subroutines, and supports all synchronization primitives. We have applied McPatom in predicting several known bugs in real world systems including one that evades several other existing tools. We provide evaluations of McPatom in terms of atomicity violation predictability and performance with additional improvement strategies. 2012 Springer-Verlag Berlin Heidelberg.",,
He,"Liu S., Zeng R., He X.","Petri nets are a formal, graphical and executable modeling technique for the specification and analysis of concurrent systems and have been widely applied in computer science and many other engineering disciplines. Low level Petri nets are simple and useful for modeling control flows; however, they are not powerful to define data and system functionality. High level Petri nets were developed to support data and functionality definitions [1]. To support the practical applications of Petri nets formalism, tools for designing and executing Petri nets are necessary. Although there are many existing tools for supporting low level Petri nets [5], few tools are available for high level Petri nets. There is especially a lack of tools to support high level Petri net notation proposed in the international standard [1]. In this paper, we present a tool, called PIPE+, to support a subset of high level Petri nets proposed in [1]. PIPE+ is built upon an existing low level Petri net tool PIPE (Platform Independent Petri Net Editor) [2]. This paper describes the functionality of PIPE+ as well as illustrates the process of extending PIPE, which provides helpful insights for others to create Petri net tools suit their own needs. Furthermore, PIPE+ is an open source tool and thus is available for various enhancements from worldwide research community.",,
He,"Zeng R., Huang Y., Liu S., Clarke P.J., He X., Van Der Linden G.W., Ebert J.L.","As embedded systems become more widespread in industry it is important to investigate new ways of maintaining these systems that are less time consuming and conform to the industry standards for quality assurance. One of the key software components in many embedded systems is the controller logic whose responsibility is the coordination of sensors, actuators, user interfaces and machine interfaces. Making changes to the controller logic without having to recompile the entire system can offer major benefits to the development process. This paper introduces a lightweight scripting language for embedded systems, SC-xScript, and the environment that supports the compilation and interpretation of Sc-xScripl programs. Using Sc-xScript, application engineers can rapidly and safely test changes to the control logic, customize a product for different customers or applications, and add extra functionality to applications, all without requiring a new release of the underlying embedded software product. The strength of SC-xScript is its small size, simplicity and portability making it suitable for embedded systems. SC-xScript combines procedural constructs with matrix driven data structures.",,
He,"Zeng R., He X., Van Der Aalst W.M.P.","Scientific workflows have recently emerged as a new paradigm for representing and managing complex distributed scientific computations and are used to accelerate the pace of scientific discovery. In many disciplines, individual workflows are large and complicated due to the large quantities of data used. As such, the workflow construction is difficult or even impossible when relevant domain knowledge is missing or the workflows require collaboration within multiple domains. Recent efforts from scientific workflow community aiming at largescale capturing of provenance present a new opportunity for using provenance to provide recommendations during building scientific workflows. This paper presents a method based on provenance to mine models for scientific workflows, including data and control dependency. The mining result can either suggest part of others' workflows for consideration, or make familiar part of workflow easily accessible, thus provide recommendation support for scientific workflow composition. 2011 IEEE.",,
He,"Chang L., He X.","In our previous work, we developed a nested Petri net framework for modeling multi-agent systems. In this paper, we present a method to analyze the nested Petri net model using model checking. Our method systematically translates a nested Petri net model into a PROMELA program in SPIN. 2011 ACM.",,
He,"Chang L., He X.","In this paper, we apply a multi-agent modeling paradigm based on predicate transition nets (PrT nets) to model a Business Continuity Information Network (BCIN) for disaster mitigation. A two-level nested PrT net is defined to address a multi-agent architecture. We extend the semantic definitions of PrT nets and give the algorithms for modeling the interactions between nets at different level. The resulting BCIN net is translated to Process Meta Language (PROMELA) and run under SPIN. The objective of this work is to provide a dynamic model to study the interdependences between distributed entities and resources in disaster mitigation.",,
He,"Zeng R., He X.","Mondex, an electronic purse, is the first pilot project of the software verification Grand Challenge to establish the correctness of software. Several research groups around the world have applied different formal methods in specifying and analyzing the Mondex since 2006. In this paper, we present a method to analyze the Sam specification of Mondex using model checking. Our specification uses Sam that integrates high level Petri nets and temporal logic. Our analysis method translates the Sam Mondex specification into a behavior preserving Promela program and uses Spin to model check the resulting Promela program. Our results and experiences are discussed, which contributes to the world wide effort in developing a verified software repository. 2010 Springer-Verlag.",,
He,"Ding J., He X.","A mobile agent system is a special distributed system with moving programs in networks. Mobile agent systems provide a powerful and flexible paradigm for building high performance distributed systems. Due to dynamic configuration property, assuring quality of a mobile agent system is a challenge work. Formal specification and analysis of a mobile agent system provides one of the best approaches to ensure the correctness of a system design. However, it is difficult to find a formal specification tool for modeling a mobile agent system with an easy to understand and concise model. In addition, it is a challenge but also important work to provide an automatic formal analysis approach for verifying whether a system specification correctly meets certain requirements in a mobile agent system. In this paper, a framework for specification and analysis of mobile agent systems is defined. First, Predicate/Transition nets are extended with dynamic channels for modeling mobile agent systems. The formalism has the expressive power to naturally model the software architecture of a mobile agent system, and easily capture the properties especially the mobility, mobile communication and dynamic configuration of a mobile agent system. Then, model checking is instrumented to the framework for automatically verifying the correctness of the specification of a mobile agent system. In order to illustrate the capability of the formalism and the verification strategy, a medical image processing system using mobile agents is modeled using the extended Predicate/Transition nets and system properties are verified using the SPIN model checker. World Scientific Publishing Company.",,
He,"Chang L., He X.","In this paper, aspect-oriented concept is incorporated into predicate transition nets to model agents based on BDI structure, which describes the mental attitudes of autonomous agents. Our modeling approach not only explicitly models the BDI structure to bridge the gap between agent theory and agent design, but also modularizes the BDI agent model into various aspects to enhance the adaptability and reusability of agent models.",,
He,"Ding J., Clarke P.J., Argote-Garcia G., He X.","High level Petri nets have been extensively used for modeling concurrent systems; however, their strong expressive power reduces their ability to be easily analyzed. Currently there are few effective formal analysis techniques to support the validation of high level Petri nets. The executable nature of high level Petri nets means that during validation they can be analyzed using test criteria defined on the net model. Recently, theoretical test adequacy coverage criteria for concurrent systems using high level Petri nets have been proposed. However, determining the applicability of these test adequacy criteria has not yet been undertaken. In this paper, we present an approach for evaluating the proposed test adequacy criteria for high level Petri nets through experimentation. In our experiments we use the simulation functionality of the model checker SPIN to analyze various test coverage criteria on high level Petri nets. 2009 Elsevier B.V. All rights reserved.",,
He,"Lian J., Shatz S.M., He X.","One approach to modeling multi-agent systems (MASs) is to employ a method that defines components which describe the local behavior of individual agents, as well as a special component, called a coordinator. The coordinator component coordinates the resource sharing behavior among the agents. The agent models define a set of local plans, and the combination of local plans and a coordinator defines a system's global plan. Although earlier work has provided the base functionality needed to synthesize inter-agent resource sharing behavior for a global, conflict-free MAS environment, the lack of coordination flexibility limits the modeling capability at both the local plan level and the global plan level. In this paper, we describe a flexible design method that supports a range of coordinator components. The method defines four levels of coordination and an associated four-step coordinator generation process, which allows for the design of coordinators with increasing capabilities for handling complexity associated with resource coordination. Colored Petri net based simulation is used to analyze various properties that derive from different coordinators and synthesis of a reduced coordinator component is discussed for cases that involve homogeneous agents. 2009 Elsevier Inc. All rights reserved.",,
He,He X.,"This paper presents a formal framework for software system modeling, analysis, and realization. The major software architecture design perspective is outlined. Modeling methods for several popular software paradigms are presented. Major validation and verification techniques used in the framework are introduced. A translation approach for generating Java code from a SAM design is described.",,
He,"Ding J., Argote-Garcia G., Clarke P.J., He X.","How to ensure the quality of complex software systems is a grand challenge. Formal methods and software testing techniques are two major complementary approaches for software quality assurance. In this paper, we present a unique approach that uses the simulation capability of the Spin model checker to evaluate the test adequacy of high level Petri nets based on various coverage criteria. In our approach, a high level Petri net is expressed as a program in Promela, the input language for Spin, and the simulation capability of Spin is used to execute the program and evaluate different test coverage criteria for high level Petri nets. We use high level Petri nets as a concrete formal specification method to demonstrate our approach, and our results can be easily generalized to other formal models as well. Copyright 2008 ACM.",,
He,"Zeng R., Liu J., He X.","This paper presents a formal specification of Mondex, an electronic purse, using SAM. Mondex is the first pilot project for the 6th Grand Challenge to develop an integrated, automated toolset that developers can use to establish the correctness of software. Several research groups around the world have applied different formal methods in specifying and analyzing the Mondex smart card since 2006. Our specification is unique, which uses a software architecture model integrating high level Petri nets and temporal logic; thus contributes to the world wide effort in tackling one of the grand challenges in computer sciences. 2008 IEEE.",,
He,"Ding J., Mo L., He X.","This paper proposes a refinement method based on a set of formal refinement patterns for software architecture design using Software Architecture Model (SAM). First, an approach for specification construction through property-preserving refinement patterns is discussed. The refinement patterns are categorized into connector refinement, component refinement and high-level Petri nets refinement. Then, modeling and refining a life insurance system is used to demonstrate how to applying the refinement patterns for software architecture design using SAM. The results demonstrate that a refinement method is an effective way to develop a high assurance system. Our result can be easily generalized to other formal methods as well. Copyright 2008 ACM.",,
He,"Argote-Garcia G., Clarke P.J., He X., Fu Y., Shi L.","Quality assurance is recognized as a critical aspect in software construction. SAM is a formal software architecture description model that combines Petri Nets and Temporal Logic. PROMELA is the language used in the Spin model checker. This paper presents an approach to translate a restricted SAM model to a PROMELA program, enabling the model checking of the SAM model. We define the translation and show its correctness in terms of completeness and consistency. Completeness establishes that a SAM model maps all of its elements to PROMELA ones; whereas, consistency defines that an execution of a SAM model has a corresponding execution in a PROMELA program. The translation is also implemented as part of our software tool supporting SAM. Some aspects of the tool are discussed.",,
He,"Fu Y., Ding J., Dong Z., He X.","In this paper we present a systematic translation algorithm that maps a software architecture model to rewriting logics. We consider a nowadays typical component-based software architecture model - SAM. SAM is a formal software architecture model that integrates two formalisms - Petri Nets and Temporal Logic. Our goal is to effectively describe the component based software architecture model SAM using a rewriting based semantics. This algorithm is implemented in Maude, a high performance declarative programming language that supports membership and rewriting logics. The contribution of this paper is we defined the translation algorithm to rewriting logic to show an interleaving semantic matching between the behavior model Petri net and rewriting logic. 2008 IEEE.",,
He,"Fu Y., Dong Z., He X.","A software architecture design has many benefits including aiding comprehension, supporting early analysis, and providing guidance for subsequent development activities. An additional major benefit is if a partial prototype implementation can be automatically generated from a given software architecture design. However, in the past decade less progress was made on automatically realizing software architecture designs. In this paper, we present a translator for automatically generating an implementation from a software architectural description. The implementation not only captures the functionality of the given architecture description, but also contains additional monitoring code for ensuring desirable behavior properties through runtime verification. Our method takes a software description written in SAM, a software architecture model integrating dual formal methods Petri nets and temporal logic, and generates ArchJava/Java/AspectJ code. More specifically, the structure of a SAM architecture description produces ArchJava code, the behavior models of components/connectors represented in Petri nets lead to plain Java code, and the property specifications defined in temporal logic generate AspectJ code; the above code segments are then integrated into Java code. An experimental result is provided. 2007 World Scientific Publishing Company.",,
He,"Fu Y., Dong Z., Argote-Garcia G., Shi L., He X.","SAM is a formal software architecture description model based on Petri nets and temporal logic. SAM Parser is a tool to automatically translate a SAM architecture design into a program in ArchJava/Java/AspectJ with run-time verification capability. In this paper, we present an approach to show the correctness of the translation algorithm implemented in SAM Parser. Our approach requires a restricted Petri net model with an interleaving semantics, defines the operational semantics of a Java program using communication traces, and shows the consistency between an execution sequence of a Petri net and a communication trace of the corresponding Java program. Copyright (2007) by Knowledge Systems Institute (KSI).",,
He,"Xu D., He X.","To effectively uncover aspect defects, system (or subsystem) testing is of importance because aspects crosscut multiple system components. This paper presents an approach for generating system test requirements from aspect-oriented use cases. Central to this approach is the formalization of a testable system model from aspect-oriented use cases. We explicitly capture various constraints among base and aspectual use cases. Specifically, we transform aspect-oriented use case diagrams and descriptions into aspect-oriented Petri nets. This makes it possible to generate meaningful use case sequences with respect to various coverage criteria (such as use case coverage, transition coverage and state coverage). When scenario tests for individual use cases are available, they can be composed into system tests according to the generated use case sequences. Copyright 2007 ACM.",,
He,"Huang Y., He X.","Centered on data abstraction and encapsulation, objectoriented technologies have been widely accepted in software industry in the past decade However, existing object technologies lack precise semantics, which makes them hard to analyze In this paper, a group of patterns is presented to formalize object-oriented systems with PZ nets - a formal integration of Petri nets and Z language These patterns use Petri nets to depict the overall system structure and control flows, class structures and collaborations, and dynamic behavior, and use Z to define the underlying specification of Petri nets.",,
He,"Fu Y., Dong Z., He X.","A software architecture design provides a high-level abstraction of system topology, functionality, and/or behavior; which provides the basis for early system understanding and analysis as well as the foundation for subsequent detailed design and implementation. However, research on software architecture in the past decade primarily focused on architecture description languages and their analysis techniques and less progress was made on automatically realizing software architecture designs. In this paper, we present a method for automatically generating an implementation from a software architectural description. The implementation not only captures the functionality of the given architecture description, but also contains additional monitoring code for ensuring desirable behavior properties through run-time verification. Our method takes a software description written in SAM, a software architecture model integrating dual formal methods Petri nets and temporal logic, and generates Java code. More specifically, the structure of a SAM architecture description produces Arch-Java code the behavior models of components/connectors represented in Petri nets lead to plain Java code, and the property specifications defined in temporal logic generates AspectJ code; the above code segments are then integrated into Java code. 2006 IEEE.",,
He,"Fu Y., Dong Z., He X.","Web systems are self-descriptive software components which can automatically be discovered and engaged, together with other web components, to complete tasks over the Internet. Unified Modeling Language (UML), a widely accepted object-oriented system modeling and design language, and adapted for software architecture descriptions for several years, has been used for the web system description recently. However, it is hard to detect the system problems, such as correctness, consistency etc., of the integration of Web services without a formal semantics of web services architecture. In this paper, we proposed an approach to solving this issue by translating the UML web service architecture description into a formal modeling language - SO-SAM, and verify the correctness of the web system design using model checking techniques. We presented this approach through an imaging processing scenario in the distributed web application.",,
He,"Yu H., Liu D., Shao Z., He X.","Aspect-oriented programming aims to enhancing concern modulization and integration of complex software systems, which arouses great interest for researchers and practitioners However, less attention has been paid to modeling and quality assurance at the early phase of aspect-oriented software development This paper proposes a modeling method based on an aspect extension of Object-Z The aspect extension provides means for observing behaviors of class schemas and depicting their interrelationships System functionality and other crosscutting concerns can be separately modeled, and a weaving mechanism systematically composes these models into a complete system model Our aspect-oriented modeling method enjoys good traceability for property analysis, and reusability for system specification.",,
He,"Fu Y., Dong Z., He X.","Current service architecture description language and composition approaches consider simplistic method invocation. They pay less attention to the formal semantics and verification of service composition in the design, and less support property specifications and architecture validation. This paper presents an executable web service architecture model, Service-Oriented Software Architecture Model (SO-SAM), which is an extension of SAM (Software Architecture Model [16]) to the web service applications, and verification of web system properties in the design. SO-SAM describes each web service in terms of component and service composition in terms of connector separately. Furthermore, we validate SO-SAM model to prove that it facilitates the verification and monitoring of web services integration through translation to the Maude programming langauge, a high level language and high performance executable specification with the componentized and object-oriented design, as well as using model checking technique in the system design level. Finally, a case study of the validation of the model is demonstrated.",,
He,"Dong Z., Fu Y., He X.","This paper presents a framework to model componentbased systems made up of a set of components interacting with each other through message exchange Component behavior is specified by algebraic high-level nets The idea of ""nets as tokens"" is explored to integrate behavioral model with component communication mechanisms Component interactions are modeled by transformation rules based on the category of algebraic high-level nets Component models are synthesized into a valid system model by applying transformation rules according to predefined consistent conditions In addition, transformation rules can also be explored to refine component models The main contribution of the framework is the explicit separation and seamless synthesis between component models and their interaction models based on different techniques Another contribution is the application of ""net as token"" to algebraic high-level nets to model more complex components.",,
He,"Sun W., Shi T., Argote-Garcia G., Deng Y., He X.","User-centric Communication Middleware (UCM) is proposed to provide a high-level unified user-centric communication abstraction for upper-layer communication applications by separating and isolating the complexities of network-level communication control and media delivery from the diversity of application-dependent communication logic Due to the complexity of UCM, it is a major challenge to ensure its correct design and implementation In this paper we present our approach for designing UCM through formal modeling and analysis based on SAM, a general formal framework for specifying and analyzing software architectures We present the formal model for UCM using SAM and provide a method to analyze SAM architectural specifications using model checker SPIN.",,
He,"Ding J., Clarke P.J., Xu D., He X., Deng Y.","Mobile agents provide an effective and flexible approach to developing complex distributed systems. Formalizing the software architecture of mobile agent systems supports the development of high quality agent systems. In this paper, the software architecture of an interoperable mobile agent system is defined using two-layer predicate transition (PrT) nets. Based on the two-layer PrT net models, a formal model-based approach to develop mobile agents systems is proposed. The approach presented in this paper naturally integrates formal methods and practical approaches in each phase of the development life-cycle of the agent system. The proposed approach for agent systems can also be used to develop other complex software systems. 2007 - IOS Press and the authors.",,
He,"Fu Y., Dong Z., He X.","Architecture description languages (ADLs) are developed to precisely and formally describe software conceptual architecture that is distinguished from the system's implementation. Due to the gap between an architecture model and its implementation, the benefits of ADLs cannot be fully realized without a systematic mapping from an architectural description to an implementation. However, the implementation of an architecture model is not only error-prone, but also hard to verify. Some ADLs support code generation from software architecture. However, most of them cannot enforce communication integrity in the implementation. In this paper, we present a methodology to translate software architecture designs to Arch,Java automatically and the communication integrity is guaranteed by ArchJava.",,
He,"Yu H., Liu D., Yang L., He X.","Separation of concerns is one of the software engineering design principles that is getting more attention from practitioners and researchers in order to promote design and code reuse. Aspect-oriented programming is a maturing technique to enhance concern modulizalion and integration, which arouses great interest in both research society and software industry. However, less attention has been paid to modeling and quality assurance of aspect-oriented software development method. This paper proposes a formal aspect-oriented modeling language called AspectZ. and an aspect-oriented modeling method in AspectZ. The basic idea is to provide means for observing behaviors of Z schemas and depicting their interrelationships. and to provide ways for weaving interrelated schemas. Correctness of aspect weaving can he formally verified by the reasoning mechanisms of Z notation.",,
He,"Fu Y., Dong Z., He X.","Software architectures shift developers 'focus from lines-of-code to coarser-grained architectural elements and their interconnection structure. However, the benefits of architecture description languages (ADLs) cannot be fully captured without an automated realization of software architecture designs because manually shifting from a model to its implementation is error-prone. We proposed an integrated approach for automatically translating software architecture design models to an implementation and validating the translation as well as the implementation by exploring runtime verification technique and aspect-oriented programming. Specifically, system properties are not only verified against design models, but also verified during the execution of the generated implementation of software architecture design. A prototype tool, SAM Parser, is developed to demonstrate the approach on SAM(Software Architecture Model). In SAM Parser, all the realization and verification code can be automatically generated without human intervention. In this paper, we first brief describe the approach report on a case study conducted at an e-commerce scenario, an online shopping system to assess the benefits of automated realization of software architecture design and validation in a web service domain. 2005 IEEE.",,
He,"Dong Z., Fu Y., Fu Y., He X.","The benefits of architecture description languages (ADLs) cannot be fully captured without a automated and validated realization of software architecture designs. In addition to the automated realization of software architecture designs, we validate the realization process by exploring the runtime verification technique and aspect-oriented programming. More specifically, system properties are not only verified against design models, but also verified during execution of the generated implementation of software architecture designs. All these can be done in an automated way. In this paper, we show that our methodology of automated realization of software architecture designs and validation of the implementation is viable through a case study. Springer-Verlag Berlin Heidelberg 2005.",,
He,"Ding J., Dai Z., Wang J., He X.","Mobile agents provide a powerful and flexible paradigm for the development of autonomic computing systems. However, due to the security concern, mobile agents are not popularly used for real-world systems. In this paper, we define a security framework that can effectively protect mobile agents and agent systems from intruder attacking. In the framework, a mobile agent finder, which is extended with a registration protocol, is used to authenticate and authorize agent systems, incoming messages, and agents. We formally model the secure mobile agent finder using predicate transition nets, and analyze the models using model checking tool Spin. The results help us to develop high confidence applications using mobile agents. In addition, the modeling and analysis approach can be easily extended to develop other complex software systems. ©2005 IEEE.Iyengar",,
Iyengar,"Baral R., Iyengar S.S., Li T., Balakrishnan N.","The location-based social networks (LBSN) (e.g., Facebook, etc.) have been explored in the past decade for Point-of-Interest (POI) recommendation. Many of the existing systems focus on recommending a single location or a list which might not be contextually coherent. In this paper, we propose a model termed CLoSe (Contextualized Location Sequence Recommender) that generates contextually coherent POI sequences relevant to user preferences. The POI sequence recommenders are helpful in many day-to-day activities, for e.g., itinerary planning, etc. To the best of our knowledge, this paper is the first to formulate contextual POI sequence recommendation by exploiting Recurrent Neural Network (RNN). We incorporate check-in contexts to the hidden layer and global context to the hidden and output layers of RNN. We also demonstrate the efficiency of extended Long-short term memory (LSTM) in sequence generation. The main contributions of this paper are: (i) it exploits multi-context, personalized user preferences to formulate contextual POI sequence generation, (ii) it presents contextual extensions of RNN and LSTM that incorporate different contexts applicable to a POI and POI sequence, and (iii) it demonstrates significant performance gain of proposed model on pair-F1 and NDCG metrics when evaluated with two real-world datasets. 2018 Association for Computing Machinery.",,
Iyengar,"Shahid A.R., Pissinou N., Iyengar S.S., Makki K.","In Location-Based Social Networks (LBSNs) users' interaction is largely carried through check-ins and photo sharing. The study of location privacy issues with check-ins has yielded different Location Privacy-Preserving Mechanisms (LPPMs), including dummies to hide user's whereabouts in a set of dummy locations without using a trusted-third party(TTP). However, the impact of shared photos on location privacy is yet to be understood. Our experiment on real data from a LBSN reveals that, like the check-ins, the spatial distribution of shared photos can influence user's location privacy. In this paper, we propose an inference model based on spatial distribution of historical check-ins and photos, and show that it is possible to deduce user's location at a high accuracy through spatiotemporal analysis of multiple events, comprising check-ins and photos. In the process, we evaluate state-of-the-art dummy mechanisms with the proposed inference model to stress the limitation of existing LPPMs. Then, we design a LPPM, called photo-check, to protect user privacy in LBSN for both check-in and photos; and carry out the experiment with real data from Foursquare to show its effectiveness and efficacy. 2018 IEEE.",,
Iyengar,"Meda N.S., Sadashiva T.G., Ramani S.K., Iyengar S.S.","Wireless Sensor Network is having a potential to change the scope of new generation computing. Wireless Sensor Network (WSN) provides powerful means to gather the different kind of data and for data acquisition. It has a wide array of applications ranging from health monitoring to military. Our work focuses mainly on plant monitoring. In this paper, we propose the novel idea of designing mobile testbeds which are capable of moving randomly all along the agricultural field. We then concentrate on the technique to collect the data with the help of mobile testbeds at different locations in the agricultural field. Mobile testbeds collect and transmit the data from many locations at different time intervals. The combined data is analysed in the base station. Forecasting with regression is useful in predicting or forecasting the variable. Prediction about the desired variable is made by the technique called Regression model. Plant health monitoring also includes prevention against bacterial and fungal diseases. This paper also discusses the detection of disease and its name for a particular plant by using image processing techniques. Index terms: Mobile testbed, Forecasting with regression, Bilateral filtering, Canny edge detection, Contour method. 2017 IEEE.",,
Iyengar,"Nagaraj K., Sadashiva T.G., Ramani S.K., Iyengar S.S.","Fire and smoke are common sight at mines and could be catastrophic for humans working there. Our paper focuses on an alert mechanism that is built based on detection and validation of smoke from mines. We propose a computer vision based model that is robust enough to detect presence of smoke and the direction of motion of smoke. This research paper proposes a smoke detection model integrated with Unmanned Aerial Vehicle (UAV). The model focuses on smoke detection that leads to early detection of fire. The model works on the basis of image processing. The distinguishing feature of our model is the dynamic and real time processing. Here the system tracks the dynamic environment in the mining areas using UAVs featured with cameras. The real time captured video frames are calibrated to overcome distortion considering the radial and tangential factors. The calibrated frame thus obtained is further processed to get the Region of Interest (ROI) based on the spatial Hue Saturation Value (HSV) colouring model. Each pixel of the frame is then compared with colour range of smoke using opencv functions. The matched ROI is de-noised using Non-local means De-nosing algorithm to avoid unnecessary colours. These filtered frames are then processed concurrently to identify the pattern of change in motion. Based on this pattern the ROI is classified as either smoke or no smoke. With the analysis of turbulence in each pixel the intensity of smoke is detected. The experiment is conducted with-1) Prerecorded smoke video clippings. 2) Real time video captured in the camera. The results turned out to be positive in both the test cases. So, the model works quite efficiently with both recorded and real time video. Key Terms: UAV, ROI, HSV, De-noising algorithm, pattern matching. 2017 IEEE.",,
Iyengar,"Archana S., Thejas G.S., Ramani S.K., Iyengar S.S.","Computer vision can be used as an integral part of any autonomous systems. Visual input and processing enables faster and early decisions. An important challenge in computer vision is detection and recognition of objects. This challenge is more pronounced in low illumination. In this paper, we are proposing a detection and recognition model for road warning signs with voice notification system for both autonomous and usual vehicles considering varied level of illumination. Realtime video from the vehicles was analysed using opencv. The noise from the video was removed using filters. Detection was based on Haar-cascades and training was done with sample positive and negative images. Text recognition was based on pattern matching. Voice notification was done using string to voice converters. The night vision was lightened considering the glare of vehicles headlight. 2017 IEEE.",,
Iyengar,"Mithil K.M., Thejas G.S., Ramani S.K., Iyengar S.S.","This paper investigates the challenges related to a very fundamental problem of efficient obstacle avoidance on roads in the modern world, with respect to the self balancing robot. The amalgamation of navigation in self balancing robot pose newer challenges in circumvention of traffic congestion. Comprehensive research has been done on the obstacle avoidance of the four wheeled vehicles, but we have other problems to address in the case of self balancing robots on two wheels. Speed is becoming a predominant factor in the present day automotive industry. In this paper we address this very issue and propose a model driven by the outputs from multi sensorial data generated using various sensors and image processing techniques. The distinguishing feature of our technique is how we tackle the high speed obstacles. The paper channelizes its focus on avoiding high speed obstacles by tracking the object in real time using image processing techniques and then creating a 3-D point cloud of the object and its static surroundings through a matrix of arrays, using the Light Detection And Ranging (LiDAR) module. 2017 IEEE.",,
Iyengar,"Mithil K.M., Thejas G.S., Ramani S.K., Iyengar S.S.","This paper investigates the challenges related to a very fundamental problem of efficient obstacle avoidance on roads in the modern world, with respect to the self balancing robot. The amalgamation of navigation in self balancing robot pose newer challenges in circumvention of traffic congestion. Comprehensive research has been done on the obstacle avoidance of the four wheeled vehicles, but we have other problems to address in the case of self balancing robots on two wheels. Speed is becoming a predominant factor in the present day automotive industry. In this paper we address this very issue and propose a model driven by the outputs from multi sensorial data generated using various sensors and image processing techniques. The distinguishing feature of our technique is how we tackle the high speed obstacles. The paper channelizes its focus on avoiding high speed obstacles by tracking the object in real time using image processing techniques and then creating a 3-D point cloud of the object and its static surroundings through a matrix of arrays, using the Light Detection And Ranging (LiDAR) module. 2017 IEEE.","Cickovski T., Narasimhan G.","Motivation: Software pipelines have become almost standardized tools for microbiome analysis. Currently many pipelines are available, often sharing some of the same algorithms as stages. This is largely because each pipeline has its own source language and file formats, making it typically more economical to reinvent the wheel than to learn and interface to an existing package. We present Plugin-Based Microbiome Analysis (PluMA), which addresses this problem by providing a lightweight back end that can be infinitely extended using dynamically loaded plugin extensions. These can be written in one of many compiled or scripting languages. With PluMA and its online plugin pool, algorithm designers can easily plug-and-play existing pipeline stages with no knowledge of their underlying implementation, allowing them to efficiently test a new algorithm alongside these stages or combine them in a new and creative way. Results: We demonstrate the usefulness of PluMA through an example pipeline (P-M16S) that expands an obesity study involving gut microbiome samples from the mouse, by integrating multiple plugins using a variety of source languages and file formats, and producing new results. The Author(s) 2018."
Iyengar,"Mithil K.M., Thejas G.S., Ramani S.K., Iyengar S.S.","This paper investigates the challenges related to a very fundamental problem of efficient obstacle avoidance on roads in the modern world, with respect to the self balancing robot. The amalgamation of navigation in self balancing robot pose newer challenges in circumvention of traffic congestion. Comprehensive research has been done on the obstacle avoidance of the four wheeled vehicles, but we have other problems to address in the case of self balancing robots on two wheels. Speed is becoming a predominant factor in the present day automotive industry. In this paper we address this very issue and propose a model driven by the outputs from multi sensorial data generated using various sensors and image processing techniques. The distinguishing feature of our technique is how we tackle the high speed obstacles. The paper channelizes its focus on avoiding high speed obstacles by tracking the object in real time using image processing techniques and then creating a 3-D point cloud of the object and its static surroundings through a matrix of arrays, using the Light Detection And Ranging (LiDAR) module. 2017 IEEE.",,
Iyengar,"Prathima E.G., Venugopal K.R., Iyengar S.S., Patnaik L.M.","In heterogeneous sensor networks, multiple events may occur simultaneously that have different QoS requirements. Adaptive Reliable Routing for QoS based Data Aggregation (ARRDA) method is presented that allows to route the aggregated data from simultaneous events reliably to the sink based on their flow specification. On detection of an event type with a particular specification, an adaptive aggregation tree is established from the source to the sink that can satisfy the QoS specification. Implicit acknowledgement mechanisms are used to ensure reliability. Simulation results show that the ARRDA incurs low communication cost, energy and delay in comparison with SASR [1]. 2017 IEEE.",,
Iyengar,"Tian H., Pouyanfar S., Chen J., Chen S.-C., Iyengar S.S.","Deep neural networks such as Convolutional Neural Networks (CNNs) have achieved several significant milestones in visual data analytics. Benefited from transfer learning, many researchers use pre-trained CNN models to accelerate the training process. However, there is still uncertainty about the deep learning models, structures, and applications. For instance, the diversity of the datasets may affect the performance of each pre-trained model. Therefore, in this paper, we proposed a new approach based on genetic algorithms to select or regenerate the best pre-trained CNN models for different visual datasets. A new genetic encoding model is presented which denotes different pre-trained models in our population. During the evolutionary process, the optimal genetic code that represents the best model is selected, or new competitive individuals are generated using the genetic operations. The experimental results illustrate the effectiveness of the proposed framework which outperforms several existing approaches in visual data classification. 2018 IEEE.",,
Iyengar,"Baral R., Zhu X.L., Iyengar S.S., Li T.","The Location-Based Social Networks (LBSN) (e.g., Facebook, etc.) have many attributes (e.g., ratings, reviews, etc.) that play a crucial role for the Point-of-Interest (POI) recommendations. Unlike ratings, the reviews can help users to elaborate their consumption experience in terms of relevant factors of interest (aspects). Though some of the existing systems have exploited user reviews, most of them are less transparent and non-interpretable (as they conceal the reason behind recommendation). These reasons have motivated us towards explainable and interpretable recommendation. To the best of our knowledge, only few of the researchers have exploited user reviews to incorporate the sentiment and opinions on different aspects for personalized and explainable POI recommendation. This paper proposes a model termed as ReEL (Review aware Explanation of Location Recommendation) which models the reviewaspect correlation by exploiting deep neural network, formulates user-aspect bipartite relation as a bipartite graph, and models the explainable recommendation by using dense subgraph extraction and ranking-based techniques. The major contributions of this paper are: (i) it models users and POIs using the aspects posted on user reviews, and it provisions incorporation of multiple contexts (e.g., categorical, spatial, etc.) in POI recommendation, (ii) it formulates preference of users' on aspects as a bipartite relation, represents it as a location-aspect bipartite graph, and models the explainable recommendation with the notion of ordered dense subgraph extraction using bipartite cores, shingles, and ranking-based techniques, and (iii) it extensively evaluates the proposed models using three real-world datasets and demonstrates an improvement of 5.8% to 29.5% on F-score metric, when compared to the relevant studies. 2018 Association for Computing Machinery.",,
Iyengar,"Pattar S., Buyya R., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Internet of Things (IoT) paradigm links physical objects in the real world to cyber world and enables the creation of smart environments and applications. A physical object is the fundamental building block of the IoT, known as a Smart Device, that can monitor the environment. These devices can communicate with each other and have data processing abilities. When deployed, smart devices collect real-time data and publish the gathered data on the Web. The functionality of smart devices can be abstracted as a service and an IoT application can be built by combining the smart devices with these services that help to address challenges of day-to-day activities. The IoT comprises billions of these intelligent communicating devices that generate enormous amount of data, and hence performing analysis on this data is a significant task. Using search techniques, the size and extent of data can be reduced and limited, so that an application can choose just the most important and valuable data items as per its necessities. It is, however, a tedious task to effectively seek and select a proper device and/or its data among a large number of available devices for a specific application. Search techniques are fundamental to IoT and poses various challenges like a large number of devices, dynamic availability, restrictions on resource utilization, real time data in various types and formats, past and historical monitoring. In the recent past, various methods and techniques have been developed by the research community to address these issues. In this paper, we present a review of the state-of-the-art search methods for the IoT, classifying them according to their design principle and search approaches as: IoT data and IoT object-based techniques. Under each classification, we describe the method adopted, their advantages and disadvantages. Finally, we identify and discuss key challenges and future research directions that will allow the next generation search techniques to recognize and respond to user queries and satisfy the information needs of users. 1998-2012 IEEE.",,
Iyengar,"Tasnim S., Caldas J., Pissinou N., Iyengar S.S., Ding Z.","The rapid development of mobile sensing technologies (like GPS, RFID, accelerometer, gyroscope etc. various sensors in smart phones) has caused a rise in the large-scale capture of positioning data. These mobility data generated by heterogeneous mobile devices with embedded sensors are mostly known as trajectories. There are different algorithms to process the large amounts of mobility data for identifying mobility patterns. Even though few of these algorithms consider the use of semantic annotations on the data, none of the existing research has considered semantic annotations for online sub-trajectory clustering-based movement behavior analysis. In this paper, we incorporate semantics annotation in the raw trajectory data in order to discover various movement relationships between subtrajectories of mobile devices. We conduct experiment on a realworld data set. Along with the added advantage of semanticaware movement behavior analysis, our method is able to identify outliers in the clustering process with almost similar performance (average recall 0.92) as classic density-based clustering algorithm DBSCAN. 2018 IEEE.",,
Iyengar,"Raghavendra S., Girish S., Geeta C.M., Buyya R., Venugopal K.R., Iyengar S.S., Patnaik L.M.","A substitute solution for various organizations of data owners to store their data in the cloud using storage as a service(SaaS). The outsourced sensitive data is encrypted before uploading into the cloud to achieve data privacy. The encrypted data is search based on keywords and retrieve interested files by data user using a lot of traditional Search scheme. Existing search schemes supports exact keyword match or fuzzy keyword search, but synonym based multi-keyword search are not supported. In the real world scenario, cloud users may not know the exact keyword for searching and they might give synonym of the keyword as the input for search instead of exact or fuzzy keyword due to lack of appropriate knowledge of data. In this paper, we describe an efficient search approach for encrypted data called as Split Keyword Fuzzy and Synonym Search (SKFS). Multi-keyword ranked search with accurate keyword and Fuzzy search supports synonym queries are a major contribution of SKFS. The wildcard Technique is used to store the keywords securely within the index tree. Index tree helps to search faster, accurate and low storage cost. Extensive experimental results on real-time data sets shows, the proposed solution is effective and efficient for multi-keyword ranked search and synonym queries Fuzzy based search over encrypted cloud data. 2017, Springer Science+Business Media, LLC.",,
Iyengar,"Kamhoua G.A., Pissinou N., Iyengar S.S., Beltran J., Miller J., Kamhoua C.A., Njilla L.L.","Crowdsourcing services have become one of the most common ways organizations can gather ideas for new products and services from large crowds of consumers by offering monetary rewards depending on the tasks. However, this monetary reward has begun to attract malicious crowds of users who wish to complete the task with minimal effort through collaboration. For instance, a task based on reviews of a product can be degraded when malicious users copy each other with minimal edits of the review, giving a misrepresentation of the true quality of the product. More specifically, we investigate the case where different malicious crowd sizes cooperate on different tasks, known as overlapping groups. Such sophisticated and hard to detect malicious crowds provide unfair evaluations and misleading results to the crowdsourcers. To overcome this type of attack, we propose two methods to point out such groups with high accuracy. The first method detects similar reviews by including a new proposed similarity between review texts and show the results outperform the vectorial similarity measures used in prior works. The second method is based on community detection on networks and exploits the semantic similarity of the reviews. The experiments were conducted on reviews from Ott dataset on Amazon Mechanical Turk. 2017 IEEE.",,
Iyengar,"Venkatesh, Sengar C.S., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Real-time industrial application requires a routing protocol that guarantees data delivery with reliable, efficient and low end-to-end delay. Existing Two-Hop Velocity based Routing (THVR) protocol relates twohop velocity to end-to-end delay to select the next forwarding node, that has the overhead of exchanging control packets and depleting the available energy in nodes. We propose a Real-Time Reliable Data delivery based on Virtual Coordinates Routing (RRDVCR) algorithm, based on the number of hops to the destination rather than geographic distance. Selection of forwarding node is based on packet progress offered by two-hops, link quality and available energy at the forwarding nodes. All these metric are co-related by dynamic co-relation factor. The proposed protocol uses a selective acknowledgment scheme that results in lower overhead and energy consumption. Simulation results show that there is about 22 and 9.5% decrease in energy consumption compared to SPEED respectively, 16 and 38% increase in packet delivery compared to THVR and SPEED respectively and overhead is reduced by 50%. 2018 Venkatesh, Chanchal Singh Sengar, Kuppanna Rajuk Venugopal, Sundaraja Sitharama Iyengar and Lalit Mohan Patnaik.",,
Iyengar,"Wang Q., Li T., Iyengar S.S., Shwartz L., Grabarnik G.Ya.","The increasing complexity of IT environments urgently requires the use of analytical approaches and automated problem resolution for more efficient delivery of IT services. In this paper, we model the automation recommendation procedure of IT automation services as a contextual bandit problem with dependent arms, where the arms are in the form of hierarchies. Intuitively, different automations in IT automation services, designed to automatically solve the corresponding ticket problems, can be organized into a hierarchy by domain experts according to the types of ticket problems. We introduce a novel hierarchical multi-armed bandit algorithms leveraging the hierarchies, which can match the coarse-to-fine feature space of arms. Empirical experiments on a real large-scale ticket dataset have demonstrated substantial improvements over the conventional bandit algorithms. In addition, a case study of dealing with the cold-start problem is conducted to clearly show the merits of our proposed algorithms. 2018 by SIAM.",,
Iyengar,"Venkatesh, Akshay A.L., Kushal P., Venugopal K.R., Patnaik L.M., Iyengar S.S.","Existing work Geographic opportunistic routing (GOR) selects a forwarding sensor node to progress data packets on the basis of geographic distance. Similarly, the multipath routing uses multiple paths to achieve both reliability and delay. However, geographic opportunistic routing results in lower packet delivery rate and high latency. The multipath routing introduces channel contention, interference, and quick depletion of energy of the sensor node in an asymmetric link wireless environment. The existing work Efficient QoS-aware Geographic Opportunistic Routing (EQGOR) elects and prioritize the forwarding nodes to achieve different QoS parameters. However, in EQGOR, the count of forwarding nodes increases with the increase in the required reliability. To improve energy efficiency, delay, and successful ratio of packet delivery in WSNs, we propose a Two-Hop Geographic Opportunistic Routing (THGOR) protocol that selects a subset of 2-hop neighbors of node which has high packet reception ratio and residual energy at the next forwarder node, and the selected 1-hop neighbors of node has supreme coverage of 2-hop neighbors as relay nodes. THGOR is comprehensively evaluated through ns-2 simulator and compared with existing protocols EQGOR and GOR. Simulation results show that THGOR significant improvement in packet advancement, delay, reliable transmission, and energy efficient. 2018, Springer Nature Singapore Pte Ltd.",,
Iyengar,"Shiva P.T., Raja K.B., Venugopal K.R., Iyengar S.S., Patnaik L.M.","This paper proposes a Link Reliability based Two-Hop Routing protocol for Wireless Sensor Networks (WSNs). The protocol achieves to reduce packet deadline miss ratio (DMR) while considering link reliability, two-hop delay and power efficiency and utilizes memory and computational effective methods for estimating the link metrics. Numerical results provide insights that the protocol has a lower packet deadline miss ratio and results into longer sensor network lifetime. The results show that the proposed protocol is a feasible solution to the QoS routing problem in WSNs that support real-time applications. 2013 NICT.",,
Iyengar,"Meng T., Soliman A.T., Shyu M.-L., Yang Y., Chen S.-C., Iyengar S.S., Yordy J.S., Iyengar P.","With the rapid development of next generation sequencing technology, the amount of biological sequence data of the cancer genome increases exponentially, which calls for efficient and effective algorithms that may identify patterns hidden underneath the raw data that may distinguish cancer Achilles' heels. From a signal processing point of view, biological units of information, including DNA and protein sequences, have been viewed as one-dimensional signals. Therefore, researchers have been applying signal processing techniques to mine the potentially significant patterns within these sequences. More specifically, in recent years, wavelet transforms have become an important mathematical analysis tool, with a wide and ever increasing range of applications. The versatility of wavelet analytic techniques has forged new interdisciplinary bounds by offering common solutions to apparently diverse problems and providing a new unifying perspective on problems of cancer genome research. In this paper, we provide a survey of how wavelet analysis has been applied to cancer bioinformatics questions. Specifically, we discuss several approaches of representing the biological sequence data numerically and methods of using wavelet analysis on the numerical sequences. 2013 IEEE.",,
Iyengar,"Iyer V., Iyengar S., Pissinou N., Ren S.","The process of inversion, estimation and reconstruction of the sensor quality matrix, allows modeling the precision and accuracy, and in general the reliability of the model. When the sensor data ranges are not known a priori, current systems do not train on new data samples, rather they approximate based on the parameter's global average value, losing most of the spatial and temporal features. The proposed model, which we call SPOTLESS, checks the spatial integrity and temporal plausibility of streams generated by mobility patterns due to varying channel conditions. We define a minimum quality of the measured sensor data as local stream (QoD) requirements to give high precision by using distributed labeled training. In our SPOTLESS data-cleaning steps, to account for packet errors due to varying channel conditions, a soft-phy based decoding is selected for various Bit Error Rates (BER), minimizing packet loss at the mobile receiver. Numerical experiments for Rayleigh fading channels and mobile BER model examples are compared with large deployment of ground sensor collecting static data streams and Data MULE collecting multi-hop temporal data from the sensor to provide hypothetical parameter accuracy. Our results were obtained in the context of provisioning a minimum precision and accuracy stream (QoD) required for 802.15.4 mobile services. SPOTLESS data-cleaning algorithm coding provides 90% precision for static streams, and increases the plausible relevance of multi-hop mobile streams by 85% for task-based learning. 2013 IEEE.",,
Iyengar,"Weltman J.S., Iyengar S.S., Hegarty M.","In natural language, there are many gaps between what is stated and what is understood. Speakers and listeners fill in these gaps, presumably from some life experience, but no one knows how to get this experiential data into a computer. As a first step, we have created a methodology and software interface for collecting commonsense data about simple experiences. This work is intended to form the basis of a new resource for natural language processing. We model experience as a sequence of comic frames, annotated with the changing intentional and physical states of the characters and objects. To create an annotated experience, our software interface guides non-experts in identifying facts about experiences that humans normally take for granted. As part of this process, the system asks questions using the Socratic Method to help users notice difficult-to-articulate commonsense data. A test on ten subjects indicates that non-experts are able to produce high quality experiential data. Copyright 2013 ACM.",,
Iyengar,"Krishna Kumar P.T., Vinod P.T., Phoha V.V., Iyengar S.S., Iyengar P.","Cancer risk management involves obliterating excess concentration of cancer causing trace elements by the natural immune system and hence intake of nutritious diet is of paramount importance. Human diet should consist of essential macronutrients that have to be consumed in large quantities and trace elements are to be consumed in very little amount. As some of these trace elements are causative factors for various types of cancer and build up at the expense of macronutrients, cancer risk management of these trace elements should be based on their initial concentration in the blood of each individual and not on their tolerable upper intake level. We propose an information theory based Expert System (ES) for estimating the lowest limit of toxicity association between the trace elements and the macronutrients. Such an estimate would enable the physician to prescribe required medication containing the macronutrients to annul the toxicity of cancer risk trace elements. The lowest limit of toxicity association is achieved by minimizing the correlated information of the concentration correlation matrix using the concept of Mutual Information (MI) and an algorithm based on a Technique of Determinant Inequalities (TDI) developed by the authors. The novelty of our ES is that it provides the lowest limit of toxicity profile for all trace elements in the blood not restricted to a group of compounds having similar structure. We demonstrate the superiority our algorithm over Principal Component Analysis in mitigating trace element toxicity in blood samples. the author(s), publisher and licensee Libertas Academica Ltd.",,
Iyengar,"Sivasankari H., Aparna R., Venugopal K.R., Iyengar S.S., Patnaik L.M.","In Wireless Sensor Networks (WSNs), energy and reliable data delivery are two major issues. Sending data from source to destination without void problem is an objective of any routing algorithm. The existing Greedy Anti-void Routing (GAR) uses the Rolling ball Undirected Traversal to guarantee the packet delivery from source to the destination. In the case of sparse network when it encounters an obstacle in the route it fails to deliver the data. To address this issue, we propose Trust dependent Greedy Anti-void Routing (TGAR) to find the reliable path from source to sink. We use Bayesian estimation model to calculate the trust value for the entire path. Simulation results show that TGAR achieves successful data delivery and energy conservation in sparse networks when compared with the existing Greedy Anti-void Routing (GAR) Algorithm. 2013 Springer Science+Business Media New York.",,
Iyengar,"Shiva P.T., Raja K.B., Venugopal K.R., Iyengar S.S., Patnaik L.M.","This paper proposes a Traffic-Differentiated Two-Hop Routing protocol for Quality of Service (QoS) in Wireless Sensor Networks (WSNs). It targets WSN applications having different types of data traffic with several priorities. The protocol achieves to increase packet reception ratio (PRR) and reduce end-to-end delay while considering multi-queue priority policy, two-hop neighborhood information, link reliability and power efficiency. The protocol is modular and utilizes effective methods for estimating the link metrics. Numerical results show that the proposed protocol is a feasible solution to addresses QoS service differentiation for traffic with different priorities. 2013 IEEE.",,
Iyengar,"Sivasankari H., Leelavathi R., Vallabh M., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Technological development in wireless communication enables the development of smart, tiny, low cost and low power sensor nodes to outperform for various applications in Wireless Sensor Networks. In the existing Tabu search algorithm, clusters are formed using initial solution algorithm to conserve energy. We propose a Cluster Based Energy Aware Routing (CEAR) algorithm to maximize energy conservation and lifetime of network with active and sleep nodes. The proposed algorithm, removes duplication of data through aggregation at the cluster heads with active and sleep modes. A comparative study of CEAR algorithm with Tabu search algorithm is obtained. Comparative study shows improvement in the Lifetime and energy conservation by 17 and 22 % respectively over the existing algorithm. 2013 Springer Science+Business Media New York.",,
Iyengar,"Tanuja R., Rekha M.K., Manjula S.H., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Wireless Sensor Networks (WSNs) are currently being used in a wide range of applications that demand high security requirements. Since sensor network is highly resource constrained, providing security becomes a challenging issue. Attacks must be detected and eliminated from the network as early as possible to enhance the rate of successful transactions. In this paper, we propose to eliminate Black Hole and False Data Injection attacks initiated by the compromised inside nodes and outside malicious nodes respectively using a new acknowledge scheme with low overhead. Simulation results show that our scheme can successfully identify and eliminate 100 % black hole nodes and ensures more than 99 % packet delivery with increased network traffic. 2013 Springer Science+Business Media New York.",,
Iyengar,"Prabakar N., Kim J.-H., Cerron U., Iyengar S.S.","Large institutions such as universities, companies, etc. face parking shortage problems due to the increase in number of vehicles and the limited availability of parking spaces. At the peak time of work, invariably all parking spaces will be taken and newly arriving vehicles will be searching for a vacant parking space in the entire parking facility without any clue. The frustration and loss of time for people looking for parking spaces are the major issues. In addition, this wastes lots of fuel and increases the probability for accidents. The proposed parking information system with distributed sensors will monitor the status of a parking facility and will provide a real-time status summary online. Drivers can access the status summary prior to arriving at the parking lot using smartphones or computers, or through displays at the parking entrance. Further, the frequency and parking duration of each vehicle can be tracked to improve infrastructure security. Other potential applications include effective monitoring and management of air travelers, conference attendees, and visitors in any major facility. Copyright 2012 by International Society of Computers and Their Applications (ISCA).",,
Iyengar,"Zhang H., Wu H.-C., Lu L., Iyengar S.S.","The optimal data fusion rule for multiple sensor detection systems based on the Bayesian criterion has been derived by Chair and Varshney in 1986. However, most of the following works are focused on how to implement such a fusion rule, since the probability of false alarm and the probability of miss detection are hard to evaluate in practice. Till now, although more and more satisfactory data-fusion implementation schemes are available, most of the cooperative spectrum sensing techniques are based on the simple energy-detection algorithm, which only relies on the energy of the received signal. However, when noise is relatively large or the time-varying characteristics of the signal are conspicuous, the energy-detection spectrum sensing algorithm is more prone to fail. Thus, in this paper, we propose a new adaptive cooperative spectrum sensing scheme, which is based on a novel detection algorithm involving JB (Jarque-Bera) statistic. The ROC (receiver-operating characteristic) curves show that our new cooperative spectrum sensing scheme is more robust than that based on the energy-detection spectrum sensing algorithm. Besides, the performance comparison also implies that the optimal data-fusion rule in our new cooperative spectrum sensing scheme is superior to the commonly adopted 'OR' and 'AND' rules in the existing literature. 2012 IEEE.",,
Iyengar,"Srikantaiah K.C., Suraj M., Venugopal K.R., Iyengar S.S., Patnaik L.M.","The Internet is a major source of all information that we essentially need. The information on the web cannot be analyzed and queried as per the user requests. Here, we propose and develop a similarity based web data extraction and integration system (WDES and WDICS) to extract search result pages from the web and integrate its contents to enable the user to perform intended analysis. The system provides for local replication of search result pages, in a manner convenient for offline browsing. The system organizes itself into two possible phases that are involved in performing the above task. We develop and implement algorithms for extracting and integrating the content from the web. Experiment is performed on the contents of Bluetooth product listings and it gives us a better Precision and Recall than DEPTA [1]. 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.",,
Iyengar,"Vishwanath R.H., Thanagamani M., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Stream time series retrieval has been a major area of study due to its vast application in various fields like weather forecasting, multimedia data retrieval and huge data analysis. Presently, there is a demand for stream data processing, high speed searching and quick response. In this paper, we use a alternate data cluster or segment mean method for stream time series data, where the data is pruned with a computational cost of O(log w). This approach can be used for both static and dynamic stream data processing. The results obtained are the better than the existing algorithms. 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.",,
Iyengar,"Shaila K., Sivasankari H., Manjula S.H., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Wireless Sensor Networks consists of sensor nodes that are capable of sensing the information and maintaining security. In this paper, an Anonymity Cluster based Trust Management algorithm(ACTM) is proposed which enhances the security level and provides a stable path for communication. It is observed that the performance of the network is better than existing schemes through simulation. 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.",,
Iyengar,"Sivasankari H., Leelavathi R., Shaila K., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Wireless Sensor Networks(WSNs) have micro sensors connected in a network with processing capabilities for communications. It is subjected to a set of resource constraints such as battery power, bandwidth and lifetime. The existing cooperative routing is addressed to reduce the energy consumption. We propose Dynamic Cooperative Routing (DCR) to conserve energy and maximization of lifetime with a static and mobile sink. A mathematical model is developed for the best placement of the sink in the deployment to reduce the distance between sources and sink. A comparative study of DCR algorithm with the existing algorithm is obtained for a static and mobile sink. In case of mobile sink, Comparative analysis shows the improvement in lifetime and energy conservation by 70% and 65% respectively over the existing algorithms. 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.",,
Iyengar,"Xu H., Chen P., Yu W., Sawant A., Iyengar S.S., Li X.","In this paper, we develop a feature-aware 4D spatiotemporal image registration method. Our model is based on a 4D (3D+time) free-form B-spline deformation model which has both spatial and temporal smoothness. We first introduce an automatic 3D feature extraction and matching method based on an improved 3D SIFT descriptor, which is scale- and rotation- invariant. Then we use the results of feature correspondence to guide an intensity-based deformable image registration. Experimental results show that our method can lead to smooth temporal registration with good matching accuracy; therefore this registration model is potentially suitable for dynamic tumor tracking. 2012 ICPR Org Committee.",,
Iyengar,"Sivsankari H., Leelavathi R., Shaila K., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Wireless Sensor Networks (WSNs) consist of hundreds or thousands of battery operated, computable and low cost sensors. In WSNs energy conservation is an important design issue for routing, power management and data dissemination protocols. The existing cooperative routing is addressed to reduce the energy consumption. We propose an algorithm Energy Efficient Adaptive Cooperative Routing (EEACR), that maximizes the energy and Lifetime of the network. Mathematical model is developed to find the best location of the sink to reduce the distance between sources and the sink. A comparative study of EEACR with multiple static and mobile sinks is carried out. Simulation result shows that the EEACR algorithm produces better result than the existing Algorithms. 2012 IEEE.",,
Iyengar,"Khurana S., Brener N., Karki B., Benger W., Roy S., Acharya S., Ritter M., Iyengar S.S.","This paper presents a multi scale color coding technique which enhances the visualization of scalar fields along integration lines in vector fields. In particular, this multi scale technique enables one to see detailed variations in selected small ranges of the scalar field while at the same time allowing one to observe the entire range of values of the scalar field. This type of visualization, observing small variations as well as the entire range of values, is usually not possible with uniform color coding. This multi scale approach, which is linear within each division of the scale (piecewise linear), is a general visualization technique that can be applied to many different scalar fields of interest. As an example, in this paper we apply it to the visualization of fluid flow mixing indicators along pathlines in computational fluid dynamics (CFD) simulations. Pathlines are the trajectories of fluid particles over time in the CFD simulations, and applying multi scale color coding on the pathlines brings out quantities of interest in the flow such as curvature, torsion and specific measure of length generation, which are indicators of the degree of mixing in the fluid system. In contrast to uniform color coding, this multi scale scheme can display small variations in the mixing indicators and still show the entire range of values of these indicators. In this paper, the mixing indicators are computed and displayed only along line structures (pathlines) in the flow field rather than at all points in the flow field.",,
Iyengar,"Huang S.C.-H., Wu H.-C., Iyengar S.S.","Nowadays, there is urgent demand for wireless sensor network applications. In these applications, usually a base station is responsible for monitoring the entire network and collecting information. If emergency happens, it will propagate such information to all other nodes. However, quite often the message source is not a fixed node, since there may be base stations in charge of different regions or events. Therefore, how to propagate information efficiently when message sources vary from time to time is a challenging issue. None of conventional broadcast algorithms can deal with this case efficiently, since the change of message source incurs a huge implementation cost of rebuilding a broadcast tree. To deal with this difficult problem, we make endeavor in studying multiple source broadcast, in which targeted algorithms should be source-independent to serve the practical need. In this paper, we formulate the Minimum-Latency Multisource Broadcast problem. We propose a novel solution using a fixed shared backbone, which is independent of the message sources and can be used repeatedly to reduce the broadcast latency. To the best of our knowledge, our work is deemed the first attempt to design such a multisource broadcast algorithm with a derived theoretical latency upper bound. 2012 IEEE.",,
Iyengar,"Tanuja R., Anoosha V., Manjula S.H., Venugopal K.R., Iyengar S.S., Patnaik L.M.","Wireless Sensor Networks (WSNs) are by its nature more prone to security attacks and data losses. Security and data privacy have become need of the day. The most challenging area in WSNs which needs security is target localization. In addition to this complexity we are here concentrating on acoustic sensor nodes which uses Particle Swarm Optimization (PSO) algorithm for Location Estimation. We propose Secure Reputation Update Target Localization (SRUTL) algorithm which addresses target localization and security issues viz., bad-mouth attack, Sybil attack, on-off attack and malicious node attack at different levels of target localization. Simulation results shows positive response in attack detection hence contribute in building a secure wireless sensor network. 2012 Springer-Verlag.",,
Iyengar,"Kumaraswamy M., Shaila K., Sivasankari H., Tejaswi V., Venugopal K.R., Iyengar S.S., Patnaik L.M.","The design of hybrid MAC protocol in Wireless Sensor Networks for delay sensitive data traffic QoS is a challenging work. We present Reservation Control Hybrid MAC (RCH-MAC) protocol, which reduces end-to-end delay, energy efficiency and maximizes the packet delivery ratio by minimizing the contention at the nodes. Here, a node operates the reservation procedure at the contention-based period and reserves a time slot in the adaptive contention-free time. All the neighbor nodes of the sender and receiver receives their own reservation control packets. Once reserved, the sender transmits data and receives ACK packets at the adaptive contention-free time. Since reservation packets occur in nodes along the routing path, the nodes reserve time slots successively in multi-hop. Simulation results demonstrate that the proposed protocol has significantly reduced the end-to-end latency and improved other QoS parameters like energy efficiency and packet delivery ratio. 2012 Springer-Verlag.",,
Iyengar,"Kamhoua C.A., Pissinou N., Makki K., Kwiat K., Iyengar S.S.","The demand on mobile data usage is exponentially increasing since the introduction of iPhones in 2007. The network became congested as millions of users tried to browse website and social networks, send e-mail, stream multimedia, and transfer file simultaneously. An immediate solution for the providers will be to change their pricing strategy with the goal of slowing down heavy users and decreasing the bandwidth demand. From the users' standpoint, network providers must constantly upgrade their infrastructure to accommodate new applications and devises. However, upgrading the infrastructure will be costly for the provider. A provider will prefer a minimum investment to upgrade the network while attracting the maximum number of customers. On the other hand, without regular upgrade of the network from the provider, there may be more congestion, more delays, and generally a low QoS at the user's dissatisfaction. Moreover, users that experience bad connection will be tempted to switch providers. We analyze the dynamic communication market and the users and providers' interactions in the framework of repeated game theory. We consider noise in user monitoring. We also compare two scenarios: individual and independent actions of users as opposed to the collective actions of users. For a collective action, a database aggregating users' QoS through a binary vote (good or bad QoS) needs to be implemented. The users keep their provider if and only if their majority reports a good QoS. This research shows that if the users collaborate, their bargaining power is increased. 2012 IEEE.",,
Iyengar,"Srinivasagopalan S., Busch C., Iyengar S.S.","We consider the problem of constructing a single spanning tree for the single-sink buy-at-bulk network design problem for doubling-dimension graphs. We compute a spanning tree to route a set of demands along a graph G to or from a designated sink node. The demands could be aggregated at (or symmetrically distributed to) intermediate edges where the fusion cost is specified by a nonnegative concave function f. We describe a novel approach for developing an oblivious spanning tree in the sense that it is independent of the number and location of data sources (or demands) and cost function at the edges. We present a deterministic, polynomial-time algorithm for constructing a spanning tree in low doubling-dimension graphs that guarantees a log 3 D-approximation over the optimal cost, where D is the diameter of the graph G. With a constant fusion-cost function, our spanning tree gives an O(log 3 D)-approximation for every Steiner tree that includes the sink. We also provide a ? (log n) lower bound for any oblivious tree in low doubling-dimension graphs. To our knowledge, this is the first paper to propose a single spanning tree solution to the single-sink buy-at-bulk network design problem (as opposed to multiple overlay trees). 2012 IEEE.",,
Iyengar,"Lee Y., Iyengar S.S., Min C., Ju Y., Kang S., Park T., Lee J., Rhee Y., Song J.","User context is defined by data generated through everyday physical activity in sensorrich, resource-limited mobile environments. 2012 ACM.",,
Iyengar,"Li X., Yu W., Lin X., Iyengar S.S.","This paper studies the optimal inspection of autonomous robots in a complex pipeline system. We solve a 3-D region-guarding problem to suggest the necessary inspection spots. The proposed hierarchical integer linear programming optimization algorithm seeks the fewest spots necessary to cover the entire given 3-D region. Unlike most existing pipeline inspection systems that focus on designing mobility and control of the explore robots, this paper focuses on global planning of the thorough and automatic inspection of a complex environment. We demonstrate the efficacy of the computation framework using a simulated environment, where scanned pipelines and existing leaks, clogs, and deformation can be thoroughly detected by an autonomous prototype robot. 2006 IEEE.",,
Iyengar,"Wilson Z.S., Iyengar S.S., Pang S.-S., Warner I.M., Luces C.A.","Increasing college degree attainment for students from disadvantaged backgrounds is a prominent component of numerous state and federal legislation focused on higher education. In 1999, the National Science Foundation (NSF) instituted the ""Computer Science, Engineering, and Mathematics Scholarships"" (CSEMS) program; this initiative was designed to provide greater access and support to academically talented students from economically disadvantaged backgrounds. Originally intended to provide financial support to lower income students, this NSF program also advocated that additional professional development and advising would be strategies to increase undergraduate persistence to graduation. This innovative program for economically disadvantaged students was extended in 2004 to include students from other disciplines including the physical and life sciences as well as the technology fields, and the new name of the program was Scholarships for Science, Technology, Engineering and Mathematics (S-STEM). The implementation of these two programs in Louisiana State University (LSU) has shown significant and measurable success since 2000, making LSU a Model University in providing support to economically disadvantaged students within the STEM disciplines. The achievement of these programs is evidenced by the graduation rates of its participants. This report provides details on the educational model employed through the CSEMS/S-STEM projects at LSU and provides a path to success for increasing student retention rates in STEM disciplines. While the LSU's experience is presented as a case study, the potential relevance of this innovative mentoring program in conjunction with the financial support system is discussed in detail. 2011 The Author(s).",,
Iyengar,"McDowell P., Bourgeois B.S., Sofge D.A., Iyengar S.S.","The ultimate goal of our research is to provide teams of unmanned underwater vehicles (UUVs) some of the abilities of animals to adapt to their environment using their memories, without requiring exhaustive trial-and-error testing or complex modeling of the environment. We focus on UUVs because they offer the promise of making dangerous tasks such as searching for underwater hazards or surveying the ocean bottom more safe and economical for government and commercial operations. We adopt a team concept to reduce overall mission cost using several low-cost subordinate UUVs to augment the sensor capabilities of a higher-capability lead UUV. Our goal is to develop a team of robots that would have the capability to learn their roles and improve team strategies so that the team can meet its overall goals in dynamic unstructured. Our research uses a sensor-input-based metric for success combined with a training regimen based on recently collected memories - a temporal series of sensor/action relationships - in which robots with ""ears"" listen for a leader robot and attempt to follow, and where the ensuing formations are a result of emergent behavior. 2006 IEEE.",,
Iyengar,"Trivedi N., Elangovan G., Iyengar S.S., Balakrishnan N.","Lifetime of sensor nodes determines lifetime of the network and is crucial for the sensing capability. Among the important techniques proposed for prolonging the network lifetime by exploiting redundant deployment is using hierarchical architecture or clustering. However, the primary drawback of hierarchical control is the control message overhead; it is essential that the overhead does not dominate the network operations cost. The clustering algorithm should also scale to the network sizes and the nodes that are awake at any point in time must preserve the desired sensing coverage of the entire network utilizing the redundant deployment. Also, it has been shown that in sensor networks, a fundamental tradeoff exists between energy and latency for data delivery [2]. If clustering should lead to efficient MAC and Routing protocols it could effectively address the real-time requirements posed by WSAN. The principle contributions of this paper are as follows. We propose an extremely lightweight scalable clustering algorithm for clustering in WSN. Inspired by the cellular infrastructure model, we produced clusters of bounded geographic size (given a certain node distribution density this guarantees a bound on the number of nodes in a cluster as well), handles perturbations in the network locally in space and in time. The novelty of the algorithm lies in its deterministic operation to optimally exploit the redundant deployment and produce balanced clusters while retaining the desired sensing coverage with minimum possible control overhead. Extensive simulation has been conducted and results are presented to show the superiority of our algorithm. 2006 IEEE.",,
Iyengar,"Balasubramanian M., Perkins A.L., Beuerman R.W., Iyengar S.S.","We present a fractal measure based pattern classification algorithm for automatic feature extraction and identification of fungus associated with an infection of the cornea of the eye. A white-light confocal microscope image of suspected fungus exhibited locally linear and branching structures. The pixel intensity variation across the width of a fungal element was gaussian. Linear features were extracted using a set of 2D directional matched gaussian-filters. Portions of fungus profiles that were not in the same focal plane appeared relatively blurred. We use gaussian filters of standard deviation slightly larger than the width of a fungus to reduce discontinuities. Cell nuclei of cornea and nerves also exhibited locally linear structure. Cell nuclei were excluded by their relatively shorter lengths. Nerves in the cornea exhibited less branching compared with the fungus. Fractal dimensions of the locally linear features were computed using a box-counting method. A set of corneal images with fungal infection was used to generate class-conditional fractal measure distributions of fungus and nerves. The a priori class-conditional densities were built using an adaptive-mixtures method to reflect the true nature of the feature distributions and improve the classification accuracy. A maximum-likelihood classifier was used to classify the linear features extracted from test corneal images as 'normal' or 'with fungal infiltrates', using the a priori fractal measure distributions. We demonstrate the algorithm on the corneal images with culture-positive fungal infiltrates. The algorithm is fully automatic and will help diagnose fungal keratitis by generating a diagnostic mask of locations of the fungal infiltrates.",,
Iyengar,"Suri A., Iyengar S.S., Cho E.","Wireless sensor networks have the potential to become significant subsystems of ecological experiment. Sensor networks consist of large number of tiny sensor nodes, all of which have sensing capabilities. These networks allow coordinated signal detection, monitoring, and tracking to enable sensor nodes to simultaneously capture geographically distinct measurements. Sensor nodes do not require predetermined positioning making such networks especially useful for applications in remote, inhospitable environments. In this paper we have tried to see the various ecological experimental scenarios, and how wireless sensor networks can be used in that field. One of the most challenging bottlenecks in the usage of wireless sensor networks in large scale experiments is the energy constraint. Various routing protocols which have tried to optimize the energy usage are also studied in the paper. 2006 Elsevier B.V. All rights reserved.",,
Iyengar,"Rishel T., Perkins A.L., Yenduri S., Zand F., Iyengar S.S.",We consider the improvement in accuracy of latent semantic analysis when a part of speech tagger is used to augment a term/document matrix. We first construct an augmented term/document matrix as input into singular value decomposition (SVD). The singular values then serve as principal components for a cosine projection. The results show that the addition of POS tags can decrease ambiguities significantly.Lisetti,,
Lisetti,"Pan Z., Polceanu M., Lisetti C.","Real time user independent facial expression recognition is important for virtual agents but challenging. However, since in real time recognition users are not necessarily presenting all the emotions, some proposed methods are not applicable. In this paper, we present a new approach that instead of using the traditional base face normalization on whole face shapes, performs normalization on the point cloud of each landmark. The result shows that our method outperforms the other two when the user input does not contain all six universal emotions. Springer International Publishing AG 2016.",,
Lisetti,"LeRouge C., Dickhut K., Lisetti C., Sangameswaran S., Malasanos T.","Objective This research focuses on the potential ability of animated avatars (a digital representation of the user) and virtual agents (a digital representation of a coach, buddy, or teacher) to deliver computer-based interventions for adolescents' chronic weight management. An exploration of the acceptance and desire of teens to interact with avatars and virtual agents for self-management and behavioral modification was undertaken. Materials and Methods The utilized approach was inspired by community-based participatory research. Data was collected from 2 phases: Phase 1) focus groups with teens, provider interviews, parent interviews; and Phase 2) mid-range prototype assessment by teens and providers. Results Data from all stakeholder groups expressed great interest in avatars and virtual agents assisting self-management efforts. Adolescents felt the avatars and virtual agents could: 1) reinforce guidance and support, 2) fit within their lifestyle, and 3) help set future goals, particularly after witnessing the effect of their current behavior(s) on the projected physical appearance (external and internal organs) of avatars. Teens wanted 2 virtual characters: A virtual agent to act as a coach or teacher and an avatar (extension of themselves) to serve as a ""buddy"" for empathic support and guidance and as a surrogate for rewards. Preferred modalities for use include both mobile devices to accommodate access and desktop to accommodate preferences for maximum screen real estate to support virtualization of functions that are more contemplative and complex (e.g., goal setting). Adolescents expressed a desire for limited co-user access, which they could regulate. Data revealed certain barriers and facilitators that could affect adoption and use. Discussion The current study extends the support of teens, parents, and providers for adding avatars or virtual agents to traditional computerbased interactions. Data supports the desire for a personal relationship with a virtual character in support of previous studies. The study provides a foundation for further work in the area of avatar-driven motivational interviewing. Conclusions This study provides evidence supporting the use of avatars and virtual agents, designed using participatory approaches, to be included in the continuum of care. Increased probability of engagement and long-term retention of overweight, obese adolescent users and suggests expanding current chronic care models toward more comprehensive, socio-technical representations. The Author 2015.",,
Lisetti,"Even C., Bosser A.-G., Ferreira J.F., Buche C., Stephan F., Cavazza M., Lisetti C.","Social skills training (SST) has recently emerged as a typical application for emotional conversational agents (ECA). While a number of prototypes have targeted the general population, fewer have been used for psychiatric patients despite the widely recognised potential of ECAs technologies in the field of mental health. Social cognition impairment is however a widely shared symptom in psychiatric patients suffering from pathologies such as schizophrenia. Going further than SST, rehabilitation programmes involving role-play, but also problem solving have been successfully used by clinicians, drastically improving the quality of life of patients suffering from such disabilities. One of the challenges of these programmes is to ensure that the patients will be able to adapt their behavior when the situation varies, rather than training them with the appropriate behavior for a set of specific situations. In this paper, we describe a novel approach for the development of a serious game supporting rehabilitation programmes for social skills, which will primarily target schizophrenia patients. We propose to use an ECA in combination with a narrative generation engine issued from interactive storytelling research to provide varied situations. This approach reflects the combination of both role-play and problem solving exercises on which remediation therapies rely, and has the potential to support patient's progress and motivation through the rehabilitation programme. 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,
Lisetti,"Amini R., Lisetti C., Ruiz G.","With the growing number of researchers interested in modeling the inner workings of affective social intelligence, the need for tools to easily model its associated expressions has emerged. The goal of this article is two-fold: 1) we describe HapFACS, a free software and API that we developed to provide the affective computing community with a resource that produces static and dynamic facial expressions for three-dimensional speaking characters; and 2) we discuss results of multiple experiments that we conducted in order to scientifically validate our facial expressions and head animations in terms of the widely accepted Facial Action Coding System (FACS) standard, and its Action Units (AU). The result is that users, without any 3D-modeling nor computer graphics expertise, can animate speaking virtual characters with FACS-based realistic facial expression animations, and embed these expressive characters in their own application(s). The HapFACS software and API can also be used for generating repertoires of realistic FACS-validated facial expressions, useful for testing emotion expression generation theories. 2015 IEEE.",,
Lisetti,"Yasavur U., Lisetti C., Rishe N.","We developed a virtual counseling system which can deliver brief alcohol health interventions via a 3D anthropomorphic speech-enabled interfaceãa new field for spoken dialog interactions with intelligent virtual agents in the health domain. We present our spoken dialog system design and its evaluation. We developed our dialog system based on Markov decision processes framework and optimized it by using reinforcement learning algorithms with data we collected from real user interactions. The system begins to learn optimal dialog strategies for initiative selection and for the type of confirmations that it uses during the interaction. We compared the unoptimized system with the optimized system in terms of objective measures (e.g. task completion) and subjective measures (e.g. ease of use, future intention to use the system) and obtained positive results. 2014, OpenInterface Association.",,
Lisetti,"Amini R., Lisetti C., Yasavur U.","In this paper, we discuss a novel approach to design an emotionally responsive system in the context of virtual health interventions for behavior change. We describe the system's design with a focus on enabling a multimodal Embodied Conversational Agent (ECA) to deliver the interventions empathetically. This is done by adapting its verbal and non-verbal behavior, in real-time, to those of the clients. Our current approach is based on a successful existing patient-centered intervention for behavior change - the Drinker's Check-Up (DCU). Although, the DCU uses a text-only web interface, it has been reported to reduce alcohol consumption in problem drinkers. We discuss the results of users' evaluation of the DCU intervention compared to the same intervention delivered with empathic and non-empathic ECAs. Results show that, the empathic virtual counselor has better acceptance than the other two systems. 2014 Springer International Publishing.",,
Lisetti,"Yasavur U., Lisetti C., Rishe N.",We focus on creating a programming tool which enables to create dialog managers for speech-enabled IVA applications in the standardized health screening and assessment domain. Our approach aims to bridge the gap between the intelligent virtual agents (IVA) and the spoken dialog systems (SDS) research communities for the delivery of standardized health interviews by embodied conversational agents (ECA). 2014 Springer International Publishing Switzerland.,"Shahid A.R., Pissinou N., Iyengar S.S., Makki K.","In Location-Based Social Networks (LBSNs) users' interaction is largely carried through check-ins and photo sharing. The study of location privacy issues with check-ins has yielded different Location Privacy-Preserving Mechanisms (LPPMs), including dummies to hide user's whereabouts in a set of dummy locations without using a trusted-third party(TTP). However, the impact of shared photos on location privacy is yet to be understood. Our experiment on real data from a LBSN reveals that, like the check-ins, the spatial distribution of shared photos can influence user's location privacy. In this paper, we propose an inference model based on spatial distribution of historical check-ins and photos, and show that it is possible to deduce user's location at a high accuracy through spatiotemporal analysis of multiple events, comprising check-ins and photos. In the process, we evaluate state-of-the-art dummy mechanisms with the proposed inference model to stress the limitation of existing LPPMs. Then, we design a LPPM, called photo-check, to protect user privacy in LBSN for both check-in and photos; and carry out the experiment with real data from Foursquare to show its effectiveness and efficacy. 2018 IEEE."
Lisetti,"Yasavur U., Travieso J., Lisetti C., Rishe N.","There has recently been growing interest in valence and emotion sensing using a variety of signals. Text, as a communication channel, gathers a substantial amount of interest for recognizing its underlying sentiment (valence or polarity), affect or emotion (e.g. happy, sadness). We consider recognizing the valence of a sentence as a prior task to emotion sensing. In this article, we discuss our approach to classify sentences in terms of emotional valence. Our supervised system performs syntactic and semantic analysis for feature extraction. Our system processes the interactions between words in sentences using dependency parse trees, and it can identify the current polarity of named-entities based on on-the-fly topic modeling. We compared the performance of three rule-based approaches and two supervised approaches (i.e. Naive Bayes and Maximum Entropy).We trained and tested our system using the SemEval-2007 affective text dataset, which contains news headlines extracted from news websites. Our results show that our systems outperform the systems demonstrated in SemEval-2007. Copyright 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,
Lisetti,"Abeyruwan S., Baral R., Yasavur U., Lisetti C., Visser U.","We combined a spoken dialog system that we developed to deliver brief health interventions with the fully aut onomous humanoid robot (NAO). The dialog system is based on a framework facilitating Markov decision processes (MDP). It is optimized using reinforcement learning (RL) algorithms with data we collected from real user interactions. The system begins to learn optim al dialog strategies for initiative selection and for the type of confirmations that it uses during the interaction. The health intervention, delivered by a 3D character ins tead of the NAO, has already been evaluated, with posit ive results in terms of task completion, ease of use, and future intention to use the system. The current spoken dialog system for the humanoid robot is a novelty and exists so far as a proof of concept. Copyright 2014, Association for the Advancement of Artificial Intelligence.",,
Lisetti,"Yasavur U., Amini R., Lisetti C., Rishe N.","Named-Entity Recognizers (NERs) are an important part of information extraction systems in annotation tasks. Although substantial progress has been made in recognizing domain-independent named entities (e.g. location, organization and person), there is a need to recognize named entities for domain-specific applications in order to extract relevant concepts. Due to the growing need for smart health applications in order to address some of the latest worldwide epidemics of behavioral issues (e.g. over eating, lack of exercise, alcohol and drug consumption), we focused on the domain of behavior change, especially lifestyle change. To the best of our knowledge, there is no named-entity recognizer designed for the lifestyle change domain to enable applications to recognize relevant concepts. We describe the design of an ontology for behavioral health based on which we developed a NER augmented with lexical resources. Our NER automatically tags words and phrases in sentences with relevant (lifestyle) domain-specific tags (e.g. [un/]healthy food, potentially-risky/healthy activity, drug, tobacco and alcoholic beverage). We discuss the evaluation that we conducted with with manually collected test data. In addition, we discuss how our ontology enables systems to make further information acquisition for the recognized named entities by using semantic reasoners. Copyright 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,
Lisetti,"Amini R., Lisetti C.","We present HapFACS (ver. beta), a new open source software and API for generating FACS-based facial expressions on 3D virtual characters that have accompanying lip-synchronized animation abilities. HapFACS has two main usage scenarios: First, with the HapFACS software, users can generate repertoires of realistic FACS-validated facial expressions, either as static images or as videos, Second, with the accessible HapFACS API, users can animate speaking virtual characters with real-time realistic facial expressions, and embed these expressive characters in their own application(s) without any prior experience in computer graphics and modeling. We describe how HapFACS (1) provides control over 49 FACS Action Units at all levels of intensity, (2) enables the animation of faces with a single AU or a composition of AUs, activated unilaterally or bilaterally, and (3) can be applied to any supported character in the underlying 3D-character system. Finally, we provide details of evaluation experiments we conducted with FACS-certified scorers to validate the facial expressions generated by HapFACS. 2013 IEEE.",,
Lisetti,"Amini R., Lisetti C., Yasavur U., Rishe N.","In this paper, we discuss a novel approach for the computer-delivery of Brief Motivational Interventions (BMIs) for health behavior change. We describe the basic elements of our system architecture, and focus on enabling a multimodal Embodied Conversational Agent (ECA) to deliver the health behavior change interventions empathetically by adapting, in real-time, its verbal and non-verbal communication messages to those of its clients. The designed empathy model integrates a cognitive component and an affective components. We then discuss the evaluation experiment that we designed and conducted to evaluate the impact of empathy model on users' experience with the empathic character. Results indicate that, in comparison with the non-empathic counselor, the empathic one is better accepted (e.g., more enjoyable, empathizing, engaging, and likable) and some users might be willing to disclose more private information (e.g., drinking habits) to the counselor endowed with empathic abilities than the one without. 2013 IEEE.",,
Lisetti,"Lisetti C., Amini R., Yasavur U., Rishe N.","We discuss our approach to developing a novel modality for the computer-delivery of Brief Motivational Interventions (BMIs) for behavior change in the form of a personalized On-Demand VIrtual Counselor (ODVIC), accessed over the internet. ODVIC is a multimodal Embodied Conversational Agent (ECA) that empathically delivers an evidence-based behavior change intervention by adapting, in real-time, its verbal and nonverbal communication messages to those of the user's during their interaction. We currently focus our work on excessive alcohol consumption as a target behavior, and our approach is adaptable to other target behaviors (e.g., overeating, lack of exercise, narcotic drug use, non-adherence to treatment). We based our current approach on a successful existing patient-centered brief motivational intervention for behavior change-the Drinker's Check-Up (DCU)-whose computer-delivery with a text-only interface has been found effective in reducing alcohol consumption in problem drinkers. We discuss the results of users' evaluation of the computer-based DCU intervention delivered with a text-only interface compared to the same intervention delivered with two different ECAs (a neutral one and one with some empathic abilities). Users rate the three systems in terms of acceptance, perceived enjoyment, and intention to use the system, among other dimensions. We conclude with a discussion of how our positive results encourage our long-term goals of on-demand conversations, anytime, anywhere, with virtual agents as personal health and well-being helpers. 2013 ACM.",,
Lisetti,"Yasavur U., Lisetti C., Rishe N.","This paper describes the design of a multimodal spoken dialogue system using Markov Decision Processes (MDPs) to enable embodied conversational virtual health coach agents to deliver brief interventions for lifestyle behavior change - in particular excessive alcohol consumption. Its contribution is two fold. First, it is the first attempt to-date to study stochastic dialogue policy optimization techniques in the health dialogue domain. Second, it provides a model for longer branching dialogues (in terms of number of dialogue turns and number of slots) than the usual slot filling dialogue interactions currently available (e.g. tourist information domain). In addition, the model forms the basis for the generation of a richly annotated dialogue corpus, which is essential for applying optimization methods based on reinforcement learning. 2013 Springer-Verlag.",,
Lisetti,"Amini R., Yasavur U., Lisetti C.","In this article, we present HapFACS 1.0, a new software/API for generating static and dynamic three-dimensional facial expressions based on the Facial Action Coding System (FACS). HapFACS provides total control over the FACS Action Units (AUs) activated at all levels of intensity. HapFACS allows generating faces with an individual AU or composition of AUs activated unilaterally or bilaterally with different intensities. The reliable and emotionally valid facial expressions can be generated on infinite number of faces in different ethnicities, genders, and ages using HapFACS to be used in numerous scientific areas including psychology, emotion, FACS learning, clinical, and neuroscience research. 2012 Authors.",,
Lisetti,"Yasavur U., Amini R., Lisetti C.","In this paper, we have proposed a user model for computer based drinking behavior change intervention and recommender systems. We discuss specific requirements of user modeling in health promotion and specifically alcohol interventions. We believe that making behavior change systems available pervasively may lead to better and sustainable results. Therefore, our proposed user model takes advantage of the target-behavior related features such as contextual features (e.g., social interactions, location, and time). The proposed user model uses well-validated questionnaires to capture target-behavior specific aspects. We also introduced approaches for enhancing users' experience in the model creation stage by using Embodied Conversational Agents(ECAs) and users' affective states.",,
Lisetti,"Lisetti C., Yasavur U., De Leon C., Amini R., Rishe N., Visser U.","We discuss the design and implementation of the prototype of an avatar-based health system aimed at providing people access to an effective behavior change intervention which can help them to find and cultivate motivation to change unhealthy lifestyles. An empathic Embodied Conversational Agent (ECA) delivers the intervention. The health dialog is directed by a computational model of Motivational Interviewing, a novel effective face-to-face patient-centered counseling style which respects an individual's pace toward behavior change. Although conducted on a small sample size, results of a preliminary user study to asses users' acceptance of the avatar counselor indicate that the system prototype is well accepted by 75% of users. Copyright 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,
Lisetti,"Lisetti C.L., Yasavur U., Visser U., Rishe N.","In this article we describe work-in-progress about the development of avatar-based personalized assistants that can delivered motivational interviewing health behavior change interventions, tailored to its specific users Our approach combines the latest progress in Embodied Conversational Agents (ECAs), believable agents, and dialog systems. We discuss how we use different platforms to aim at providing accessibility of personalized health assistant, anytime anywhere. 2011 ICST.",,
Lisetti,Lisetti C.L.,"In this article we review some of the main contributions that the believable agents community has brought about and we review some of the main believable agent architectures existing to date. We also discuss the enormous potential that believable agents can bring in the domain of health, including health communication, health promotion, health counseling and psychotherapy. We discuss some of our work-in-progress aimed at building believable agents with dialog abilities to conduct supportive healthcare interventions for the management of chronic diseases and addiction issues. 2011 Springer-Verlag.",,
Lisetti,"Horswill I., Lisetti C.","Artificial intelligence generally focuses on engineering rationality. While rational agents are undoubtedly useful, we argue that computational methods are also useful for modeling aspects of human behavior that fall short of perfect rationality. And moreover, that for certain kinds of applications, this is preferable to perfect rationality. 2011 The authors and IOS Press. All rights reserved.",,
Lisetti,"Nasoz F., Lisetti C.L., Vasilakos A.V.","In this article, we describe a new approach to enhance driving safety via multi-media technologies by recognizing and adapting to drivers' emotions with multi-modal intelligent car interfaces. The primary objective of this research was to build an affectively intelligent and adaptive car interface that could facilitate a natural communication with its user (i.e., the driver). This objective was achieved by recognizing drivers' affective states (i.e., emotions experienced by the drivers) and by responding to those emotions by adapting to the current situation via an affective user model created for each individual driver. A controlled experiment was designed and conducted in a virtual reality environment to collect physiological data signals (galvanic skin response, heart rate, and temperature) from participants who experienced driving-related emotions and states (neutrality, panic/fear, frustration/anger, and boredom/sleepiness). k-Nearest Neighbor (KNN), Marquardt-Backpropagation (MBP), and Resilient Backpropagation (RBP) Algorithms were implemented to analyze the collected data signals and to find unique physiological patterns of emotions. RBP was the best classifier of these three emotions with 82.6% accuracy, followed by MBP with 73.26% and by KNN with 65.33%. Adaptation of the interface was designed to provide multi-modal feedback to the users about their current affective state and to respond to users' negative emotional states in order to decrease the possible negative impacts of those emotions. Bayesian Belief Networks formalization was employed to develop the user model to enable the intelligent system to appropriately adapt to the current context and situation by considering user-dependent factors, such as personality traits and preferences. 2010 Elsevier Inc. All rights reserved.",,
Lisetti,"Verhoef T., Lisetti C., Barreto A., Ortega F., Van Der Zant T., Cnossen F.","In this article, we address some of the issues concerning emotion recognition from processing physiological signals captured by bio-sensors. We discuss some of our preliminary results, and propose future directions for emotion recognition based on our lessons learned. 2009 Springer Berlin Heidelberg.",,
Lisetti,Lisetti C.,We explore how avatars can be used as social orthotics defined as therapeutic computer-based social companions aimed at promoting healthy behaviors. We review some of the health interventions deployed in helping at-risk populations along with some of the unique advantages that computer-based interventions can add to face-to-face interventions. We posit that artificial intelligence has rendered possible the creation of culturally appropriate dialogagents for interventions and we identify specific features for social avatars that are important - if not necessary - when applied to the domain of social orthotic systems for health promotion. 2009 The Interactive Media Institute and IOS Press. All rights reserved.,,
Lisetti,"Lisetti C., Pozzo E., Lucas M., Hernandez F., Silverman W., Kurtines B., Pasztor A.","We give an overview of the Second Life (SL) virtual world, explaining what objects can be manipulated to implement user-defined scenari in SL, and give an example of a work-in-progress scenario of exposure therapy for anxiety disorders, coupled with automatic processing of bio-sensed emotional signals. Both e-Health and emotion recognition researchers can benefit from such experiments. 2009 The Interactive Media Institute and IOS Press. All rights reserved.",,
Lisetti,"Sadjadi S.M., Fong L., Badia R.M., Figueroa J., Delgado J., Collazo-Mojica X.J., Saleem K., Rangaswami R., Shimizu S., Limon H.A.D., Welsh P., Pattnaik S., Praino A., Villegas D., Kalayci S., Dasgupta G., Ezenwoye O., Martinez J.C., Rodero I., Chen S., Mu_oz J., Lopez D., Corbalan J., Willoughby H., McFail M., Lisetti C., Adjouadi M.","The impact of hurricanes is so devastating throughout different levels of society that there is a pressing need to provide a range of users with accurate and timely information that can enable effective planning for and response to potential hurricane landfalls. The Weather Research and Forecasting (WRF) code is the latest numerical model that has been adopted by meteorological services worldwide. The current version of WRF has not been designed to scale out of a single organization's local computing resources. However, the high resource requirements of WRF for fine-resolution and ensemble forecasting demand a large number of computing nodes, which typically cannot be found within one organization. Therefore, there is a pressing need for the Grid-enablement of the WRF code such that it can utilize resources available in partner organizations. In this paper, we present our research on Grid enablement of WRF by leveraging our work in transparent shaping, GRID superscalar, profiling, code inspection, code modeling, meta-scheduling, and job flow management. Copyright 2008 ACM.",,
Lisetti,"Lisetti C.L., Wagner E.","In this article, we explore the possibility of using animated characters as personal social companions for supporting interventions for promoting health behaviors. We explore how supportive feedback could be provided to users of such artificial companion systems, by coupling both personalized intervention content from a mental health perspective with personalized affective social agents such as graphical facial avatars or Embodied Conversational Agents (ECAs). We discuss the issues and potential of such an approach. Copyright 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,
Lisetti,"Hudlicka E., Lisetti C., Hodge D., Paiva A., Rizzo A., Wagner E.","We have outlined a number of questions that should be addressed as we explore the emerging research and practice area of agent-augmented virtual environments for psychotherapy. The questions addressed theoretical, technological and 'cultural' challenges, the latter referring to issues of acceptance of these technologies by the clinical community. The panelists' statements discussed progress toward some of these goals. The panelists also discussed a broad range of issues that need be explored, to establish both the development guidelines for synthetic therapeutic agents, and the evaluation and validation criteria for their effectiveness. We should emphasize that we are not suggesting that synthetic agents should replace experienced clinicians. However, existing research indicates that humans can, and do, establish affective relationships with agents (e.g., Brave & Nass, 2003), and it is on this premise that the potential of these technologies in psychotherapy should continue to be explored. Copyright 2008. Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,
Lisetti,"Balduccini M., Baral C., Brodaric B., Colton S., Fox P., Gutelius D., Hinkelman K., Horswill I., Huberman B., Hudlicka E., Lerman K., Lisetti C., McGuinness D., Maher M.L., Musen M.A., Sahami M., Sleeman D., Th_nssen B., Velasquez J., Ventura D.","The Association for the Advancement of Artificial Intelligence (AAAI) was pleased to present the AAAI 2008 Spring Symposium Series, held Wednesday through Friday, March 26-28, 2008, at Stanford University, California. The eight symposia were titled (1) AI Meets Business Rules and Process Management, (2) Architectures for Intelligent Theory-Based Agents, (3) Creative Intelligent Systems, (4) Emotion, Personality, and Social Behavior, (5) Semantic Scientific Knowledge Integration, (6) Social Information Processing, (7) Symbiotic Relationships between Semantic Web and Knowledge Engineering, (8) Using Al to Motivate Greater Participation in Computer Science. The goal of the AI Meets Business Rules and Process Management AAAI symposium was to investigate the various approaches and standards to represent business rules, business process management, and the semantic web with respect to expressiveness and reasoning capabilities. The focus of the Architectures ror Intelligent Theory-Based Agents AAAI symposium was the definition or architectures for intelligent theory-based agents, comprising languages, knowledge representation methodologies, reasoning algorithms, and control loops. The Creative Intelligent Systems symposium included five major discussion sessions and a general poster session (in which all contributing papers were presented). The purpose of this symposium was to explore the synergies between creative cognition and intelligent systems. The goal of the Emotion, Personality, and Social Behavior symposium was to examine fundamental issues in affect and personality in both biological and artificial agents, focusing on the roles of these factors in mediating social behavior. The Semantic Scientific Knowledge Integration symposium brought together the semantic technologies community with the scientific information technology community in an effort to build the general semantic science information community. The Social Information Processing symposium's goal was to investigate computational and analytic approaches that will enable users to harness the efforts of large numbers of other users to solve a variety of information processing problems, from discovering high-quality content to managing common resources. The goal of the Symbiotic Relationships between the Semantic Web and Software Engineering symposium was to explore how the lessons learned by the knowledge-engineering community over the past three decades could be applied to the bold research agenda of current workers in semantic web technologies. The purpose of the Using AI to Motivate Greater Participation in Computer Science symposium was to identify ways that topics in AI may be used to motivate greater student participation in computer science by highlighting fun, engaging, and intellectually challenging developments in the AI-related curriculum at a number of educational levels. Technical reports of the symposia were published by AAAI Press. Copyright 2008, Association for the Advancement of Artificial Intelligence. All rights reserved.",,
Lisetti,"Paleari M., Grizard A., Lisetti C.","Starting from the assumptions that human-ambient intelligence interaction will be improved by having more human-human like communications and that facial expressions are fundamental in human-human communications, we describe how we developed facial expressions for artificial agents based on a psychological theory developed by Scherer. In this current article we describe briefly the psychological theory that we have chosen as well as some of the reason for adopting it. We then describe the two different platforms we used, the Cherry avatar and the iCat Robot with a particular focus on the robot. Finally we explore the steps needed to adapt the psychological theory to the two different platforms and we present some conclusions and future development. Copyright 2007, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",,
Lisetti,"Villon O., Lisetti C.","The interpretation of physiological signals in terms of emotion requires an appropriate mapping between physiological features and emotion representations. We present a user model associating psychological and physiological representation of emotion in order to bring findings from the psychophysiology domain into User-Modeling computational techniques. We discuss results based on an experiment we performed based on bio-sensors to get physiological measure of emotion, involving 40 subjects. Springer-Verlag Berlin Heidelberg 2007.",,
Lisetti,"Nasoz F., Lisetti C.L.","In this paper we describe the User Modeling phase of our general research approach: developing Adaptive Intelligent User Interfaces to facilitate enhanced natural communication during the Human-Computer Interaction. Natural communication is established by recognizing users' affective states (i.e., emotions experienced by the users) and responding to those emotions by adapting to the current situation via an affective user model. Adaptation of the interface was designed to provide multi-modal feedback to the users about their current affective state and to respond to users' negative emotional states in order to compensate for the possible negative impacts of those emotions. Bayesian Belief Networks formalization was employed to develop the User Model to enable the intelligent system to appropriately adapt to the current context and situation by considering user-dependent factors, such as: personality traits and preferences. Springer-Verlag Berlin Heidelberg 2007.",,
Lisetti,"Lisetti C.L., Marpaung A.","In this article, we propose the design of sensory motor level as part of a three-layered agent architecture inspired from the Multilevel Process Theory of Emotion (Leventhal 1979, 1980; Leventhal and Scherer, 1987). Our project aims at modeling emotions on an autonomous embodied agent, a more robust robot than our previous prototype. Our robot has been equipped with sonar and vision for obstacle avoidance as well as vision for face recognition, which are used when she roams around the hallway to engage in social interactions with humans. The sensory motor level receives and processes inputs and produces emotion-like states without any further willful planning or learning. We describe: (1) the psychological theory of emotion which inspired our design, (2) our proposed agent architecture, (3) the needed hardware additions that we implemented on the commercialized ActivMedia's robot, (4) the robot's multi-modal interface designed especially to engage humans in natural (and hopefully pleasant) social interaction, and finally (5) our future research efforts. Springer-Verlag Berlin Heidelberg 2007.",,
Lisetti,"Villon O., Lisetti C.","During last decade, an increasing interest for interpreting users' emotional subjective experience on the basis of physiological signals has led to various approaches. In this article, we focus on two different approaches toward emotion recognition: (1) the user-dependency of psycho-physiological data collected (i.e. do we choose to keep track of the specificity of individuals ' responses or do we ignore such specificity), and (2) the degree of subjectivity of the stimuli used to elicit emotions (i.e. stimuli with high level of agreement in terms of what emotional experience they elicit among a population can be chosen versus stimuli without such an agreement). In order to assess the implications of adopting one of these methodologies on the personalization of emotion recognition from physiological signals we present our empirical results for emotion recognition from physiological signals based on an experiment involving 40 subjects. We conclude by proposing requirements for any chosen approach to achieve suitable online emotion recognition, in an out-of-the-lab context (e.g. interactive art, e-Health application). 2007 IEEE.",,
Lisetti,"Lisetti C.L., Nasoz F.","In this article, we discuss the strong relationship between affect and cognition and the importance of emotions in Multimodal Human Computer Interaction (HCI) and User-Modeling. We introduce the overall paradigm for our multimodal system that aims at recognizing its users' emotions and at responding to them accordingly depending upon the current context or application. We then describe the design of the emotion elicitation experiment we conducted by collecting, via wearable computers, physiological signals from the autonomic nervous system (galvanic skin response, heart rate, temperature) and mapping them to certain emotions (Sadness, Anger, Fear, Surprise, Frustration, and Amusement). We show the results of three different supervised learning algorithms that categorize these collected signals in terms of emotions, and generalize their learning to recognize emotions from new collections of signals. We finally discuss possible broader impact and possible applications of emotion recognition for multimodal intelligent systems. 2006 IEEE.",,
Lisetti,"Villon O., Lisetti C.","Near to real-time emotion recognition is a promising task for Human-Computer Interaction (HCI) and HumanRobot Interaction (HRI). Using knowledge about the user's emotions depends upon the possibility to extract information about users' emotions during HCI or HRI without explicitely asking users about the feelings they are experiencing. To be able to sense the user's emotions without interrupting the HCI, we present a new method applied to the emotional experience of the user for extracting semantic information from the Autonomic Nervous System (ANS) signals associated with emotions. We use the concepts of 1st person - where the subject consciously (and subjectively) extracts the semantic meaning of a given lived experience, e.g. 'I felt amused') - and 3rd person approach - where the experimenter interprets the semantic meaning of the subject's experience from a set of externally (and objectively) measured variables (e.g. galvanic skin response measures) - Based on the 3rd person approach, our technique aims at psychologically interpreting physiological parameters (skin conductance and heart rate), and at producing a continuous extraction of the user's affective state during HCI or HRI. We also combine it with the 1st person approach measure which allows a tailored interpretation of the physiological measure closely related to the user own emotional experience. ©2006 IEEE.",,
Lisetti,"Paleari M., Lisetti C.L.","During face to face communication, it has been suggested that as much as 70% of what people communicate when talking directly with others is through paralanguage involving multiple modalities combined together (e.g. voice tone and volume, body language). In an attempt to render humancomputer interaction more similar to human-human communication and enhance its naturalness, research on sensory acquisition and interpretation of single modalities of human expressions have seen ongoing progress over the last decade. These progresses are rendering current research on artificial sensor fusion of multiple modalities an increasingly important research domain in order to reach better accuracy of congruent messages on the one hand, and possibly to be able to detect incongruent messages across multiple modalities (incongruency being itself a message about the nature of the information being conveyed). Accurate interpretation of emotional signals -quintessentially multimodal - would hence particularly benefit from multimodal sensor fusion and interpretation algorithms. In this paper we provide a state of the art multimodal fusion and describe one way to implement a generic framework for multimodal emotion recognition. The system is developed within the MAUI framework [31]and Scherer's Component Process Theory (CPT) [49, 50, 51, 24, 52], with the goal to be modular and adaptive. We want the designed framework to be able to accept different single and multi modality recognition systems and to automatically adapt the fusion algorithm to find optimal solutions. The system also aims to be adaptive to channel (and system) reliability. Copyright 2006 ACM.",,
Lisetti,"Nasoz F., Lisetti C.L.",In this paper we describe the multimodal affective user interface (MAUI) we created to capture its users' emotional physiological signals via wearable computers and visualize the categorized signals in terms of recognized emotion. MAUI aims at (1) giving feedback to the users about their emotional states via various modalities (e.g. mirroring the users' facial expressions and describing verbally the emotional state via an anthropomorphic avatar) and (2) animating the avatar's facial expressions based on the users' captured signals. We first describe a version of MAUI which we developed as an in-house research tool for developing and testing affective computing research. We also discuss applications for which building intelligent user interfaces similar to MAUI can be useful and we suggest ways of adapting the MAUI approach to fit those specific applications. 2006 Elsevier Ltd. All rights reserved.,,
Lisetti,"LeRouge C., Lisetti C.","This study exemplifies the integration of information systems (IS) behavioural science in the area of technology adoption and diffusion into the design science paradigm. We first present a research framework for triangulating design and behavioural science paradigms. From the design science perspective, we introduce an intelligent interface (Model Of User's Emotions - MOUE) aimed at discerning emotional state from processing sensory modalities input. We contextualise MOUE within the tele-home healthcare setting as a means to provide the caregivers with an assessment of the patient's emotional state. We use an IS adoption model in an exploratory field study as theoretical foundation to integrate behavioural science into the design science process of adapting MOUE to the context. Data analysis indicates that future iterations of MOUE user interfaces for the tele-home health context should augment rather than replace the existing processes and interfaces. Data also indicates efforts related to adoption and implementation should address anxiety regarding artificial intelligence. To address the prototype phase of the research framework, we propose future work to expand the application of 'Wizard of Oz' type studies in which researchers simulate system interaction with subjects who believe they are interacting with the system to afford realism in prototypical experimentation. Copyright 2006 Inderscience Enterprises Ltd.",,
Lisetti,"Lisetti C.L., Marpaung A.","In this article, we propose the design of a three-layered agent architecture inspired from the Multilevel Process Theory of Emotion (Leventhal and Scherer, 1987). Our project aims at modeling emotions on an autonomous embodied robotic agent, expanding upon our previous work (Lisetti, et al., 2004). Our agent is designed to socially interact with humans, navigating in an office suite environment, and engaging people in social interactions. We describe: (1) the psychological theory of emotion which inspired our design, (2) our proposed agent architecture, (3) the needed hardware additions that we implemented on a robot, (3) the robot's multi-modal interface designed especially to engage humans in natural (and hopefully pleasant) social interactions. Springer-Verlag Berlin Heidelberg 2005.",,
Lisetti,"Marpaung A.H., Lisetti C.L.","In this article, we propose the design of sensory motor level as part of a three-layered agent architecture inspired from the Multilevel Process Theory of Emotion (Leventhal 1979, 1980; Leventhal and Scherer, 1987). Our project aims at modeling emotions on an autonomous embodied agent, Petra, a more robust robot than our previous prototype - Cherry. Our robot has been equipped with sonar and vision for obstacle avoidance as well as vision for face recognition, which are used when she roams around the hallway to engage in social interactions with humans. The sensory motor level receives and processes inputs and produces emotion-like states without any further willful planning or learning. We describe: (1) the psychological theory of emotion which inspired our design, (2) our proposed agent architecture, (3) the needed hardware additions that we implemented on the commercialized ActivMedia's robot, (4) Petra's multi-modal interface designed especially to engage humans in natural (and hopefully pleasant) social interaction, and finally (5) our future research efforts.",,
Lisetti,"Lisetti C., LeRouge C.","This study exemplifies the integration of IS behavioral science in the area of technology adoption and diffusion into the design science process. We first identify the computer-mediated paradox, as it exists in the tele-home health care setting. Specifically, we address the challenges of providing quality patient inclusive of affective assessment. From the design science perspective, we then introduce an intelligent interface (MOUE) aimed at discerning emotional state from processing sensory modalities (or modes) input via various media and building (or encoding) a model of the user's emotions. We contextualize MOUE within the tele-home health care setting to provide the health care provider with an easy-to-use and useful assessment of the patient's emotional state in order to facilitate patient care. We then use an IS adoption model developed and tested in the general telemedicine context in a qualitative exploratory manner as a means to inform design science regarding adapting affective state output in consideration of tele-home health adoption and diffusion issues. Based upon this integrative exploration, we propose to expand the application of ""Wizard of Oz"" type studies [1] to computer-mediated communication (CMC) environments to investigate how emotional state assessments influence responses from health care professionals and how MOUE can be accepted into the health care environment.",,
Lisetti,"Cami A., Lisetti C., Sierhuis M.","Recent psychological theories of emotion have explicitly focused on modeling the multiplicity of levels of the human emotion system. This increased interest in creating multi-level theories of emotion matches similar efforts in the area of cognitive architectures. In this paper we report our initial work toward integrating a psychological multi-level model of emotions with Brahms, a multi-agent system that is used to model and simulate work practice.",,
Lisetti,"Lisetti C.L., Brown S.M., Alvarez K., Marpaung A.H.","The development of an autonomous social robot, Cherry, is occurring in tandem with studies gaining potential user preferences, likes, dislikes, and perceptions of her features. Thus far, results have indicated that individuals 1) believe that service robots with emotion and personality capabilities would make them more acceptable in everyday roles in human life, 2) prefer that robots communicate via both human-like facial expressions, voice, and text-based media, 3) become more positive about the idea of service and social robots after exposure to the technology, and 4) find the appearance and facial features of Cherry pleasing. The results of these studies provide the basis for future research efforts, which are discussed.",,
Lisetti,"Lisetti C., Nasoz F., LeRouge C., Ozyer O., Alvarez K.","Accounting for a patient's emotional state is integral in medical care. Tele-health research attests to the challenge clinicians must overcome in assessing patient emotional state when modalities are limited (J. Adv. Nurs. 36(5) 668). The extra effort involved in addressing this challenge requires attention, skill, and time. Large caseloads may not afford tele-home health-care (tele-HHC) clinicians the time and focus necessary to accurately assess emotional states and trends. Unstructured interviews with experienced tele-HHC providers support the introduction of objective indicators of patients' emotional status in a useful form to enhance patient care. We discuss our contribution to addressing this challenge, which involves building user models not only of the physical characteristics of users - in our case patients - but also models of their emotions. We explain our research in progress on Affective Computing for tele-HHC applications, which includes: developing a system architecture for monitoring and responding to human multimodal affect and emotions via multimedia and empathetic avatars; mapping of physiological signals to emotions and synthesizing the patient's affective information for the health-care provider. Our results using a wireless non-invasive wearable computer to collect physiological signals and mapping these to emotional states show the feasibility of our approach, for which we lastly discuss the future research issues that we have identified. 2003 Elsevier Science Ltd. All rights reserved.",,
Lisetti,"Lisetti C.L., Nasoz F.","Human intelligence is being increasingly redefined to include the all-encompassing effect of emotions upon what used to be considered 'pure reason'. With the recent progress of research in computer vision, speech/prosody recognition, and bio-feedback, real-time recognition of affect will enhance human-computer interaction considerably, as well as assist further progress in the development of new emotion theories. In this article, we describe how affect, moods and emotions closely interact with cognition and how affect and emotion are the quintesseniial multimodal processes in humans. We then propose an adaptive system architecture designed to sense the user's emotional and affective states via three multimodal subsystems (V, K, A): namely (1) the Visual (from facial images and videos). (2) Kinesthetic (from autonomic nervous system (ANS) signals), and (3) Auditory (from speech). The results of the system sensing are then integrated into the multimodal perceived user's state. A multimodal anthropomorphic interface agent then adapts its interface by responding most appropriately to the current emotional states of its user, and provides intelligent multi-modal feedback to the user.",,
Lisetti,"Nasoz F., Ozyer O., Lisetti C.L., Finkelstein N.","In this paper, we uncover a new potential application for multimedia technologies: car interfaces for enhanced driver's safety. We also describe the experiment we conducted in order to map certain physiological signals (galvanic skin response, heart beat, and temperature) to certain emotions (Neutral, Anger, Fear, Sadness, and Frustration). We demonstrate the results we gained and describe how we use these results to our Multimodal Affective Driver Interface for the drivers of the future cars.",,
Lisetti,"Marpaung A.H., Brown S.M., Lisetti C.L.","A demonstration of Lola, an autonomous intelligent mobile robot produced by ActivMedia was presented. The interface was programmed using ARIA (ActivMedia Robotics Interface for Application. In order to communicate with the users, Lola was able to speak through the avatar.",,
Lisetti,"Murphy R.R., Lisetti C.L, Tardif R., Irish L., Gage A.","Previous experiences show that it is possible for agents such as robots cooperating asynchronously on a sequential task to enter deadlock, where one robot does not fulfill its obligations in a timely manner due to hardware or planning failure, unanticipated delays, etc. Our approach uses a formal multilevel hierarchy of emotions where emotions both modify active behaviors at the sensory-motor level and change the set of active behaviors at the schematic level. The resulting implementation of a team of heterogeneous robots using a hybrid deliberative/reactive architecture produced the desired emergent societal behavior. Data collected at two different public venues illustrate how a dependent agent selects new behaviors (e.g., stop serving, move to intercept the refilled to compensate for delays from a subordinate agent (e.g., blocked by the audience). The subordinate also modifies the intensity of its active behaviors in response to feedback from the dependent agent. The agents communicate asynchronously through Knowledge Query and Manipulation Language via wireless Ethernet.Narasimhan",,
Liu,"Liu P., Hu L., Xu H., Shi Z., Liu J., Wang Q., Dayal J., Tang Y.","There has been a dramatic increase in the popularity of Container as a Service (CaaS) clouds. The CaaS multi-tier applications could be optimized by using network topology, link or server load knowledge to choose the best endpoints to run in CaaS cloud. However, it is difficult to apply those optimizations to the public datacenter shared by multi-tenants. This is because of the opacity between the tenants and the datacenter providers: Providers have no insight into tenant's container workloads and dependencies, while tenants have no clue about the underlying network topology, link, and load. As a result, containers might be booted at wrong physical nodes that lead to performance degradation due to bi-section bandwidth bottleneck or co-located container interference. We propose 'DocMan', a toolset that adopts a black-box approach to discover container ensembles and collect information about intra-ensemble container interactions. It uses a combination of techniques such as distance identification and hierarchical clustering. The experimental results demonstrate that DocMan enables optimized containers placement to reduce the stress on bi-section bandwidth of the datacenter's network. The method can detect container ensembles at low cost and with 92% accuracy and significantly improve performance for multi-tier applications under the best of circumstances. 2018 IEEE.",,
Liu,"Ma X., Du Z., Liu J.","Power profiling tools based on fast and accurate workload analysis can be useful for job scheduling and resource allocation aiming to optimize the power consumption of large-scale, high-performance computer systems. In this article, we propose a novel method for predicting the power consumption of a complete workload or application by extrapolating the power consumption of only a few code segments of the same application obtained from measurements. As such, it provides a fast and yet effective way for predicting the power consumption of the execution of both single and multi-threaded programs on arbitrary architectures without having to profile the entire program's execution. The latter would be costly to obtain, especially if it is a long-running program. Our method employs a set of code analysis tools to capture the program's phase behavior and then uses a multi-variable linear regression method to estimate the power consumption of the entire program. For validation, we select the SPEC 2006 benchmark suite and the NAS parallel benchmarks to evaluate the accuracy and effectiveness of our method. Experimental results on three generations of multicore processors show that our power profiling method achieves good accuracy in predicting program's energy use with relatively small errors. 2018 Elsevier Inc.",,
Liu,"Obaida M.A., Liu J., Chennupati G., Santhi N., Eidenbenz S.","Parallel application performance models provide valuable insight about the performance in real systems. Capable tools providing fast, accurate, and comprehensive prediction and evaluation of high-performance computing (HPC) applications and system architectures have important value. This paper presents PyPassT, an analysis based modeling framework built on static program analysis and integrated simulation of target HPC architectures. More specifically, the framework analyzes application source code written in C with OpenACC directives and transforms it into an application model describing its computation and communication behavior (including CPU and GPU workloads, memory accesses, and message-passing transactions). The application model is then executed on a simulated HPC architecture for performance analysis. Preliminary experiments demonstrate that the proposed framework can represent the runtime behavior of benchmark applications with good accuracy. 2018 Association for Computing Machinery.",,
Liu,"Obaida M.A., Liu J.","The paper presents a simulator designed specifically for evaluating job scheduling algorithms on large-scale HPC systems. The simulator was developed based on the Performance Prediction Toolkit (PPT), which is a parallel discrete-event simulator written in Python for rapid assessment and performance prediction of large-scale scientific applications on supercomputers. The proposed job scheduler simulator incorporates PPT's application models, and when coupled with the sufficiently detailed architecture models, can represent more realistic job runtime behaviors. Consequently, the simulator can evaluate different job scheduling and task mapping algorithms on the specific target HPC platforms more accurately. 2017 IEEE.",,
Liu,"Ahmed K., Liu J., Badawy A.-H., Eidenbenz S.","High-performance Computing (HPC) systems have gone through many changes during the past two decades in their architectural design to satisfy the increasingly large-scale scientific computing demand. Accurate, fast, and scalable performance models and simulation tools are essential for evaluating alternative architecture design decisions for the massive-scale computing systems. This paper recounts some of the influential work in modeling and simulation for HPC systems and applications, identifies some of the major challenges, and outlines future research directions which we believe are critical to the HPC modeling and simulation community. 2017 IEEE.",,
Liu,"Li X., Men C., Du Z., Liu J., Li M., Zhang X.","Learners participating in Massive Open Online Courses (MOOC) have a wide range of backgrounds and motivations. Many MOOC learners enroll in the courses to take a brief look; only a few go through the entire content, and even fewer are able to eventually obtain a certificate. We discovered this phenomenon after having examined 92 courses on both xuetangX and edX platforms. More specifically, we found that the learning coverage in many courses-one of the metrics used to estimate the learners' active engagement with the online courses-observes a Zipf distribution. We apply the maximum likelihood estimation method to fit the Zipf's law and test our hypothesis using a chi-square test. In the xuetangX dataset, the learning coverage in 53 of 76 courses fits Zipf's law, but in all of 16 courses on the edX platform, the learning coverage rejects the Zipf's law. The result from our study is expected to bring insight to the unique learning behavior on MOOC. 2017 by the authors.","Fang C., Li C., Cabrerizo M., Barreto A., Andrian J., Rishe N., Loewenstein D., Duara R., Adjouadi M.","Over the past few years, several approaches have been proposed to assist in the early diagnosis of Alzheimer's disease (AD) and its prodromal stage of mild cognitive impairment (MCI). Using multimodal biomarkers for this high-dimensional classification problem, the widely used algorithms include Support Vector Machines (SVM), Sparse Representation-based classification (SRC), Deep Belief Networks (DBN) and Random Forest (RF). These widely used algorithms continue to yield unsatisfactory performance for delineating the MCI participants from the cognitively normal control (CN) group. A novel Gaussian discriminant analysis-based algorithm is thus introduced to achieve a more effective and accurate classification performance than the aforementioned state-of-the-art algorithms. This study makes use of magnetic resonance imaging (MRI) data uniquely as input to two separate high-dimensional decision spaces that reflect the structural measures of the two brain hemispheres. The data used include 190 CN, 305 MCI and 133 AD subjects as part of the AD Big Data DREAM Challenge #1. Using 80% data for a 10-fold cross-validation, the proposed algorithm achieved an average F1 score of 95.89% and an accuracy of 96.54% for discriminating AD from CN; and more importantly, an average F1 score of 92.08% and an accuracy of 90.26% for discriminating MCI from CN. Then, a true test was implemented on the remaining 20% held-out test data. For discriminating MCI from CN, an accuracy of 80.61%, a sensitivity of 81.97% and a specificity of 78.38% were obtained. These results show significant improvement over existing algorithms for discriminating the subtle differences between MCI participants and the CN group. 2018 World Scientific Publishing Company."
Liu,"Ahmed K., Liu J., Wu X.","Demand response refers to reducing energy consumption of participating systems in response to transient surge in power demand or other emergency events. Demand response is particularly important for maintaining power grid transmission stability, as well as achieving overall energy saving. High Performance Computing (HPC) systems can be considered as ideal participants for demand-response programs, due to their massive energy demand. However, the potential loss of performance must be weighed against the possible gain in power system stability and energy reduction. In this paper, we explore the opportunity of demand response on HPC systems by proposing a new HPC job scheduling and resource provisioning model. More specifically, the proposed model applies power-bound energy-conservation job scheduling during the critical demand-response events, while maintaining the traditional performance-optimized job scheduling during the normal period. We expect such a model can attract willing participation of the HPC systems in the demand response programs, as it can improve both power stability and energy saving without significantly compromising application performance. We implement the proposed method in a simulator and compare it with the traditional scheduling approach. Using trace-driven simulation, we demonstrate that the HPC demand response is a viable approach toward power stability and energy savings with only marginal increase in the jobs' execution time. 2017 IEEE.",,
Liu,"Men C., Li X., Du Z., Liu J., Li M., Zhang X.","Learners participating in Massive Open Online Courses (MOOC) have a wide range of backgrounds and motivations. Many MOOC learners sign up the courses to take a brief look; only a few go through the entire content, and even fewer are able to eventually obtain a certificate. We discovered this phenomenon after having examined 76 courses on the xuetangX platform. More specifically, we found that in many courses the learning coverage-one of the metrics used to estimate the learners' active engagement with the online courses-observes a Zipf distribution. We apply the maximum likelihood estimation method to fit the Zipf's law and test our hypothesis using a chi-square test. The result from our study is expected to bring insight to the unique learning behavior on MOOC and thus help improve the effectiveness of MOOC learning platforms and the design of courses. 2017 IEEE.",,
Liu,"Obaida M.A., Liu J.","Real-time network simulation enables simulation to operate in real time, and in doing so allows experiments with simulated, emulated, and real network components acting in concert to test novel network applications or protocols. Real-time simulation can also run in parallel for large-scale network scenarios, in which case network traffic is represented as simulation events passed as messages to remote simulation instances running on different machines. We note that substantial overhead exists in parallel real-time simulation to support synchronization and communication among distributed instances, which can significantly limit the performance and scalability of the hybrid approach. To overcome these challenges, we propose several techniques for improving the performance of parallel real-time simulation, by eliminating parallel synchronization and reducing communication overhead. Our experiments show that the proposed techniques can indeed improve the overall performance. In a use case, we demonstrate that our hybrid technique can be readily integrated for studies of software-defined networks. 2017 Association for Computing Machinery.",,
Liu,"Feng T., Du Z., Sun Y., Wei J., Bi J., Liu J.","Ground-based Wide-Angle Camera array (GWAC) is a short time-scale survey telescope that can take images covering a field of view of over 5, 000 square degrees every 15 seconds or even shorter. One scientific missions of GWAC is to accurately and quickly detect anomaly astronomical events. For that, a huge amount of data must be handled in real time. In this paper, we propose a new time series analysis model, called DARIMA (or Dynamic Auto-Regressive Integrated Moving Average), to identify the anomaly events that occur in light curves obtained from GWAC as early as possible with high degree of confidence. A major advantage of DARIMA is that it can dynamically adjust its model parameters during the realtime processing of the time series data. We identify the anomaly points based on the weighted prediction result of different time windows to improve accuracy. Experimental results using real survey data show that the DARIMA model can identify the first anomaly point for all light curves. We also evaluate our model with simulated anomaly events of various types embedded in the real time series data. The DARIMA model is able to generate the early warning triggers for all of them. The results from the experiments demonstrate that the proposed DARIMA model is a promising method for real-time anomaly detection of short time-scale GWAC light curves. 2017 IEEE.",,
Liu,"Rong R., Liu J.","Mininet is a container-based emulation environment that can study networks with virtual hosts and OpenFlow-enabled virtual switches on Linux. However, it is well-known that experiments using Mininet may lose fidelity for large-scale networks and heavy traffic load. One solution is to use a distributed setup where an experiment constitutes multiple instances of Mininet running on a cluster, each handling a subset of virtual hosts and switches. Such arrangement, however, is still constrained by bandwidth and latency limitations in the physical connection between the instances. In this paper, we propose a novel method of integrating distributed Mininet instances using a symbiotic approach, which extends an existing method for combining real-time simulation and emulation. We use an abstract network model to coordinate the distributed instances, which are superimposed to represent the target network. In this case, one can more effectively study the behavior of real implementation of network applications on large-scale networks, since the interaction between the Mininet instances is only capturing the effect of contentions among network flows in shared queues, as opposed to having to exchange individual network packets, which can be limited by bandwidth or sensitive to latency. We provide a prototype implementation of the new approach and present validation studies to show it can achieve accurate results. We also present a case study that successfully replicates the behavior of a denial-of-service (DoS) attack protocol. 2017 IEEE.",,
Liu,"Hui X., Du Z., Liu J., Sun H., He Y., Bader D.A.","Power is a primary concern for mobile, cloud, and high-performance computing applications. Approximate computing refers to running applications to obtain results with tolerable errors under resource constraints, and it can be applied to balance energy consumption with service quality. In this paper, we propose a 'Good Enough (GE)' scheduling algorithm that uses approximate computing to provide satis- factory QoS (Quality of Service) for interactive applications with significant energy savings. Given a user-specified quality level, the GE algorithm works in the AES (Aggressive Energy Saving) mode for the majority of the time, neglecting the low- quality portions of the workload. When the perceived quality falls below the required level, the algorithm switches to the BQ (Best Quality) mode with a compensation policy. To avoid core speed thrashing between the two modes, GE employs a hybrid power distribution scheme that uses the Equal-Sharing (ES) policy to distribute power among the cores when the workload is light (to save energy) and the Water-Filling (WF) policy when the workload is high (to improve quality). We conduct simulations to compare the performance of GE with existing scheduling algorithms. Results show that the proposed algorithm can provide large energy savings with satisfactory user experience. 2017 IEEE.",,
Liu,"Ma X., Du Z., Liu J.","Power profiling tools based on fast and accurate workload analysis can be useful for job scheduling and resource allocation aiming to optimize the power consumption of large-scale high-performance computer systems. In this paper, we propose a novel method for predicting the power consumption of a complete workload or application by extrapolating the power consumption of only a few code segments of the same application obtained from measurement. As such, it provides a fast and yet effective way for predicting the power consumption of a single-threaded execution of a program on arbitrary architectures without having to profile the entire program's execution. The latter would be costly to obtain, especially if it's a long running program. Our method employs a set of code analysis tools to capture the program's phase behavior and then adopts a multi-variable linear regression method to estimate the power consumption of the entire program. We use SPEC 2006 benchmark to evaluate the accuracy and effectiveness of our method. Experimental results show that our power profiling method achieves good accuracy in predicting program's energy use with relatively small errors. 2016 IEEE.",,
Liu,"Ahmed K., Liu J., Eidenbenz S., Zerr J.","Performance Prediction Toolkit (PPT) is a simulator mainly developed at Los Alamos National Laboratory to facilitate rapid and accurate performance prediction of large-scale scientific applications on existing and future HPC architectures. In this paper, we present three interconnect models for performance prediction of large-scale HPC applications. They are based on interconnect topologies widely used in HPC systems: torus, dragonfly, and fat-tree. We conduct extensive validation tests of our interconnect models, in particular, using configurations of existing HPC systems. Results show that our models provide good accuracy for predicting the network behavior. We also present a performance study of a parallel computational physics application to show that our model can accurately predict the parallel behavior of large-scale applications. 2016 IEEE.",,
Liu,"Uhrmacher A.M., Brailsford S., Liu J., Rabe M., Tolk A.","Scientific research should be reproducible, and as such also simulation research. However, the question is - is this really the case? In some application areas of simulation, e.g., cell biology, simulation studies cannot be published without data, models, methods, including computer code being made available for evaluation. With the applications and methodological areas of modeling and simulation, how the problem of reproducibility is assessed and addressed differs. The diversity of answers to this question will be illuminated by looking into the area of network simulations, simulation in logistics, in military, and health. Making different scientific cultures, different challenges, and different solutions in discrete event simulation explicit is central to improving the reproducibility and thus quality of discrete event simulation research. 2016 IEEE.",,
Liu,"Ahmed K., Obaida M., Liu J., Eidenbenz S., Santhi N., Chapuis G.","Interconnection network is a critical component of high-performance computing architecture and application co-design. For many scientific applications, the increasing communication complexity poses a serious concern as it may hinder the scaling properties of these applications on novel architectures. It is apparent that a scalable, efficient, and accurate interconnect model would be essential for performance evaluation studies. In this paper, we present an interconnect model for predicting the performance of large-scale applications on high-performance architectures. In particular, we present a sufficiently detailed interconnect model for Cray's Gemini 3-D torus network. The model has been integrated with an implementation of the Message-Passing Interface (MPI) that can mimic most of its functions with packet-level accuracy on the target platform. Extensive experiments show that our integrated model provides good accuracy for predicting the network behavior, while at the same time allowing for good parallel scaling performance. 2016 ACM.",,
Liu,"Santhi N., Eidenbenz S., Liu J.","We introduce Simian, a family of open-source Parallel Discrete Event Simulation (PDES) engines written using Lua and Python. Simian reaps the benefits of interpreted languages-ease of use, fast development time, enhanced readability and a high degree of portability on different platforms-and, through the optional use of Just-In-Time (JIT) compilation, achieves high performance comparable with the state-of-the-art PDES engines implemented using compiled languages such as C or C++. This paper describes the main design concepts of Simian, and presents a benchmark performance study, comparing four Simian implementations (written in Python and Lua, with and without using JIT) against a traditionally compiled simulator, MiniSSF, written in C++. Our experiments show that Simian in Lua with JIT outperforms MiniSSF, sometimes by a factor of three under high computational workloads. 2015 IEEE.",,
Liu,"Liu J., Marcondes C., Ahmed M., Rong R.","Mininet is a popular container-based emulation environment built on Linux for testing Open Flow applications. Using Mininet, one can compose an experimental network using a set of virtual hosts and virtual switches with flexibility. However, it is well understood that Mininet can only provide a limited capacity, both for CPU and network I/O, due to its underlying physical constraints. We propose a method for combining simulation and emulation to improve the scalability of network experiments. This is achieved by applying the symbiotic approach to effectively integrate emulation and simulation for hybrid experimentation. In this case, one can use Mininet to directly run Open Flow applications on the virtual machines and software switches, with network connectivity represented by detailed simulation at scale. 2015 IEEE.",,
Liu,"Jo E., Pan D., Liu J., Butler L.","The fat tree topology with multipath capability has been used in many recent data center networks (DCNs) for increased bandwidth and fault tolerance. Traditional routing protocols have only limited support for multipath routing, and cannot fully utilize the available bandwidth in such networks. In this paper, we study multipath routing for fat tree networks. We formulate the problem as a linear program and prove its NP-completeness. We propose a practical solution, which takes advantage of the emerging software-defined networking paradigm. Our algorithm relies on a central controller to collect necessary network state information in order to make optimized routing decisions. We implemented the algorithm as an OpenFlow controller module and validated it with Mininet emulation. We also developed a fluid-based DCN simulator and conducted experiments, which show that our algorithm outperforms the traditional multipath algorithm based on random assignments, both in terms of increased throughput and in reduced end-to-end delay. 2014 IEEE.",,
Liu,"Erazo M.A., Rong R., Liu J.","A testbed capable of representing detailed operations of complex applications under diverse network conditions is invaluable for understanding the design and performance of new protocols and applications before their real deployment. We introduce a novel method that combines high-performance large-scale network simulation and high-fidelity network emulation, and thus enables real instances of network applications and protocols to run in real operating environments and be tested under simulated network settings. Using our approach, network simulation and emulation can form a symbiotic relationship, through which they are synchronized for an accurate representation of the network-scale traffic behavior. We introduce a model downscaling method along with an efficient queuing model and a traffic reproduction technique, which can significantly reduce the synchronization overhead and improve accuracy. We validate our approach with extensive experiments via simulation and with a real-system implementation. We also present a case study using our approach to evaluate a multipath data transport protocol. 2015 ACM.",,
Liu,"Li T., Liu J.","To reduce the computational complexity of large-scale network simulation, one needs to distinguish foreground traffic generated by the target applications one intends to study from background traffic that represents the bulk of the network traffic generated by other applications. Background traffic competes with foreground traffic for network resources and consequently plays an important role in determining the behavior of network applications. Existing background traffic models either operate only at coarse time granularity or focus only on individual links. There is little insight on how to meaningfully apply realistic background traffic over the entire network. In this article, we propose a method for generating background traffic with spatial and temporal characteristics observed from real traffic traces.We apply data clustering techniques to describe the behavior of end hosts as a function of multidimensional attributes and group them into distinct classes, and then map the classes to simulated routers so that we can generate traffic in accordance with the cluster-level statistics. The proposed traffic generator makes no assumption on the target network topology. It is also capable of scaling the generated traffic so that the traffic intensity can be varied accordingly in order to test applications under different and yet realistic network conditions. Experiments show that our method is able to generate traffic that maintains the same spatial and temporal characteristics as in the observed traffic traces. 2014 ACM 1049-3301/2014/07-ART1 $15.00.",,
Liu,"Liu J., Obaida M.A., Santos F.D.","PrimoGENI provides a GENI aggregate interface through which experimenters can launch large-scale network experiments on GENI resources consisting of both simulated network and real instances of network applications directly running on either virtual or physical machines. Real network traffic generated by the network applications can be introduced into the simulated network in real time and be subjected to proper delays and losses according to the simulated network conditions. To leverage the previous PrimoGENI prototype activities, PrimoGENI Constellation is a newly launched project, which will focus specifically on facilitating distributed at-scale hybrid experiments for real-world high-impact applications. In this paper, we provide an overview of the major achievements of PrimoGENI, and more importantly, discuss the ongoing efforts in PrimoGENI Constellation aiming to achieve the full potential of the hybrid network experiment approach. The main thrusts of PrimoGENI Constellation include: 1) supporting at-scale network experiments potentially distributed on different types of GENI resources in accordance with the GENI experiment workflow, 2) focusing on target applications supporting prominent and high-impact future Internet research, and 3) building the user community through extensive education and research training, and online archives of experiment results and user experiences. 2014 IEEE.",,
Liu,"Liu J., Liu Y., Du Z., Li T.","Large-scale network simulation imposes extremely high computing demand. While parallel processing techniques allows network simulation to scale up and benefit from contemporary high-end computing platforms, multi-resolutional modeling techniques, which differentiate network traffic representations in network models, can substantially reduce the computational requirement. In this paper, we present a novel method for offloading computationally intensive bulk traffic calculations to the background onto GPU, while leaving CPU to simulate detailed network transactions in the foreground. We present a hybrid traffic model that combines the foreground packet-oriented discrete-event simulation on CPU with the background fluid-based numerical calculations on GPU. In particular, we present several optimizations to efficiently integrate packet and fluid flows in simulation with overlapping computations on CPU and GPU. These optimizations exploit the lookahead inherent to the fluid equations, and take advantage of batch runs with fix-up computation and on-demand prefetching to reduce the frequency of interactions between CPU and GPU. Experiments show that our GPU-assisted hybrid traffic model can achieve substantial performance improvement over the CPU-only approach, while still maintaining good accuracy. Copyright 2014 ACM.",,
Liu,"Rong R., Hao J., Liu J.","Scalable Simulation Framework (SSF), a parallel simulation application programming interface (API) for large-scale discrete-event models, has been widely adopted in many areas. This paper presents a simplified and yet more streamlined implementation, called Mini-SSF. MiniSSF maintains the core design concept of SSF, while removing some of the complex but rarely used features, for sake of efficiency. It also introduces several new features that can greatly simplify model development efforts and/or improve the simulator's performance. More specifically, an automated compiler-based source-code translation scheme has been adopted in MiniSSF to enable scalable process-oriented simulation using handcrafted threads. A hierarchical hybrid synchronization algorithm has been incorporated in the simulator to improve parallel performance. Also, a new set of platform-independent API functions have been added for developing simulation models to be executed transparently on different parallel computing platforms. In this paper, we report performance results from experiments on different XSEDE platforms to assess the performance and scalability of MiniSSF. It is shown that the simulator can achieve superior performance. The simulator can adapt its synchronization according to the model's computation and communication demands, as well as the underlying parallel platform. The results also suggest that more automatic adaptation and fine-grained performance tuning is necessary for handling more complex large-scale simulation scenarios. Copyright 2014 ACM.",,
Liu,Liu J.,"We tackle the problem of scheduling logical processes that can significantly affect the performance of running parallel simulation either in real time or proportional to real time. In particular, we present a comprehensive solution to dealing with the mixture of simulated and emulated events in a full-fledged conservatively synchronized parallel simulation kernel to improve efficiency and timeliness for processing the emulated events. We propose an event delivery mechanism for the parallel simulator to incorporate emulated events originated from the physical system. We augment the parallel simulation API to support emulation capabilities independent from a particular simulation domain. Preliminary experiments demonstrate the our simulator's real-time performance on high-performance computing platforms. 2013 IEEE.",,
Liu,"Jin H., Cheocherngngarn T., Levy D., Smith A., Pan D., Liu J., Pissinou N.","Data centers consume significant amounts of energy. As severs become more energy efficient with various energy saving techniques, the data center network (DCN) has been accounting for 20% or more of the energy consumed by the entire data center. While DCNs are typically provisioned with full bisection bandwidth, DCN traffic demonstrates fluctuating patterns. The objective of this work is to improve the energy efficiency of DCNs during off-peak traffic time by powering off idle devices. Although there exist a number of energy optimization solutions for DCNs, they consider only either the hosts or network, but not both. In this paper, we propose a joint optimization scheme that simultaneously optimizes virtual machine (VM) placement and network flow routing to maximize energy savings, and we also build an Open Flow based prototype to experimentally demonstrate the effectiveness of our design. First, we formulate the joint optimization problem as an integer linear program, but it is not a practical solution due to high complexity. To practically and effectively combine host and network based optimization, we present a unified representation method that converts the VM placement problem to a routing problem. In addition, to accelerate processing the large number of servers and an even larger number of VMs, we describe a parallelization approach that divides the DCN into clusters for parallel processing. Further, to quickly find efficient paths for flows, we propose a fast topology oriented multipath routing algorithm that uses depth-first search to quickly traverse between hierarchical switch layers and uses the best-fit criterion to maximize flow consolidation. Finally, we have conducted extensive simulations and experiments to compare our design with existing ones. The simulation and experiment results fully demonstrate that our design outperforms existing host-or network-only optimization solutions, and well approximates the ideal linear program. 2013 IEEE.",,
Liu,"Jin H., Pan D., Liu J., Pissinou N.","Flow-level bandwidth provisioning (FBP) achieves fine-grained bandwidth assurance for individual flows. It is especially important for virtualization-based computing environments such as data centers. However, existing flow-level bandwidth provisioning solutions suffer from a number of drawbacks, including high implementation complexity, poor performance guarantees, and inefficiency to process variable length packets. In this paper, we study flow-level bandwidth provisioning for Combined Input Crosspoint Queued (CICQ) switches in the OpenFlow context. First, we propose the Flow-level Bandwidth Provisioning algorithm for CICQ switches, which reduces the switch scheduling problem to multiple instances of fair queuing problems, each utilizing a well-studied fair queuing algorithm. We theoretically prove that FBP can closely emulate the ideal Generalized Processing Sharing model, and accurately guarantee the provisioned bandwidth. Furthermore, we implement FBP in the OpenFlow software switch to obtain realistic performance data by a prototype. Leveraging the capability of OpenFlow to define and manipulate flows, we experimentally demonstrate a practical flow-level bandwidth provisioning solution. Finally, we conduct extensive simulations and experiments to evaluate the design. The simulation data verify the correctness of the analytical results, and show that FBP achieves tight performance guarantees. The experiment results demonstrate that our OpenFlow-based prototype can conveniently and accurately provision bandwidth at the flow level. 1968-2012 IEEE.",,
Liu,"Erazo M.A., Liu J.","A testbed capable of representing detailed operations of complex applications under diverse large-scale network conditions can be extremely helpful for investigating potential system design and implementation problems, and studying application performance issues, such as scalability and robustness, even before the applications are deployed in a real environment. We introduce a novel method that combines high-performance large-scale network simulation and high-fidelity network emulation, and thereby enables real instances of network applications and protocols to run in real operating environments, and be tested under large-scale simulated network settings. In our approach, network simulation and emulation form a symbiotic relationship, through which they are synchronized for an accurate representation of the large-scale traffic behavior. We introduce a model downscaling method, along with an efficient queuing model and a traffic reproduction technique, which can significantly reduce the synchronization overhead and improve computational efficiency, while maintaining the accuracy of the system. We validate our approach with extensive experiments via simulation and with a real-system prototype. 2013 ACM.",,
Liu,"Li T., Van Vorst N., Liu J.","Traditional discrete-event simulation of large-scale networks at the packet level is computationally expensive. This article presents a fast rate-based transmission control protocol (RTCP) traffic model designed to reduce the time and space complexity for simulating network traffic whilst maintaining good accuracy. A distinct feature of the proposed model is that the transmission control protocol (TCP) congestion control behavior is represented using analytical models that describe the send rate at the traffic source as a function of the round-trip time and the packet loss rate at different phases of a TCP connection. Rather than modeling at the granularity of individual packets visiting the intermediate routers, the model approximates traffic flows as a series of rate windows, each consisting of a number of packets considered to possess the same arrival rate. The model calculates the queuing delays and the packet losses as these rate windows traverse the individual network queues along the flow path. The proposed RTCP model is able to achieve a performance advantage over other TCP models, by integrating analytical solutions and aggregating traffic using rate windows. Empirical results show that the RTCP model can correctly capture the overall TCP behavior and achieve a speedup of more than two orders of magnitude over the corresponding detailed packet-oriented simulation. 2013, The Society for Modeling and Simulation International. All rights reserved.",,
Liu,"Li T., Van Vorst N., Rong R., Liu J.","We propose an in-network caching architecture using OpenFlow to coordinate caching decisions in the network. Our scheme, called CacheFlow, extends the cache-and-forward concept by moving contents closer to the clients hop-by-hop using TCP for sending requests and retrieving contents. As such, CacheFlow can be incrementally implemented and deployed in the real network. In this paper, we present a simulation study of several caching policies, including a random cache policy, a statically optimal cache placement policy and a new disk placement strategy that places popular contents at the ""center"" of the network. Experimental results show that simple in-network caching policies can be realized using today's technology to improve network performance.",,
Liu,"Cheocherngngarn T., Jin H., Andrian J., Pan D., Liu J.","Modern data center networks (DCNs) often use multi-rooted topologies, which offer multipath capability, for increased bandwidth and fault tolerance. However, traditional routing algorithms for the Internet have no or limited support for multipath routing, and cannot fully utilize available bandwidth in such DCNs. In this paper, we study the multipath routing problem for DCNs. We first formulate the problem as an integer linear program, but it is not suitable for fast on-the-fly route calculation. For a practical solution, we propose the Depth-First Worst-Fit Search based multipath routing algorithm. The main idea is to use depth-first search to find a sequence of worst-fit links to connect the source and destination of a flow. Since DCN topologies are usually hierarchical, our algorithm uses depth-first search to quickly traverse between hierarchical layers to find a path. When there are multiple links to a neighboring layer, the worst-fit link selection criterion enables the algorithm to make the selection decision with constant time complexity by leveraging the max-heap data structure, and use a small number of selections to find all the links of a path. Further, worst-fit also achieves load balancing, and thus generates low queueing delay, which is a major component of the end-to-end delay. We have evaluated the proposed algorithm by extensive simulations, and compared its average number of link selections and average end-to-end delay with competing solutions. The simulation results fully demonstrate the superiority of our algorithm and validate the effectiveness of our designs. 2012 IEEE.",,
Liu,"Van Vorst N., Liu J.","This paper presents the model splitting method for large-scale interactive network simulation, which addresses the separation of concerns between network researchers, who focus on developing complex network models and conducting large-scale network experiments, and simulator developers, who are concerned with developing efficient simulation engines to achieve the best performance on parallel platforms. Modeling splitting divides the system into an interactive model to support user interaction, and an execution model to facilitate parallel processing. We describe techniques to maintain consistency and real-time synchronization between the two models. We also provide solutions to reduce the memory complexity of large network models and to ensure data persistency and access efficiency for out-of-core processing. 2012 IEEE.",,
Liu,"Liu J., Rong R.",This paper presents a hierarchical composite synchronization algorithm for parallel discrete-event simulation. The composite approach combines an asynchronous CMB-style channel scanning method with a synchronous window-based method to avoid pathological situations where neither synchronization algorithms would perform optimally. The hierarchical approach addresses the discrepancy in the communication and synchronization cost for shared-memory multiprocessor multicore machines and for distributed-memory machines. The paper describes a performance model for the hierarchical composite algorithm and proposes a method for predicting the runtime empirically using linear regression. Experiment results show that the model can predict the overall performance of the hierarchical composite algorithm. A significant performance improvement has been observed over different combinations of the traditional asynchronous and synchronous approaches used separately for distributed-memory machines and on shared-memory multiprocessor multicore machines. 2012 IEEE.,,
Liu,"Erazo M.A., Li T., Liu J., Eidenbenz S.","We present the design and implementation of FileSim, a simulation framework with detailed models of parallel file systems, capable of reproducing the complex I/O behavior at scale. FileSim aims to support comprehensive and accurate end-to-end I/O performance prediction and evaluation of exascale high-end computing systems. To this end, FileSim provides several key features, including detailed, pluggable models of contemporary parallel file systems, the support of trace-driven simulation, and the capability of running large-scale I/O systems using parallel and distributed simulation.We conducted extensive validation and performance studies, through which we show that the simulator is capable of reproducing important I/O system behaviors comparable to those measured from the real systems. We demonstrate the capabilities of FileSim as a tool for exploring the parameter space and design alternatives of large-scale parallel file systems. 2012 IEEE.",,
Liu,"Van Vorst N., Erazo M., Liu J.","The Global Environment for Network Innovations (GENI) is a community-driven research and development effort to build a collaborative and exploratory network experimentation platforma virtual laboratory for the design, implementation, and evaluation of future networks. The PrimoGENI project enables real-time network simulation by extending an existing network simulator to become part of the GENI federation to support large-scale experiments involving physical, simulated, and emulated network entities. In this paper, we describe a novel design of PrimoGENI, which aims at supporting realistic, scalable, and flexible network experiments with real-time simulation and emulation capabilities. We present a flexible emulation infrastructure that allows both remote client machines, local cluster nodes running virtual machines, and external networks to seamlessly interoperate with the simulated network running within a designated slice of resources. We present the results of our preliminary validation and performance studies to demonstrate the capabilities as well as limitations of our approach. 2012 Operational Research Society Ltd. All rights reserved.",,
Liu,"Van Vorst N., Li T., Liu J.","Memory consumption is a critical problem for large-scale network simulations. Particularly, the large memory footprint needed for maintaining routing tables can severely obturate scalability. We present an approach of composing large-scale network models using sharable model fragments to achieve significant reduction in the amount of memory required for storing forwarding tables in simulation. Our approach, called spherical routing, conducts static routing within spheres according to user-defined policies. Our routing scheme pre-calculates the forwarding table for each routing sphere, and allows spheres with identical sub-structures to share forwarding tables. Through extensive experiments we demonstrate that our approach can achieve several orders of magnitude in memory reduction for large-scale network models. 2011 IEEE.",,
Liu,"Jin H., Pan D., Liu J., Pissinou N.","Flow level bandwidth provisioning offers fine granularity bandwidth assurance for individual flows. It is especially important for virtual network based experiment environments, to isolate traffic of different experiments or different types, which may be fed to the same switch or router port. Existing flow level bandwidth provisioning solutions suffer from a number of drawbacks, including high implementation complexity, poor performance guarantees, and inefficiency to process variable length packets. In this paper, we study flow level bandwidth provisioning for combined-input-crosspoint-queued switches in the OpenFlow context. We propose the FEBR (Flow lEvel Bandwidth pRovisioning) algorithm, which reduces the switch scheduling problem to multiple instances of fair queueing problems, each employing a well studied fair queueing algorithm. FEBR can tightly emulate the ideal Generalized Processing Sharing model, and accurately guarantee the provisioned bandwidth. Further, we implement FEBR in the OpenFlow version 1.0 software switch. In conjunction with the capability of OpenFlow to flexibly define and manipulate flows, we thus provide a practical flow level bandwidth provisioning solution. Finally, we present extensive simulation and experiment data to validate the analytical results and evaluate our design. 2011 IEEE.",,
Liu,"Van Vorst N., Erazo M., Liu J.","The Global Environment for Network Innovations (GENI) is a community-driven research and development effort to build a collaborative and exploratory network experimentation platform - a ""virtual laboratory"" for the design, implementation and evaluation of future networks. The PrimoGENI project enables real-time network simulation by extending an existing network simulator to become part of the GENI federation to support large-scale experiments involving physical, simulated and emulated network entities. In this paper, we describe a novel design of PrimoGENI, which aims at supporting realistic, scalable, and flexible network experiments with real-time simulation and emulation capabilities. We present a flexible emulation infrastructure that allows both remote client machines and local cluster nodes running virtual machines to seamlessly interoperate with the simulated network running within a designated ""slice"" of resources. We show the results of our preliminary validation and performance studies to demonstrate the capabilities and limitations of our approach. 2011 IEEE.",,
Liu,"Erazo M.A., Liu J.","Small-scale experiments are insufficient for a comprehensive study of congestion control protocols. Similarly, results obtained from pure simulation platforms without exercising real protocols and applications lack realism. Motivated by these reasons, we developed scalable virtualised evaluation environment for TCP (SVEET), a TCP performance evaluation testbed where real implementations of TCP variants can be accurately evaluated under diverse network configurations and workloads from real applications in large-scale network settings. It is our purpose to provide the research community a standard environment through which quantitative assessment regarding TCP behaviour can be drawn from large-scale experiments. In order to accomplish our goal, we adopted a novel model-driven emulation approach combining real-time simulation, machine and time virtualisation techniques. We validate the testbed via extensive experiments to assess its potentials and limitations. Additionally, we performed case studies involving real web, streaming and peer-to-peer applications. Our results indicate that SVEET can accurately model the behaviour of the TCP variants and support large-scale network scenarios. Copyright 2010 Inderscience Enterprises Ltd.",,
Liu,"Liu J., Rangaswami R., Zhao M.","We present VENICE, a project that aims at developing a high-fidelity, high-performance, and highly-controllable experimental platform on commodity computing infrastructure to facilitate innovation in existing and futuristic network systems. VENICE employs a novel model-driven network emulation approach that combines simulation of large-scale network models and virtual-machine-based emulation of real distributed applications. To accurately emulate the target system and meet the computation and communication requirements of its individual elements, VENICE adopts a holistic machine and network virtualization technique, called virtual time machine, in which the time advancement of simulated and emulated components are regulated in complete transparency to the test applications. In this paper, we outline the challenges and solutions to realizing the vision of VENICE. ©2010 IEEE.",,
Liu,"Erazo M.A., Liu J.","The Global Environment for Network Innovations (GENI) is a community-driven research and development effort to build a collaborative and exploratory network experimentation platform, a ""virtual laboratory"" for the design, implementation and evaluation of future Internets. In this paper, we present an overview of PrimoGENI, a GENI project with the goal of extending the GENI suite of interoperable infrastructure to allow network experiments at scale, involving physical, simulated and emulated network entities. Copyright 2010 ICST.",,
Liu,"Liu J., Li Y., He Y.","We examine the capabilities of conducting network experiments involving a large-scale peer-to-peer web-content distribution network. Our study uses a real-time network simulator, called PRIME, running on EmuLab, which is a shared cluster computing environment designed specifically for network emulation studies. Our study is one of the largest network experiments that involve a real implementation of a peer-to-peer content distribution system under HTTP traffic from a public-domain empirical workload trace and using a realistic large network model. Our experiments demonstrate the potentials of real-time simulation for studying complex behaviors of distributed applications under large-scale network conditions. ©2009 IEEE.",,
Liu,"Li T., Liu J.","Background traffic has a significant impact on the behavior of network services and protocols. However, a detailed model of the background traffic can be extremely time consuming in simulation. In this paper, we extend our previous hybrid model that combines fluid and packet-oriented characterization of network traffic for a realistic representation of the background traffic on Internet. In particular, we get rid of some unrealistic assumptions in the hybrid model, by adding an acknowledgment scheme to correctly capture the mutual influence of fluid TCP flows on network queues, and by applying the Poisson Pareto Burst Process (PPBP) model to describe the long-range dependencies of the Internet traffic. Experiments show that our fluid background traffic model can capture similar level of realism as the traditional packet-oriented approach.",,
Liu,"Li Y., Liljenstam M., Liu J.","We use a realistic interdomain routing experiment platform to conduct real-time attack and defense exercises for training purposes. Our interdomain routing experiment platform integrates open-source router software, real-time network simulation, and light-weight machine virtualization technologies, and is capable of supporting realistic large-scale routing experiments. The network model used consists of major autonomous systems connecting Swedish Internet users with realistic routing configurations derived from the routing registry. We conduct a series of real-time security exercises on this routing system to study the consequence of intentionally propagating false routing information on interdomain routing and the effectiveness of corresponding defensive measures. We describe three kinds of simplistic BGP attacks in the context of security exercises designed specifically for training purposes. While an attacker can launch attacks from a compromised router by changing its routing policies, administrators will be able to observe the adverse effect of these attacks and subsequently apply appropriate defensive measures to mitigate their impact, such as installing filtering rules. These exercises, all carried out in real time, demonstrate the feasibility of large-scale realistic routing experiments using the real-time routing experiment platform. 2009 Crown Copyright.",,
Liu,"Li Y., Liu J., Rangaswami R.","This paper describes a new software infrastructure that combines the scalability and flexibility benefits of real-time network simulation with the realism of open-source routing protocol implementations. The infrastructure seamlessly integrates the open-source XORP router implementation with a real-time large-scale network simulation engine. The design uses a novel forwarding plane offloading approach that decouples routing from forwarding and confines the more resource consuming forwarding operations inside the simulation engine to reduce I/ O overhead. Experiments demonstrate superior performance of the software routing infrastructure without impairing accuracy. The infrastructure is shown to be able to support large-scale routing experiments on light-weight virtual machines. Copyright 2009, Inderscience Publishers.",,
Liu,"Erazo M.A., Li Y., Liu J.","The ability to establish an objective comparison between high-performance TCP variants under diverse networking conditions and to obtain a quantitative assessment of their impact on the global network traffic is essential to a communitywide understanding of various design approaches. Small-scale experiments are insufficient for a comprehensive study of these TCP variants. We propose a TCP performance evaluation testbed, called SVEET, on which real implementations of the TCP variants can be accurately evaluated under diverse network configurations and workloads in large-scale network settings. This testbed combines real-time immersive simulation, emulation, machine and time virtualization techniques. We validate the testbed via extensive experiments and assess its capabilities through case studies involving real web services.",,
Liu,"Liu J., Li Y., Vorst N.V., Mann S., Hellman K.","We present an open and flexible software infrastructure that embeds physical hosts in a simulated network. In real-time network simulation, where real-world implementations of distributed applications and network services can run together with the network simulator that operates in real-time, real network packets are injected into the simulation system and subject to the simulated network conditions computed as a result of both real and virtual traffic traversing the network and competing for network resources. Our real-time simulation infrastructure has been implemented based on Open Virtual Private Network (OpenVPN), modified and customized to bridges traffic between the physical hosts and the simulated network. We identify the performance advantages and limitations of our approach via a set of experiments. We also present two interesting application scenarios to show the capabilities of the real-time simulation infrastructure.",,
Liu,"Liu J., Yue L.","Fluid-based network traffic models are attractive due to their execution efficiency. They run much faster than the corresponding discrete-event packet-oriented simulation, especially when we study the aggregate traffic behavior of large-scale network scenarios. The efficiency, however, comes at a cost: fluid modeling does not include packet-level details. The ability to accurately capture the interaction between the packets and the network routers and hosts visited by the packets is essential for real-time network simulations, where the simulator must be able to interact with real applications in real time. In particular, the virtual network must be able to carry real packets subject to proper delays and losses and be able to react to these real packets (such as traceroute). Previously, we presented a hybrid network traffic model that combines a continuous-time fluid model and the discrete-event packet-oriented simulation. In this article, we examine a parallel processing method for simulations of large-scale networks using the hybrid model. Our method benefits from the observation that the time it takes to propagate fluid characteristics along the path taken by the traffic flows has a lower bound equal to the minimum link delay as manifested by the governing ordinary differential equations (ODEs). A better lookahead can thus be used to allow parallel simulation of the hybrid model to run without more synchronization overhead than the corresponding discrete-event packet-oriented model. We derive an analytical model comparing the fluid model and the packet-oriented model both for sequential and parallel simulations. We demonstrate the benefit of the parallel hybrid model through a series of simulation experiments of a large-scale network consisting of over 170 000 hosts and 1.6 million traffic flows on a small parallel cluster. 2009, SAGE Publications. All rights reserved.",,
Liu,Liu J.,"Immersive real-time large-scale network simulation is a technique that supports simulation of large-scale networks to interact with real implementations of network protocols, network services, and distributed applications. Traffic generated by real network applications is carried by the virtual network simulated in real time where delays and losses are calculated according to the simulated network conditions. We emphasize network immersion so that the virtual network is indistinguishable from a physical testbed in terms of network behavior, yet allows the flexibility of simulation. In this paper we provide a summary of current research in immersive real-time large-scale network simulation, particularly in areas of hybrid network traffic modeling and scalable emulation infrastructure design. ©2008 IEEE.",,
Liu,"Li Y., Liu J., Rangaswami R.","The ability to conduct accurate and realistic experiments is critical in furthering the research and development of network routing protocols. Existing framework for routing experiments is found to be lacking in one or more of the three required features: realism, scalability, and flexibility. We develop a new software infrastructure that combines the scalability and flexibility benefits of real-time network simulation with the realism of open-source routing protocol implementations. The infrastructure seamlessly integrates the open-source XORP router software with a previously developed real-time network simulation engine. Our design of the infrastructure uses a novel forwarding plane offloading approach that decouples routing from forwarding and confines the more resource consuming forwarding operations inside the simulation engine to reduce I/O overhead. Experiments demonstrate superior performance of the experimental infrastructure without impairing accuracy. 2008 IEEE.",,
Narasimhan,"Damaso N., Mendel J., Mendoza M., von Wettberg E.J., Narasimhan G., Mills D.","Soil DNA profiling has potential as a forensic tool to establish a link between soil collected at a crime scene and soil recovered from a suspect. However, a quantitative measure is needed to investigate the spatial/temporal variability across multiple scales prior to their application in forensic science. In this study, soil DNA profiles across Miami-Dade, FL, were generated using length heterogeneity PCR to target four taxa. The objectives of this study were to (i) assess the biogeographical patterns of soils to determine whether soil biota is spatially correlated with geographic location and (ii) evaluate five machine learning algorithms for their predictive ability to recognize biotic patterns which could accurately classify soils at different spatial scales regardless of seasonal collection. Results demonstrate that soil communities have unique patterns and are spatially autocorrelated. Bioinformatic algorithms could accurately classify soils across all scales with Random Forest significantly outperforming all other algorithms regardless of spatial level. 2018 American Academy of Forensic Sciences",,
Narasimhan,"Cickovski T., Aguiar-Pulido V., Narasimhan G.","Computing centrality involves finding the most 'central' or important nodes in a network. Although potentially useful for biological networks, this can be challenging if the definition of importance is not obvious [1]. There are many different centrality algorithms with different importance definitions that return different results. This is immediately obvious in Figure 1(a), which shows the results of betweenness (red, [2]), closeness (yellow, [3]) and degree (blue, [4]) centrality on a bacterial co-occurence network [5]. Black nodes indicate mutual agreements. We color the top 20% of nodes found by each algorithm, and use appropriate color combinations for those found by two (i.e., red+yellow=orange for betweenness and closeness). As shown, due to spatial bias there is a wide variation making these results difficult to interpret or generalize. Betweenness tends to find nodes on the same path, closeness toward the middle of the network, and degree within the same strongly connected component. 2017 IEEE.",,
Narasimhan,"Cickovski T., Peake E., Aguiar-Pulido V., Narasimhan G.","Background: The notion of centrality is used to identify ""important"" nodes in social networks. Importance of nodes is not well-defined, and many different notions exist in the literature. The challenge of defining centrality in meaningful ways when network edges can be positively or negatively weighted has not been adequately addressed in the literature. Existing centrality algorithms also have a second shortcoming, i.e., the list of the most central nodes are often clustered in a specific region of the network and are not well represented across the network. Methods: We address both by proposing Ablatio Triadum (ATria), an iterative centrality algorithm that uses the concept of ""payoffs"" from economic theory. Results: We compare our algorithm with other known centrality algorithms and demonstrate how ATria overcomes several of their shortcomings. We demonstrate the applicability of our algorithm to synthetic networks as well as biological networks including bacterial co-occurrence networks, sometimes referred to as microbial social networks. Conclusions: We show evidence that ATria identifies three different kinds of ""important"" nodes in microbial social networks with different potential roles in the community. 2017 The Author(s).",,
Narasimhan,"Mesa A., Fernandez M., Wu W., Narasimhan G., Greidinger E.L., Mills D.K.","Objective The objective of this paper is to develop novel classification criteria to distinguish between unclear systemic lupus erythematosus (SLE) and mixed connective tissue disease (MCTD) cases. Methods A total of 205 variables from 111 SLE and 55 MCTD patients were evaluated to uncover unique molecular and clinical markers for each disease. Binomial logistic regressions (BLRs) were performed on currently used SLE and MCTD classification criteria sets to obtain six reduced models with power to discriminate between unclear SLE and MCTD patients that were confirmed by receiving operating characteristic (ROC) curve. Decision trees were employed to delineate novel classification rules to discriminate between unclear SLE and MCTD patients. Results SLE and MCTD patients exhibited contrasting molecular markers and clinical manifestations. Furthermore, reduced models highlighted SLE patients exhibiting prevalence of skin rashes and renal disease while MCTD cases show dominance of myositis and muscle weakness. Additionally decision tree analyses revealed a novel classification rule tailored to differentiate unclear SLE and MCTD patients (Lu-vs-M) with an overall accuracy of 88%. Conclusions Validation of our novel proposed classification rule (Lu-vs-M) includes novel contrasting characteristics (calcinosis, CPK elevated and anti-IgM reactivity for U1-70K, U1A and U1C) between SLE and MCTD patients and showed a 33% improvement in distinguishing these disorders when compared to currently used classification criteria sets. Pending additional validation, our novel classification rule is a promising method to distinguish between patients with unclear SLE and MCTD diagnosis. SAGE Publications.","Yang Y.-J., Razib M., Zeng W.","In this paper, we present an intrinsic method to compute the bijective registration between genus zero surfaces with consistent feature graphs. First, the graph constrained surfaces are mapped to canonical domains by an intrinsic harmonic map, which extends the mean value coordinate to graph constrained surfaces in a rigorous and consistent way. The feature graph on the 3D surface is straightened to a planar straight graph, which forms a convex subdivision of the canonical domain. The parameterization exists, and is unique and intrinsic to the surface and its feature graph. Then the 3D surfaces with consistent feature graphs are registered by matching the straightened graphs and their associated convex regions in the canonical domain by constrained harmonic maps. The method is theoretically rigorous, and computationally efficient and robust. The application of surface morphing on various surfaces and images demonstrates the efficiency and practicality of the proposed methods. 2018 Elsevier Inc."
Narasimhan,"Huang W., Kazmierczak K., Zhou Z., Aguiar-Pulido V., Narasimhan G., Szczesna-Cordary D.","Using microarray and bioinformatics, we examined the gene expression profiles in transgenic mouse hearts expressing mutations in the myosin regulatory light chain shown to cause hypertrophic cardiomyopathy (HCM). We focused on two malignant RLC-mutations, Arginine 58?Glutamine (R58Q) and Aspartic Acid 166 ? Valine (D166V), and one benign, Lysine 104 ? Glutamic Acid (K104E)-mutation. Datasets of differentially expressed genes for each of three mutants were compared to those observed in wild-type (WT) hearts. The changes in the mutant vs. WT samples were shown as fold-change (FC), with stringency FC ? 2. Based on the gene profiles, we have identified the major signaling pathways that underlie the R58Q-, D166V- and K104E-HCM phenotypes. The correlations between different genotypes were also studied using network-based algorithms. Genes with strong correlations were clustered into one group and the central gene networks were identified for each HCM mutant. The overall gene expression patterns in all mutants were distinct from the WT profiles. Both malignant mutations shared certain classes of genes that were up or downregulated, but most similarities were noted between D166V and K104E mice, with R58Q hearts showing a distinct gene expression pattern. Our data suggest that all three HCM mice lead to cardiomyopathy in a mutation-specific manner and thus develop HCM through diverse mechanisms. 2016 Elsevier Inc. All rights reserved.",,
Narasimhan,"Cickovski T., Peake E., Aguiar-Pulido V., Narasimhan G.","Large-scale biological networks such as gene regulatory [1] and PPI [2] have become commonplace through systems biology. Figure 1 shows a microbial social network [3], which attempts to infer interactions between microbes within a community from metagenomics studies. In particular this is a co-occurence network [4], where green/red edges respectively represent positive/negative correlations, or how strongly two bacterial taxa tend to co-infect samples. 2015 IEEE.",,
Narasimhan,"Fernandez M., Riveros J.D., Campos M., Mathee K., Narasimhan G.","Background: It is well understood that distinct communities of bacteria are present at different sites of the body, and that changes in the structure of these communities have strong implications for human health. Yet, challenges remain in understanding the complex interconnections between the bacterial taxa within these microbial communities and how they change during the progression of diseases. Many recent studies attempt to analyze the human microbiome using traditional ecological measures and cataloging differences in bacterial community membership. In this paper, we show how to push metagenomic analyses beyond mundane questions related to the bacterial taxonomic profiles that differentiate one sample from another. Methods: We develop tools and techniques that help us to investigate the nature of social interactions in microbial communities, and demonstrate ways of compactly capturing extensive information about these networks and visually conveying them in an effective manner. We define the concept of bacterial ""social clubs"", which are groups of taxa that tend to appear together in many samples. More importantly, we define the concept of ""rival clubs"", entire groups that tend to avoid occurring together in many samples. We show how to efficiently compute social clubs and rival clubs and demonstrate their utility with the help of examples including a smokers' dataset and a dataset from the Human Microbiome Project (HMP). Results: The tools developed provide a framework for analyzing relationships between bacterial taxa modeled as bacterial co-occurrence networks. The computational techniques also provide a framework for identifying clubs and rival clubs and for studying differences in the microbiomes (and their interactions) of two or more collections of samples. Conclusions: Microbial relationships are similar to those found in social networks. In this work, we assume that strong (positive or negative) tendencies to co-occur or co-infect is likely to have biological, physiological, or ecological significance, possibly as a result of cooperation or competition. As a consequence of the analysis, a variety of biological interpretations are conjectured. In the human microbiome context, the pattern of strength of interactions between bacterial taxa is unique to body site. 2015 Fernandez et al.",,
Narasimhan,"Cickovski T., Flor T., Irving-Sachs G., Novikov P., Parda J., Narasimhan G.","In order to make multiple copies of a target sequence in the laboratory, the technique of Polymerase Chain Reaction (PCR) requires the design of ""primers"", which are short fragments of nucleotides complementary to the flanking regions of the target sequence. If the same primer is to amplify multiple closely related target sequences, then it is necessary to make the primers ""degenerate"", which would allow it to hybridize to target sequences with a limited amount of variability that may have been caused by mutations. However, the PCR technique can only allow a limited amount of degeneracy, and therefore the design of degenerate primers requires the identification of reasonably well-conserved regions in the input sequences. We take an existing algorithm for designing degenerate primers that is based on clustering and parallelize it in a web-accessible software package GPUDePiCt, using a shared memory model and the computing power of Graphics Processing Units (GPUs). We test our implementation on large sets of aligned sequences from the human genome and show a multi-fold speedup for clustering using our hybrid GPU/CPU implementation over a pure CPU approach for these sequences, which consist of more than 7,500 nucleotides. We also demonstrate that this speedup is consistent over larger numbers and longer lengths of aligned sequences. 2004-2012 IEEE.",,
Narasimhan,"Yan J., Zhang K., Zhang C., Chen S.-C., Narasimhan G.","The snake algorithm has been proposed to solve many remote sensing and computer vision problems such as object segmentation, surface reconstruction, and object tracking. This paper introduces a framework for 3-D building model construction from LIDAR data based on the snake algorithm. It consists of nonterrain object identification, building and tree separation, building topology extraction, and adjustment by the snake algorithm. The challenging task in applying the snake algorithm to building topology adjustment is to find the global minima of energy functions derived for 2-D building topology. The traditional snake algorithm uses dynamic programming for computing the global minima of energy functions which is limited to snake problems with 1-D topology (i.e., a contour) and cannot handle problems with 2-D topology. In this paper, we have extended the dynamic programming method to address the snake problems with a 2-D planar topology using a novel graph reduction technique. Given a planar snake, a set of reduction operations is defined and used to simplify the graph of the planar snake into a set of isolated vertices while retaining the minimal energy of the graph. Another challenging task for 3-D building model reconstruction is how to enforce different kinds of geometric constraints during building topology refinement. This framework proposed two energy functions, deviation and direction energy functions, to enforce multiple geometric constraints on 2-D topology refinement naturally and efficiently. To examine the effectiveness of the framework, the framework has been applied on different data sets to construct 3-D building models from airborne LIDAR data. The results demonstrate that the proposed snake algorithm successfully found the global optima in polynomial time for all of the building topologies and generated satisfactory 3-D models for most of the buildings in the study areas. 2014 IEEE.",,
Narasimhan,"Balasubramanian D., Kumari H., Jaric M., Fernandez M., Turner K.H., Dove S.L., Narasimhan G., Lory S., Mathee K.","Pathogenicity of Pseudomonas aeruginosa, a major cause of many acute and chronic human infections, is determined by tightly regulated expression of multiple virulence factors. Quorum sensing (QS) controls expression of many of these pathogenic determinants. Previous microarray studies have shown that the AmpC ?-lactamase regulator AmpR, a member of the LysR family of transcription factors, also controls non-?-lactam resistance and multiple virulence mechanisms. Using RNA-Seq and complementary assays, this study further expands the AmpR regulon to include diverse processes such as oxidative stress, heat shock and iron uptake. Importantly, AmpR affects many of these phenotypes, in part, by regulating expression of non-coding RNAs such as rgP32, asRgsA, asPrrF1 and rgRsmZ. AmpR positively regulates expression of the major QS regulators LasR, RhlR and MvfR, and genes of the Pseudomonas quinolone system. Chromatin immunoprecipitation (ChIP)-Seq and ChIP-quantitative real-time polymerase chain reaction studies show that AmpR binds to the ampC promoter both in the absence and presence of ?-lactams. In addition, AmpR directly binds the lasR promoter, encoding the QS master regulator. Comparison of the AmpR-binding sequences from the transcriptome and ChIP-Seq analyses identified an AT-rich consensus-binding motif. This study further attests to the role of AmpR in regulating virulence and physiological processes in P. aeruginosa. 2013 The Author(s).",,
Narasimhan,"Caille O., Zincke D., Merighi M., Balasubramanian D., Kumari H., Kong K.-F., Silva-Herzog E., Narasimhan G., Schneper L., Lory S., Mathee K.","Pseudomonas aeruginosa is a dreaded pathogen in many clinical settings. Its inherent and acquired antibiotic resistance thwarts therapy. In particular, derepression of the AmpC ?-lactamase is a common mechanism of ?-lactam resistance among clinical isolates. The inducible expression of ampC is controlled by the global LysR-type transcriptional regulator (LTTR) AmpR. In the present study, we investigated the genetic and structural elements that are important for ampC induction. Specifically, the ampC (PampC) and ampR (PampR) promoters and the AmpR protein were characterized. The transcription start sites (TSSs) of the divergent transcripts were mapped using 5' rapid amplification of cDNA ends-PCR (RACE-PCR), and strong ?54 and ?70 consensus sequences were identified at PampR and PampC, respectively. Sigma factor RpoN was found to negatively regulate ampR expression, possibly through promoter blocking. Deletion mapping revealed that the minimal PampC extends 98 bp upstream of the TSS. Gel shifts using membrane fractions showed that AmpR binds to PampC in vitro whereas in vivo binding was demonstrated using chromatin immunoprecipitation-quantitative PCR (ChIP-qPCR). Additionally, site-directed mutagenesis of the AmpR helix-turn-helix (HTH) motif identified residues critical for binding and function (Ser38 and Lys42) and critical for function but not binding (His39). Amino acids Gly102 and Asp135, previously implicated in the repression state of AmpR in the enterobacteria, were also shown to play a structural role in P. aeruginosa AmpR. Alkaline phosphatase fusion and shaving experiments suggest that AmpR is likely to be membrane associated. Lastly, an in vivo cross-linking study shows that AmpR dimerizes. In conclusion, a potential membrane-associated AmpR dimer regulates ampC expression by direct binding. 2014, American Society for Microbiology.",,
Narasimhan,"Jaric M., Segal J., Silva-Herzog E., Schneper L., Mathee K., Narasimhan G.","Current methods of understanding microbiome composition and structure rely on accurately estimating the number of distinct species and their relative abundance. Most of these methods require an efficient PCR whose forward and reverse primers bind well to the same, large number of identifiable species, and produce amplicons that are unique. It is therefore not surprising that currently used universal primers designed many years ago are not as efficient and fail to bind to recently cataloged species. We propose an automated general method of designing PCR primer pairs that abide by primer design rules and uses current sequence database as input. Since the method is automated, primers can be designed for targeted microbial species or updated as species are added or deleted from the database. In silico experiments and laboratory experiments confirm the efficacy of the newly designed primers for metagenomics applications. 2013 Jaric et al.; licensee BioMed Central Ltd.",,
Narasimhan,"Jaric M., Segal J., Silva-Herzog E., Schneper L., Mathee K., Narasimhan G.","Current methods of understanding microbiome composition and structure rely on accurately estimating the number species and their relative abundance. Most of these methods require an efficient PCR whose forward and reverse primers bind well to the same, large number of identifiable species, and produce amplicons that are unique. It is therefore not surprising that currently used universal primers designed many years ago are not as efficient and fail to bind to recently cataloged species. We propose an automated general method of designing PCR primer pairs that abide by primer design rules and since the method is automated, primers can be designed for targeted groups of microbial species or updated when a database is updated. In silico experiments and laboratory experiments confirm the efficacy of the newly designed primers for metagenomics applications. 2013 IEEE.",,
Narasimhan,"Fernandez M., Jaric M., Schneper L., Segal J., Silva-Herzog E., Campos M., Fishman J., Salathe M., Wanner A., Infante J., Mathee K., Narasimhan G.","Current research has shown that different sites of the human body house different bacterial communities. There is a strong correlation between an individual's microbial community profile at a given site and the onset of disease. Chronic Obstructive Pulmonary Disease (COPD) is a progressive lung disease resulting in narrowing of the airways and restricted airflow. Despite being the third leading cause of death in the United States, little is known about the differences in the lung microbial community profiles of healthy individuals vs. COPD patients. Metagenomics is the culture-independent study of genetic material obtained directly from samples. A metagenomic analysis of 56 individuals was conducted. Bronchoalveolar lavage (BAL) samples were collected from COPD patients, active or ex-smokers, and never smokers. 454 pyrosequencing of 16S rRNA was performed and analyzed using a newly designed, modular bioinformatic workflow. Substantial colonization of the lungs was found in all subjects and differentially abundant genera in each group were identified (including Tropheryma in COPD and Sneathia in smokers). These discoveries are promising and may further our understanding of how the structure of the lung microbiome is modified as COPD progresses. It is also anticipated that the results will eventually lead to improved treatments for COPD. 2013 IEEE.",,
Narasimhan,"Consuegra M.E., Narasimhan G.","We introduce the concept of Avatar problems that deal with situations where each entity has multiple copies or ""avatars"" and the solutions are constrained to use exactly one of the avatars. The resulting set of problems show a surprising range of hardness characteristics and elicit a variety of algorithmic solutions. Many avatar problems are considered. In particular, we show how to extend the concept of-kernels to find approximation algorithms for geometric avatar problems. Results for metric space graph avatar problems are also presented. Mario E. Consuegra and Giri Narasimhan;.",,
Narasimhan,"Consuegra M.E., Narasimhan G., Rangaswami R.","In this paper we experiment with practical algorithms for the vector repacking problem and its variants. Vector repacking, like vector packing, aims to pack a set of input vectors such that the number of bins used is minimized, while minimizing the changes from the previous packing. We also consider a variant of vector repacking that stores additional copies of items with the goal of improving the performance of vector repacking algorithms. In addition, our algorithms are parameterized so that they can be effectively optimized for a variety of resource allocation applications with different input characteristics and different cost functions. 2013 IEEE.",,
Narasimhan,"Zeng E., Ding C., Mathee K., Schneper L., Narasimhan G.","Almost every cellular process requires the interactions of pairs or larger complexes of proteins. The organization of genes into networks has played an important role in characterizing the functions of individual genes and the interplay between various cellular processes. The Gene Ontology (GO) project has integrated information from multiple data sources to annotate genes to specific biological process. Recently, the semantic similarity (SS) between GO terms has been investigated and used to derive semantic similarity between genes. Such semantic similarity provides us with a new perspective to predict protein functions and to generate functional gene networks. In this chapter, we focus on investigating the semantic similarity between genes and its applications. We have proposed a novel method to evaluate the support for PPI data based on gene ontology information. If the semantic similarity between genes is computed using gene ontology information and using Resniks formula, then our results show that we can model the PPI data as a mixture model predicated on the assumption that true protein-protein interactions will have higher support than the false positives in the data. Thus semantic similarity between genes serves as a metric of support for PPI data. Taking it one step further, new function prediction approaches are also being proposed with the help of the proposed metric of the support for the PPI data. These new function prediction approaches outperform their conventional counterparts. New evaluation methods are also proposed. In another application, we present a novel approach to automatically generate a functional network of yeast genes using Gene Ontology (GO) annotations. An semantic similarity (SS) is calculated between pairs of genes. This SS score is then used to predict linkages between genes, to generate a functional network. Functional networks predicted by SS and other methods are compared. The network predicted by SS scores outperforms those generated by other methods in the following aspects: automatic removal of a functional bias in network training reference sets, improved precision and recall across the network, and higher correlation between a genes lethality and centrality in the network. We illustrate that the resulting network can be applied to generate coherent function modules and their associations. We conclude that determination of semantic similarity between genes based upon GO information can be used to generate a functional network of yeast genes that is comparable or improved with respect to those that are directly based on integrated heterogeneous genomic and proteomic data. Springer-Verlag Berlin Heidelberg 2011.",,
Narasimhan,"Balasubramanian D., Schneper L., Merighi M., Smith R., Narasimhan G., Lory S., Mathee K.","In Enterobacteriaceae, the transcriptional regulator AmpR, a member of the LysR family, regulates the expression of a chromosomal ?-lactamase AmpC. The regulatory repertoire of AmpR is broader in Pseudomonas aeruginosa, an opportunistic pathogen responsible for numerous acute and chronic infections including cystic fibrosis. In addition to regulating ampC, P. aeruginosa AmpR regulates the sigma factor AlgT/U and production of some quorum sensing (QS)-regulated virulence factors. In order to better understand the ampR regulon, we compared the transcriptional profile generated using DNA microarrays of the prototypic P. aeruginosa PAO1 strain with its isogenic ampR deletion mutant, PAO?ampR. Transcriptome analysis demonstrates that the AmpR regulon is much more extensive than previously thought, with the deletion of ampR influencing the differential expression of over 500 genes. In addition to regulating resistance to ?-lactam antibiotics via AmpC, AmpR also regulates non-?-lactam antibiotic resistance by modulating the MexEF-OprN efflux pump. Other virulence mechanisms including biofilm formation and QS-regulated acute virulence factors are AmpR-regulated. Real-time PCR and phenotypic assays confirmed the microarray data. Further, using a Caenorhabditis elegans model, we demonstrate that a functional AmpR is required for P. aeruginosa pathogenicity. AmpR, a member of the core genome, also regulates genes in the regions of genome plasticity that are acquired by horizontal gene transfer. Further, we show differential regulation of other transcriptional regulators and sigma factors by AmpR, accounting for the extensive AmpR regulon. The data demonstrates that AmpR functions as a global regulator in P. aeruginosa and is a positive regulator of acute virulence while negatively regulating biofilm formation, a chronic infection phenotype. Unraveling this complex regulatory circuit will provide a better understanding of the bacterial response to antibiotics and how the organism coordinately regulates a myriad of virulence factors in response to antibiotic exposure. 2012 Balasubramanian et al.",,
Narasimhan,"Cattoir V., Narasimhan G., Skurnik D., Aschard H., Roux D., Ramphal R., Jyot J., Lory S.","Adaptation of bacterial pathogens to a host can lead to the selection and accumulation of specific mutations in their genomes with profound effects on the overall physiology and virulence of the organisms. The opportunistic pathogen Pseudomonas aeruginosa is capable of colonizing the respiratory tract of individuals with cystic fibrosis (CF), where it undergoes evolution to optimize survival as a persistent chronic human colonizer. The transcriptome of a host-adapted, alginate-overproducing isolate from a CF patient was determined following growth of the bacteria in the presence of human respiratory mucus. This stable mucoid strain responded to a number of regulatory inputs from the mucus, resulting in an unexpected repression of alginate production. Mucus in the medium also induced the production of catalases and additional peroxide-detoxifying enzymes and caused reorganization of pathways of energy generation. A specific antibacterial type VI secretion system was also induced in mucus-grown cells. Finally, a group of small regulatory RNAs was identified and a fraction of these were mucus regulated. This report provides a snapshot of responses in a pathogen adapted to a human host through assimilation of regulatory signals from tissues, optimizing its long-term survival potential. 2012 Cattoir et al.",,
Narasimhan,Narasimhan G.,"As microbial sequencing data starts to pour in at an increasing rate, comparative genomics holds the keys to decipher and mine this wealth of information. We discuss the diverse ways in which the availability of comparative genomics data allows us to answer more questions with greater ease and greater confidence. 2011 IEEE.",,
Narasimhan,"Yang X., Medvin D., Narasimhan G., Yoder-Himes D., Lory S.","Closing of gaps in draft assemblies using Next Generation Sequencing (NGS) data is becoming increasingly important. In this paper we present CloG, a software pipeline that uses NGS data to close gaps in draft assemblies. Firstly, CloG uses the VELVET assembler to generate a hybrid assembly from a mixture of reads: short reads from the NGS data and the original draft assembly (treated as long reads). It then closes gaps between adjacent contigs by reconciling (i.e., ""stitching"") the two assemblies. By exploiting the strengths of both hybrid assembly and stitching reassembly, CloG is able to outperform its contemporaries in closing gaps in the draft assembly of the bacterium Burkholderia dolosa. 2011 IEEE.",,
Narasimhan,"Weeks O., Villamor E., Tracey M., Stoddard P., Shapiro S., Makemson J., Garcia R., Gavassa S., Philippi T., Pitzer T., Dewsbury B., Narasimhan G., McGoron A., Bhaijee S., Alberte J., Graves P., G„mez R., Koptur S., Galvez M., Heffernan J.B., Kos L., Lowenstein M., Rosenblatt A., Baker J., Quirke M., Brewe E., Tashakkori A.","QBIC (Quantifying Biology in the Classroom) is a reformed four-year (freshmansenior) program within the Biological Sciences Department at Florida International University. QBIC was implemented with a cohort of 23 freshmen fall 2007, a second cohort of freshmen fall 2008, and two cohorts (3rd and 4th) entered the program fall 2009. The blueprint for QBIC was developed beginning 2004 over a 2-year period by 12 faculty members in biological sciences, mathematics, statistics, chemistry, physics, computer science and biomedical engineering. We are working towards better preparing 21st century biology students with tools to think more quantitatively, by offering them a more rigorous interdisciplinary and quantitative curriculum. 2011, Foundation Journal of Science Education. All rights reserved.",,
Narasimhan,"Doud M.S., Light M., Gonzalez G., Narasimhan G., Mathee K.","Chronic bronchopulmonary bacterial infections remain the most common cause of morbidity and mortality among patients with cystic fibrosis (CF). Recent community sequencing work has now shown that the bacterial community in the CF lung is polymicrobial. Identifying bacteria in the CF lung through sequencing can be costly and is not practical for many laboratories. Molecular techniques such as terminal restriction fragment length polymorphism or amplicon length heterogeneity-polymerase chain reaction (LH-PCR) can provide many laboratories with the ability to study CF bacterial communities without costly sequencing. The aim of this study was to determine if the use of LH-PCR with multiple hypervariable regions of the 16S rRNA gene could be used to identify organisms found in sputum DNA. This work also determined if LH-PCR could be used to observe the dynamics of lung infections over a period of time. Nineteen samples were analysed with the V1 and the V1_V2 region of the 16S rRNA gene. Based on the amplicon size present in the V1_V2 region, Pseudomonas aeruginosa was confirmed to be in all 19 samples obtained from the patients. The V1 region provided a higher power of discrimination between bacterial profiles of patients. Both regions were able to identify trends in the bacterial population over a period of time. LH profiles showed that the CF lung community is dynamic and that changes in the community may in part be driven by the patient's antibiotic treatment. LH-PCR is a tool that is well suited for studying bacterial communities and their dynamics.",,
Narasimhan,"Zhang Y., Zeng E., Li T., Narasimhan G.","In this article we present a new approach - weighted consensus clustering to identify the clusters in Protein-protein interaction (PPI) networks where each cluster corresponds to a group of functionally similar proteins. In weighed consensus clustering, different input clustering results weigh differently, i.e., a weight for each input clustering is introduced and the weights are automatically determined by an optimization process. We evaluate our proposed method with standard measures such as modularity, normalized mutual information (NMI) and the Gene Ontology (GO) consortium database and compare the performance of our approach with other consensus clustering methods . Experimental results demonstrate the effectiveness of our proposed approach. 2009 IEEE.",,
Narasimhan,"Kunkle B., Felty Q., Narasimhan G., Trevino F., Roy D.","The objective of this study was to use Oncomine (a database of DNA microarrays) to identifies NRF1, Tfam and Myc co-expressed genes and their possible implication in childhood brain tumor (CBT) development through controlling mitochondrial biogenesis. A meta-analysis was performed on several microarray studies on brain cancer found within Oncomine. The three main target genes of the meta-analysis were: V-myc myelocytomatosis viral oncogene homolog (avian) (Myc), Tfam: transcription factor A, mitochondrial (mt) (Tfam), and nuclear respiratory factor-1 (NRF1), that are involved in the mitochondrial biogenesis. For the Myc and Tfam meta-analysis we selected 5 studies having co-expression of brain data for Tfam; these same studies were selected for Myc. The meta-analysis included 5 studies with a total of 111 microarrays. A total of 208 coexpressed Myc genes with a significance of 40%+ (significant in 2 of 5 studies) and 206 co-expressed Tfam genes with significance of 40%+ were identified. 9 significant genes overlapped between Myc and Tfam: ALCAM, BMP2, CALCRL, CDH11, DUSP4, EMP1, SMAD3, SNAP23, and UBE2D1. A program called FuncAssociate was used to perform GO Term Enrichment analysis to obtain characteristics of the set of significant genes for both Tfam and Myc. The top GO Term for Myc was 'Phosphoribosylamine-glycine ligase activity'. The function of this GO Term is the Catalysis of the reaction: ATP + 5-phospho-D-ribosylamine + glycine = ADP + phosphate + N1-(5-phospho-D-ribosyl)glycine. Interestingly, this is part of the electron transport chain and thus mitochondrial biogenesis. The top GO Term for Tfam was 'Peptide-aspartate beta-dioxygenase activity'. This functions in the catalysis of the reaction: peptide L-aspartate + 2-oxoglutarate + O2 = peptide 3-hydroxy-L-aspartate + succinate + CO2. We expanded the meta-analysis to include NRF1 which also controls mitochondrial biogenesis via regulating Tfam. Interestingly, this gene has the same promoter recognition sequence as Myc . This meta-analysis, which combined 33 microarrays from two different studies, identified 26 genes co-expressed by all three genes (NRF1, Tfam, and Myc). Of these genes, three were found to be significantly co-expressed in the original metaanalysis of Tfam and myc. They were SMAD3, SNAP23 and UBE2D1. This study was able to identify a set of genes significantly correlated with NRF1, Myc and Tfam that control mitochondrial biogenesis. Furthermore, it identified 9 possible pathway partners of Myc and Tfam and a number of functions enriched in Myc, Tfam, and NRF1 in brain tumors. Since NRF1, Myc and Tfam genes play a large part in both brain development and mitochondrial biogenesis, the identification of co-expressed genes in similar pathways of these three mitochondrial biogenesis controlling genes could be key in elucidating how dysfunction of mitochondrial signaling may be involved in the development of childhood brain tumors.",,
Narasimhan,"Gonzalez G., Doud M., Mathee K., Narasimhan G.","Most microorganisms cannot be cultured and are difficult to identify. One of the most widely used markers to help identify bacteria is the ribosomal RNA gene. A component of the small ribosomal subunit, 16S rRNA is composed of alternating evolutionarily conserved and hypervariable regions. One strategy is to exploit the length heterogeneity in the highly variable regions and to use it for microbial identification. Techniques based on the polymerase chain reaction (PCR) are ideally suited for studying length heterogeneity of the 16S rRNA hypervariable regions. PCR primers were designed using the conserved regions (V1 and V1-V2) of the gene and the lengths of the resulting amplicons were estimated in the laboratory. The aim of this project is to design a computer program that takes as input the amplicon lengths arising from a PCR experiment with a given pair of primers and to output the set of known bacteria that can result in those sequence lengths. However, there are thousands of microbial organisms that display the exact same amplicon length for a given pair of primers. If two or more pairs of primers are used and the amplicon lengths estimated using PCR, then there is a much better chance of correctly identifying the bacterial organisms present in a sample. AmpliQu_ is a BioPerl program that addresses this problem. AmpliQu_ receives as input two pairs of primers and the lengths of the amplicons from the PCR experiment. It then reports all the microbial organisms that would result in those amplicon lengths. It uses the 16S rRNA sequence database from the Ribosomal Database Project (URL: http://rdp.cme.msu.edu/). Each set of primers was run independently against the 16S rRNA database using BLAST. Results from the BLAST hits were then merged into a single table. This resulting table was then queried with the observed amplicon lengths from the PCR experiments. 2009 Springer Berlin Heidelberg.",,
Narasimhan,"Klein R., Knauer C., Narasimhan G., Smid M.","Let G be a graph with n vertices which is embedded in Euclidean space R d. For any two vertices of G, their dilation is defined to be the ratio of the length of a shortest connecting path in G to the Euclidean distance between them. In this paper, we study the spectrum of the dilation, over all pairs of vertices of G. For paths, cycles, and trees in R 2, we present O(n3 /2+)-time randomized algorithms that compute, for a given value ?&gt;1, the exact number of vertex pairs of dilation at most ?. Then we present deterministic algorithms that approximate the number of vertex pairs of dilation at most ? to within a factor of 1+. They run in O(nlog 2n) time for paths and cycles, and in O(nlog 3n) time for trees, in any constant dimension d. 2009 Elsevier B.V. All rights reserved.",,
Narasimhan,"Buendia P., Narasimhan G.","Background: The HIV virus is known for its ability to exploit numerous genetic and evolutionary mechanisms to ensure its proliferation, among them, high replication, mutation and recombination rates. Sliding MinPD, a recently introduced computational method 1, was used to investigate the patterns of evolution of serially-sampled HIV-1 sequence data from eight patients with a special focus on the emergence of X4 strains. Unlike other phylogenetic methods, Sliding MinPD combines distance-based inference with a nonparametric bootstrap procedure and automated recombination detection to reconstruct the evolutionary history of longitudinal sequence data. We present serial evolutionary networks as a longitudinal representation of the mutational pathways of a viral population in a within-host environment. The longitudinal representation of the evolutionary networks was complemented with charts of clinical markers to facilitate correlation analysis between pertinent clinical information and the evolutionary relationships . Results: Analysis based on the predicted networks suggests the following:: significantly stronger recombination signals (p = 0.003) for the inferred ancestors of the X4 strains, recombination events between different lineages and recombination events between putative reservoir virus and those from a later population, an early star-like topology observed for four of the patients who died of AIDS. A significantly higher number of recombinants were predicted at sampling points that corresponded to peaks in the viral load levels (p = 0.0042). Conclusion: Our results indicate that serial evolutionary networks of HIV sequences enable systematic statistical analysis of the implicit relations embedded in the topology of the structure and can greatly facilitate identification of patterns of evolution that can lead to specific hypotheses and new insights. The conclusions of applying our method to empirical HIV data support the conventional wisdom of the new generation HIV treatments, that in order to keep the virus in check, viral loads need to be suppressed to almost undetectable levels. 2009 Buendia and Narasimhan; licensee BioMed Central Ltd.",,
Narasimhan,"Gudmundsson J., Van Kreveld M., Narasimhan G.","Clustering problems in a complex geographical setting are often required to incorporate the type and extent of land cover within a region. Given a set P of n points in a geographical setting, with the constraint that the points of P can only occur in one type of land cover, an interesting problem is the detection of clusters. First, we extend the definition of clusters and define the concept of a region-restricted cluster that satisfies the following properties: (i) the cluster has sufficient number of points, (ii) the cluster points are confined to a small geographical area, and (iii) the amount of land cover of the specific type in which the points lie is also small. Next, we give efficient exact and approximation algorithms for computing such clusters. The exact algorithm determines all axis-parallel squares with exactly m out of n points inside, size at most some prespecified value, and area of a given land cover type at most another prespecified value, and runs in O(nm log2n+(nm+n nf) log2nf) time, where nf is the number of edges that bound the regions with the given land cover type. The approximation algorithm allows the square to be a factor 1+ too large, and runs in O(nlogn+n/ 2+ nflog2 nf+(n log2nf)/(m 2)) time. We also show how to compute largest clusters and outliers. Copyright 2008 Published by Elsevier B.V. All rights reserved.",,
Narasimhan,"Doud M., Zeng E., Schneper L., Narasimhan G., Mathee K.","Microbial communities play vital roles in many aspects of our lives, although our understanding of microbial biogeography and community profiles remains unclear. The number of microbes or the diversity of the microbes, even in small environmental niches, is staggering. Current microbiological methods used to analyse these communities are limited, in that many microorganisms cannot be cultured. Even for the isolates that can be cultured, the expense of identifying them definitively is much too high to be practical. Many recent molecular technologies, combined with bioinformatic tools, are raising the bar by improving the sensitivity and reliability of microbial community analysis. These tools and techniques range from those that attempt to understand a microbial community from their length heterogeneity profiles to those that help to identify the strains and species of a random sampling of the microbes in a given sample. These technologies are reviewed here, using the microbial communities present in the lungs of cystic fibrosis patients as a paradigm.",,
Narasimhan,"Zheng G., Narasimhan G.","With the unprecedented growth in the size of sequence and structure databases, knowledge-based methods have become increasingly feasible for protein structure prediction. We developed a branch-and-bound method for structlets-based protein structure assembly. We explore the effectiveness of this approach by examining its capability to reconstruct the 3D structure of some proteins with known 3D structures. Although our algorithm involves exhaustive search, our BestFirst implementation of a branch-and bound strategy is able to eliminate around 2/3 of the total search space in order to find the optimal 3D assembly for a protein of interest.",,
Narasimhan,"Zeng E., Ding C., Narasimhan G., Holbrook S.R.","Almost every cellular process requires the interactions of pairs or larger complexes of proteins. High throughput protein-protein interaction (PPI) data have been generated using techniques such as the yeast two-hybrid systems, mass spectrometry method, and many more. Such data provide us with a new perspective to predict protein functions and to generate protein-protein interaction networks, and many recent algorithms have been developed for this purpose. However, PPI data generated using high throughput techniques contain a large number of false positives. In this paper, we have proposed a novel method to evaluate the support for PPI data based on gene ontology information. If the semantic similarity between genes is computed using gene ontology information and using Resnik's formula, then our results show that we can model the PPI data as a mixture model predicated on the assumption that true protein-protein interactions will have higher support than the false positives in the data. Thus semantic similarity between genes serves as a metric of support for PPI data. Taking it one step further, new function prediction approaches are also being proposed with the help of the proposed metric of the support for the PPI data. These new function prediction approaches outperform their conventional counterparts. New evaluation methods are also proposed.",,
Narasimhan,"Zeng E., Narasimhan G., Schneper L., Mathee K.","In the post-genomic era, the organization of genes into networks has played an important role in characterizing the functions of individual genes and the interplay between them. It is also vital in understanding complex cellular processes and their dynamics. Despite advances, gene network prediction still remains a challenge. Recently, heterogeneous genomic and proteomic data were integrated to generate a functional network of yeast genes. The Gene Ontology (GO) project has integrated information from multiple data sources to annotate genes to specific biological process. Generating gene networks using GO annotations is a novel and alternative way to efficiently integrate heterogeneous data sources. In this paper, we present a novel approach to automatically generate a functional network of yeast genes using Gene Ontology (GO) annotations. An information theoretic semantic similarity (SS) was calculated between every pair of genes based on the method proposed by Resnik. This SS score was then used to predict linkages between genes, to generate a functional network. An alternative approach has been proposed using a measure called log likelihood score (LLS). The Functional networks predicted using the SS and LLS measures were compared. We discussed our experiments on generating reliable functional gene networks and concluded that the functional network generated by SS scores is comparable to or better than those obtained using LLS scores. 2008 IEEE.",,
Narasimhan,"Entry J.A., Mills D., Mathee K., Jayachandran K., Sojka R.E., Narasimhan G.","Organic carbon (C), bacterial biomass and structural community diversity were measured in Southern Idaho soils with long term cropping histories. The soils tested were native sagebrush vegetation (NSB), irrigated moldboard plowed crops (IMP), irrigated conservation - chisel - tilled crops (ICT) and irrigated pasture systems (IP). Organic C concentration in soils decreased in the order NSB 0-5 cm &gt; IP 0-30 cm = ICT 0-15 cm &gt; IMP 0-30 cm &gt; NSB 5-15 cm = NSB 15-30 cm. Active bacterial, fungal and microbial biomass correlated with soil C as measured by the Walkely Black method in positive curvilinear relationships (r2 = 0.93, 0.80 and 0.76, respectively). Amplicon length heterogeneity (LH-PCR) DNA profiling was used to access the eubacterial diversity in all soils and at all depths. The Shannon-Weaver diversity index was used to measure the differences using the combined data from three hypervariable domains of the eubacterial 16S rRNA genes. Diversity was greatest in NSB 15-30 cm soil and lowest in the IMP soil. With the exception of IMP with the lowest diversity index, the samples highest in C (NSB 0-5 cm, IP 0-30 cm, ICT 0-15 cm) reflected lower diversity indices. However, these indices were not significantly different from each other. ICT and IP increase soil C and to some extent increase diversity relative to IMP. Since soil bacteria respond quickly to environmental changes, monitoring microbial communities may be one way to assess the impact of agricultural practices such as irrigation and tillage regimes.",,
Narasimhan,"Gudmundsson J., Levcopoulos C., Narasimhan G., Smid M.","Given an arbitrary real constant ? > 0, and a geometric graph G in d-dimensional Euclidean space with n points, O(n) edges, and constant dilation, our main result is a data structure that answers (1 + ?)-approximate shortest-path-length queries in constant time. The data structure can be constructed in O(n log n) time using O(n log n) space. This represents the first data structure that answers (1 + ?)-approximate shortest-path queries in constant time, and hence functions as an approximate distance oracle. The data structure is also applied to several other problems. In particular, we also show that approximate shortest-path queries between vertices in a planar polygonal domain with rounded obstacles can be answered in constant time. Other applications include query versions of closest-pair problems, and the efficient computation of the approximate dilations of geometric graphs. Finally, we show how to extend the main result to answer (1 + ?)-approximate shortest-path-length queries in constant time for geometric spanner graphs with m = (n) edges. The resulting data structure can be constructed in O(m + n log n) time using O(n log n) space. 2008 ACM.",,
Narasimhan,"Mathee K., Narasimhan G., Valdes C., Qiu X., Matewish J.M., Koehrsen M., Rokas A., Yandava C.N., Engels R., Zeng E., Olavarietta R., Doud M., Smith R.S., Montgomery P., White J.R., Godfrey P.A., Kodira C., Birren B., Galagan J.E., Lory S.","One of the hallmarks of the Gram-negative bacterium Pseudomonas aeruginosa is its ability to thrive in diverse environments that includes humans with a variety of debilitating diseases or immune deficiencies. Here we report the complete sequence and comparative analysis of the genomes of two representative P. aeruginosa strains isolated from cystic fibrosis (CF) patients whose genetic disorder predisposes them to infections by this pathogen. The comparison of the genomes of the two CF strains with those of other P. aeruginosa presents a picture of a mosaic genome, consisting of a conserved core component, interrupted in each strain by combinations of specific blocks of genes. These strain-specific segments of the genome are found in limited chromosomal locations, referred to as regions of genomic plasticity. The ability of P. aeruginosa to shape its genomic composition to favor survival in the widest range of environmental reservoirs, with corresponding enhancement of its metabolic capacity is supported by the identification of a genomic island in one of the sequenced CF isolates, encoding enzymes capable of degrading terpenoids produced by trees. This work suggests that niche adaptation is a major evolutionary force influencing the composition of bacterial genomes. Unlike genome reduction seen in host-adapted bacterial pathogens, the genetic capacity of P. aeruginosa is determined by the ability of individual strains to acquire or discard genomic segments, giving rise to strains with customized genomic repertoires. Consequently, this organism can survive in a wide range of environmental reservoirs that can serve as sources of the infecting organisms. 2008 by The National Academy of Sciences of the USA.",,
Narasimhan,"Buendia P., Collins T.M., Narasimhan G.","Algorithms that infer phylogenetic relationships between serially-sampled sequences have been developed in recent years to assist in the analysis of rapidly-evolving human pathogens. Our study consisted of evaluating seven relevant methods using empirical as well as simulated data sets. In particular, we investigated how the molecular clock hypothesis affected their relative performance, as three of the algorithms that accept serially-sampled data as input assume a molecular clock. Our results show that the standard phylogenetic methods and MinPD had a better overall performance. Surprisingly, when all internal node sequences were included in the data, the topological performance measure of all the methods, with the exception of MinPD, dropped significantly. Copyright 2008 Inderscience Enterprises Ltd.",,
Narasimhan,"Zeng E., Yang C., Li T., Narasimhan G.","In this paper, we have modified a constrained clustering algorithm to perform exploratory analysis on gene expression data using prior knowledge presented in the form of constraints. We have also studied the effectiveness of various constraints sets. To address the problem of automatically generating constraints from biological text literature, we considered two methods (cluster-based and similarity-based). We concluded that incomplete information in the form of constraints set should be generated carefully, in order to outperform the standard clustering algorithm, which works on the data source without any constraints. For sufficiently large constraints sets, the constrained clustering algorithm outperformed the MSC algorithm. The novelty of research presented here is the study of effectiveness of constraints sets and robustness of the constrained clustering algorithm using multiple sources of biological data, and incorporating biomedical text literature into constrained clustering algorithm in form of constraints sets. ©2007 IEEE.",,
Narasimhan,"Milledge T., Zheng G., Mullins T., Narasimhan G.","While much research has been done on finding similarities between protein sequences, there has not been the same progress on finding similarities between protein structures. Here we report a new algorithm (SBLAST) which discovers the largest common substructures between two proteins using a triangle-based variant of the geometric hashing of protein structures algorithm. The algorithm selects triples (triangles) of selected C? atoms from all proteins in a protein structure database and creates a hash table using a key based on the three inter-atomic distances. Hash table hits from the triangles of a query protein are extended recursively to determine the largest common substructures less than a threshold deviation level (rmsd). Comparisons between a query protein and a preprocessed protein database can be performed in parallel. Because SBLAST does not rely on protein sequence alignment, common substructures can be detected in the absence of sequence conservation. SBLAST has been tested using the ASTRAL subset of the PDB. ©2007 IEEE.",,
Narasimhan,"Zeng E., Mathee K., Narasimhan G.","Understanding gene regulation is a key step to investigating gene functions and their relationships. Many algorithms have been developed to discover transcription factor binding sites (TFBS); they are predominantly located in upstream regions of genes and contribute to transcription regulation if they are bound by a specific transcription factor. However, traditional methods focusing on finding motifs have shortcomings, which can be overcome by using comparative genomics data that is now increasingly available. Traditional methods to score motifs also have their limitations. In this paper, we propose a new algorithm called IEM to refine motifs using comparative genomics data. We show the effectiveness of our techniques with several data sets. Two sets of experiments were performed with comparative genomics data on five strains of P. aeruginosa. One set of experiments were performed with similar data on four species of yeast. The weighted conservation score proposed in this paper is an improvement over existing motif scores.",,
Narasimhan,"Buendia P., Narasimhan G.","Motivation: Traditional phylogenetic methods assume tree-like evolutionary models and are likely to perform poorly when provided with sequence data from fast-evolving, recombining viruses. Furthermore, these methods assume that all the sequence data are from contemporaneous taxa, which is not valid for serially-sampled data. A more general approach is proposed here, referred to as the Sliding MinPD method, that reconstructs evolutionary networks for serially-sampled sequences in the presence of recombination. Results: Sliding MinPD combines distance-based phylogenetic methods with automated recombination detection based on the best-known sliding window approaches to reconstruct serial evolutionary networks. Its performance was evaluated through comprehensive simulation studies and was also applied to a set of serially-sampled HIV sequences from a single patient. The resulting network organizations reveal unique patterns of viral evolution and may help explain the emergence of disease-associated mutants and drug-resistant strains with implications for patient prognosis and treatment strategies. The Author 2007. Published by Oxford University Press. All rights reserved.",,
Narasimhan,"Yan J., Zhang K., Zhang C., Chen S.-C., Narasimhan G.","Energy-minimizing active contour models (snakes) have been proposed for solving many computer vision problems such as object segmentation, surface reconstruction, and object tracking. Dynamic programming which allows natural enforcement of constraints is an effective method for computing the global minima of energy functions. However, this method is only limited to snake problems with one dimensional (1D) topology (i.e., a contour) and cannot handle problems with two-dimensional (2D) topology. In this paper, we have extended the dynamic programming method to address the snake problems with 2D topology using a novel graph reduction algorithm. Given a 2D snake with first order energy terms, a set of reduction operations are defined and used to simplify the graph of the 2D snake into one single vertex while retaining the minimal energy of the snake. The proposed algorithm has a polynomial-time complexity bound and the optimality of the solution for a reducible 2D snake is guaranteed. However, not all types of 2D snakes can be reduced into one single vertex using the proposed algorithm. The reduction of general planar snakes is an NP-Complete problem. The proposed method has been applied to optimize 2D building topology extracted from airborne LIDAR data to examine the effectiveness of the algorithm. The results demonstrate that the proposed approach successfully found the global optima for over 98% of building topology in a polynomial time. ©2007 IEEE.",,
Narasimhan,"Alvarez H.L., Chatfield D., Cox D.A., Crumpler E., D'Cunha C., Gutierrez R., Ibarra J., Johnson E., Kumar K., Milledge T., Narasimhan G., Masoud Sadjadi S., Chi Z.","The ""CyberBridges"" pilot project is an innovative model for creating a new generation of scientists and engineers who are capable of fully integrating cyberinfrastructure into the whole educational, professional, and creative process of their respective disciplines. CyberBridges augments graduate student education to include a foundation of understanding in Advanced Networking and Grid Infrastructure for High Performance Computing, and bridges the divide between the information technology community and diverse science and engineering disciplines. We demonstrate the effectiveness of CyberBridges by providing four case studies. Groundwork has begun to extend the outreach of CyberBridges for international research and education collaborations. 2007 IEEE.",,
Narasimhan,"Erliang Z., Narasimhan G.","Transcription factor binding sites (TFBS) are often located in the upstream regions of genes and transcription factors (TFs) cause transcription regulation by binding at these locations. Predicting these binding sites is a difficult problem, and traditional methods have a high degree of false positives in their predictions. Comparative genomics data can help to improve motif predictions. In this paper, a new strategy is presented, which refines motif by taking the comparative genomics data into account. Tested with the help of both simulation data and biological data, we show that our method makes improved predictions. We also propose a new metric to score a motif profile. This score is biologically motivated and helps the algorithm in its predictions. Springer-Verlag Berlin Heidelberg 2007.",,
Narasimhan,"Buendia P., Narasimhan G.","Determining the evolutionary history of a sampled sequence can become quite complex when multiple recombination events are part of its past. With at least five new recombination detection methods published in the last year, the growing list of over 40 methods suggests that this field is generating a lot of interest. In previous studies comparing recombination detection methods, the evaluation procedures did not measure how many recombinant sequences, breakpoints and donors were correctly identified. In this paper we will present the algorithm RecIdentify that scans a phylogenetic network and uses its edge lengths and topology to identify the parental/donor sequences and breakpoint positions for each query sequence. RecIdentify findings can be used to evaluate the output of recombination detection programs. RecIdentify may also assist in understanding how network size and complexity may shape recombination signals in a set of DNA sequences. The results may prove useful in the phylogenetic study of serially-sampled viral data with recombination events. Springer-Verlag Berlin Heidelberg 2007.",,
Narasimhan,"Gudmundsson J., Narasimhan G., Smid M.","Given a polygonal path P with vertices p1, p2, pn? Rd and a real number t?1, a path Q=(p i1,p i2, pik) is a t-distance-preserving approximation of P if 1= i1&lt; i2&lt;?&lt; ik=n and each straight-line edge (p ij,pij+ 1) of Q approximates the distance between p ij and pij+1 along the path P within a factor of t. We present exact and approximation algorithms that compute such a path Q that minimizes k (when given t) or t (when given k). We also present some experimental results. 2006 Elsevier B.V. All rights reserved.",,
Narasimhan,"Wei P., Tao L., Narasimhan G.","In this paper, we study the problems of motif discovery and gene regulation.First, although the sliding window technique based on profiles or consensus sequences is a standard method for discovering motifs in the genomes with prior knowledge of transcription binding sites in orthologous genes from related organisms, it usually has high computational costs. In this paper, we propose an efficient approximation method employing randomized algorithms to identify motifs. The approximation method can be easily combined with the sliding-window technique for efficient and accurate motif discovery. Second, we mine frequent motif combinations and sequential motif patterns to investigate the regulatory relationships between motifs and provide a better understanding of gene expression, regulation, and transcription. 2006 IEEE.",,
Narasimhan,"Kuhn D.N., Narasimhan G., Nakamura K., Brown J.S., Schnell R.J., Meerow A.W.","Identifying genetic markers linked to disease resistance in plants is an important goal in marker-assisted selection. Using a candidate-gene approach, we have previously developed genetic markers in cacao (Theobroma cacao L.) for two families eigenes involved in disease resistance: non-TIR-NBS-LRR (Toll/Interleukin-1 Receptor-nucleotide binding site-leucine rich repeat) resistance gene homologues and WRKY transcription factor genes; however, we failed to isolate TIR-NBS-LRR genes. Using a novel algorithm to design degenerate primers, we have now isolated TIR-NBS-LRR loci as determined by DNA sequence comparison. These loci have been developed as genetic markers using capillary array electrophoresis (CAE) and single-strand conformational polymorphism (SSCP) analysis. We have mapped three distinct TIR-NBS-LRR loci in an F2 population of cacao and demonstrated that one is located on linkage group 3 and the other two on linkage group 5.",,
Narasimhan,"Buendia P., Narasimhan G.","Summary: Serial NetEvolve is a flexible simulation program that generates DNA sequences evolved along a tree or recombinant network. It offers a user-friendly Windows graphical interface and a Windows or Linux simulator with a diverse selection of parameters to control the evolutionary model. Serial NetEvolve is a modification of the Treevolve program with the following additional features: simulation of serially-sampled data, the choice of either a clock-like or a variable rate model of sequence evolution, sampling from the internal nodes and the output of the randomly generated tree or network in our newly proposed NeTwick format. 2006 Oxford University Press.Pissinou",,
Navlakha,"Li T., Xie N., Zeng C., Zhou W., Zheng L., Jiang Y., Yang Y., Ha H.-Y., Xue W., Huang Y., Chen S.-C., Navlakha J., Iyengar S.S.","Improving disaster management and recovery techniques is one of national priorities given the huge toll caused by man-made and nature calamities. Data-driven disaster management aims at applying advanced data collection and analysis technologies to achieve more effective and responsive disaster management, and has undergone considerable progress in the last decade. However, to the best of our knowledge, there is currently no work that both summarizes recent progress and suggests future directions for this emerging research area. To remedy this situation, we provide a systematic treatment of the recent developments in data-driven disaster management. Specifically, we first present a general overview of the requirements and system architectures of disaster management systems and then summarize state-of-the-art data-driven techniques that have been applied on improving situation awareness as well as in addressing users' information needs in disaster management. We also discuss and categorize general data-mining and machine-learning techniques in disaster management. Finally, we recommend several research directions for further investigations. 2017 ACM.",,
Navlakha,"Yang Y., Lu W., Domack J., Li T., Chen S.-C., Luis S., Navlakha J.K.","With the proliferation of smart devices, disaster responders and community residents are capturing footage, pictures and video of the disaster area with mobile phones and wireless tablets. This multimedia disaster situation information is critical for assisting emergency management (EM) personnel to effectively respond in a timely manner. Currently, however the data is not integrated in incident command systems where situation reports, incidence action plans, etc. are being held. Therefore, we have designed and developed a Multimedia-Aided Disaster information Integration System (MADIS), which utilizes advanced data mining techniques to analyze situation reports and pictures as well as text captured in the field and automatically link the reports directly to relevant multimedia content. Specifically, a dynamic hierarchical image classification approach is proposed to categorize disaster images into different subjects by fusing image and text information. Situation reports are analyzed using advanced document processing techniques and then associated with processed multimedia data. In order to seamlessly incorporate user interactive activities for improving information integration, a user feedback processing scheme is proposed to refine the association between situation reports and images as well as the affinity among images. The system is developed on Apple's mobile operating system (iOS) and runs on iPad tablets, and its usefulness is evaluated by domain experts from the local EM department. 2012 ICST.",,
Navlakha,"Zheng L., Shen C., Tang L., Zeng C., Li T., Luis S., Chen S.-C., Navlakha J.K.","With the rise of heterogeneous information delivering platform, the process of collecting, integrating, and analyzing disaster related information from diverse channels becomes more difficult and challenging. Further, information from multiple sources brings up new challenges for information presentation. In this paper, we design and implement a Disaster Situation Reporting System (Disaster SitRep) that is essentially a disaster information collecting, integration, and presentation platform to address three critical tasks that can facilitate information acquisition, integration and presentation by utilizing domain knowledge as well as public and private web resources for major disaster recovery planning and management. Our proposed techniques create a disaster domain-specific search engine and a geographical information presentation and navigation platform using advanced data mining and information retrieval techniques for disaster preparedness and recovery that helps impacted communities better understand the current disaster situation. Specifically, hierarchical clustering with constraints are used to automatically update existing disaster concept hierarchy; taxonomy-based focused crawling component is developed to automatically detect, parse and filter those relevant web resources; a domain-oriented skeleton for each type of disasters is used to extract disaster events from disaster documents by defining the set of structural attributes. Furthermore, the platform can perform not only as a domain-specific search engine but also as an information monitoring and analysis tool for decision support during recovery phase of disasters. 2012 IEEE.",,
Navlakha,"Morris Kenneth, Navlakha Jainendra","Several weeks before each month begins, an airline pilot is issued a bid package describing the coming month's schedule. The pilot must turn in a bid on the schedule. Schedulers then assign a 'line of time' based on the category seniority number of the pilot. (A category is define by the base, aircraft type and seat, i.e. captain, copilot, or flight engineer.) The most senior pilot in a category gets his/her first choice and need bid only one line. The second most senior pilot will get his first choices if it is different from the #1 pilot, or his second choice if their first choices are the same. And so it goes on down the seniority list. It is obvious that a pilot must bid at least as many lines of times as his category seniority to insure he is not assigned an undesired line. Many factors can enter into bidding the lines of time, and especially for pilots near the bottom of the category seniority, a lot of time is involved. Each pilot weighs the importance of each factor differently, and priorities of each pilot often change from month to month. This paper describes an expert system to help make the task easier and more exact. The system is programmed in OPS5 and runs on a VAX 8800 machine.",,
Navlakha,Navlakha J.K.,"In the last decade or so, many software cost estimation models have been developed. These differ substantially from each other, particularly with respect to the inputs required and their outputs. For a software manager, the problem of selecting a particular model, or a combination of models that can be applied to an individual organization, is not all trivial. This paper describes our efforts to solve this problem for two organizations who had collected data on past development efforts. Statistical correlations between the actual and the estimated efforts calculated by using different cost estimation models were obtained. Regression analysis revealed that the cost model that is used by the organizations is not ideal for their environment. Statistical tests show that the results obtained are indeed statistically significant. The methodology used to perform the case study is applied to predict the development effort for a project, and the result is quite impressive. 1990.",,
Navlakha,"Weiss M.A., Navlakha J.K.","We show that the kth smallest element in a large heap is at expected depth ?log k. Simulation results indicate that this bound is tight, and that the variance of the depth is no more than 0.8, independent of k. This leads to a simple algorithm for actually finding the kth smallest element that appears to run in O(k) expected time, which would improve the previous best-known bound of O(klog k). We prove an ?(klog k) lower bound for worst case running time of any algorithm to solve this problem. Springer-Verlag Berlin Heidelberg 1989.",,
Navlakha,"Feild Jr. William B., Navlakha Jainendra K.","There are two parts to this research. Part 1 is simulation of the neural network on a parallel computer. We discuss some of the design decisions as well as providing code for salient aspects of the model. A pattern recognition example was used as a test case for this simulation. Part 2 is choosing input examplers to achieve network stability. We define various parameters based on the topology of the non-random patterns and based on the experiments performed, suggest ways to choose them to achieve stability of the network.",,
Navlakha,Navlakha Jainendra K.,Software engineers have asserted the necessity of a standard for software productivity metrics. The author proposes a scheme for the measurement of software productivity which satisfies varied requirements of any such measure as well as being flexible enough to accommodate the demands and desires of different types of people involved in a software project. Theoretical development of the proposed scheme is included to give a flavor of productivity calculations.,,
Navlakha,Navlakha J.K.,"Measures of the structured design of software systems are called system complexity metrics. Two particularly promising system complexity metrics are described in this paper: B. H. Yin and J. W. Winchester's metric, which is derived from a system's structured design charts; and S. Henry and D. Kafura's metric, which is derived from a system's information flow. The values computed by both are available after the end of the design phase. Consequently, they are useful in the entire software development life cycle, from the design phase on. The definition, utility, interpretation and advantages of each metric are described. Validation studies and their results are also reported for each metric. It is noted that Yin and Winchester's metric is quite successfully used at Hughes Aircraft Company but that there is no published report of the use of an information flow metric by any software organization.",,
Navlakha,Navlakha Jainendra K.,We have analyzed a commercially developed large scale software system to study the effect of external and internal interfaces on the actual development effort and the number of errors detected in the maintenance phase. Statistical analysis shows that error count is best predicted by the complexity of internal interfaces while the actual effort is very closely related to the external interfaces.,,
Navlakha,Navlakha Jainendra K.,An outline is presented of important aspects of software productivity and its measurement: why the measurement of sofware productivity is important; different viewpoints of software productivity; desirable properties of software productivity metrics; advantages of measuring software productivity; V. Basili's (1984) goal 'question' metric paradigm; and evaluation of some productivity metrics.,,
Navlakha,Navlakha Jainendra K.,"Software productivity is one of the most important attributes of the software development process. Our current knowledge of software productivity is not sufficient to define it properly, let alone measure it accurately. Indeed, for lack of something better, the industry has used some version or other of lines of code per man-month as the measure of productivity. To the author's mind this measures programmer productivity, not software productivity. In this paper, the author discussed several possible definitions of software productivity from different viewpoints. He also discussed an important issue on the subject, the management of software productivity. (Edited author abstract. )",,
Navlakha,Navlakha J.K.,"Given the values of a 3-dimensional function at unevenly spaced grid points on the grid structure of a channel, we describe a new analytical algorithm to determine the function value at an arbitrary point inside the channel. This algorithm has been implemented on a Univac 1100/81 computer in PL/I. 1984 BIT Foundations.",,
Navlakha,Navlakha Jainendra K.,The selection problem for a very large heap is defined as the problem of finding the k**t**h largest element in a heap whose depth is greater than k. We present an efficient algorithm to solve this problem in O(k log log n) time. The algorithm uses a binary search tree that is also an AVL tree. A couple of open questions linked with this problem are posed.,,
Navlakha,"Ernst G.W., Navlakha J.K., Ogden W.F.","A verification system is developed for proving the correctness of programs containing procedures with procedure-type parameters. The system, which reduces programs and their specifications to assertions to be proved in ordinary logic, is shown to be logically sound. The reduction process is controlled by the syntax of the program and is completely mechanical, requiring no human intervention. The resulting assertions involve higher-order predicates, but they engender no significant difficulties which are not already present in ordinary first-order theories. Our system views the intermediate objects in the reduction process as extended programs, thereby making verification a much less abstruse process. Treating logical assertions as commands appeals strongly to a programmer's intuition. 1982 Springer-Verlag.",,
Navlakha,Navlakha J.,"To generate an equivalent ?-free context free grammar from an arbitrary CFG, the most efficient algorithms described in the literature increase the size of the grammar by a factor, polynomial in terms of the number of nonterminals maximally occuring on the right hand side of a production. In this paper, we present an algorithm to generate a ?-free CFG whose total space requirement (or its size) is limited to seven times the initial size. The correctness of our algorithm is established by using a new proof technique based on the structure of the derivation trees and using a counting argument to establish that if a terminal string can be derived in one grammar, it can also be derived in the other. 1982 BIT Foundations.",,
Navlakha,Navlakha J.K.,"Computer centers and particularly data processing centers deal with a stack load of information files as a daily routine. The information in many of these files is organized according to a particular format. For example, in an ""employee file,"" the information about an employee might consist of the name of that employee, his or her age, sex, social security number, salary, etc., in some order. 1980 by AFIPS Press.",,
Navlakha,Navlakha J.K.,"Regular Expression Compiler (REC) is a pro-gramming language of extremely simple struc-ture. It is a gûto-less language which has very appealing transfer of control opera-tions. It is a generalized language which can be implemented for special purpose applications by a proper choice of operators and predicates. This paper describes BEC language and one of its ""symbol manipulation"" implementations, REC/SM. The language has been enormously successful from the users satisfaction point of view. And like BLISS, it has once again proved that the well-publicized inconvenience of programming without a goto is a myth. 1980 Proceedings of the 18th Annual Southeast Regional Conference, ACM-SE 1980. All rights reserved.",,
Navlakha,Navlakha Jainendra K.,"A software verifier system to check the correctness of information in a file is described. The (valid-format) and the file record to be verified are input to the system, the output is the correctness result for every (individual-format) of the (valid-format), based on whether the file information was conformal with respect to it, or not. The system, being a rather general one, saves a tremendous amount of programmer time in the sense that one program is good for verification of information in all types of files and separate programs are not required. The program is written in REC/Markov language and is implemented on a UNIVAC 1106 computer.",,
Navlakha,Navlakha J.K.,An algorithm for generating symmetric solutions of the modified no-three-in-line problem for the square boards of even size is presented. The correctness of the algorithm for arbitrarily large square boards is established. A computer program in PL/I was written and executed on the UNIVAC 1108 computer to generate the solutions for some boards of even size. 1979 ACM.,,
Pan,"Ma W., Sandoval O., Beltran J., Pan D., Pissinou N.","Network function virtualization enables flexible implementation of network functions, or middleboxes, as virtual machines running on standard servers. However, the flexibility also creates a challenge for efficiently placing such middleboxes, due to the availability of multiple hosting servers, capability of middleboxes to change traffic volumes, and dependency between middleboxes. In this paper, we address the optimal placement challenge of NFV middleboxes, and propose solutions for middleboxes of different traffic changing effects and with different dependency relations. First, we formulate the Traffic Aware Placement of Interdependent Middleboxes problem as a graph optimization problem. When the flow path is predetermined, we design optimal algorithms to place a non-ordered or totally-ordered middlebox set, and propose an efficient heuristic for the general scenario of a partially-ordered middlebox set after proving its NP-hardness. When the flow path is not predetermined, we show that the problem is NP-hard even for a non-ordered middlebox set, and propose a traffic and space aware routing heuristic. We have evaluated the proposed algorithms using large scale simulations and prototype experiments, and present extensive evaluation results to demonstrate the effectiveness of our design. 2017 IEEE.",,
Pan,"Ma W., Beltran J., Pan Z., Pan D., Pissinou N.","Network function virtualization (NFV) enables flexible deployment of middleboxes as virtual machines running on general hardware. Since different middleboxes may change the volume of processed traffic in different ways, improper deployment of NFV middleboxes will result in hot spots and congestion. In this paper, we study the traffic changing effects of middleboxes, and propose software-defined networking based middlebox placement solutions to achieve optimal load balancing. We formulate the traffic aware middlebox placement (TAMP) problem as a graph optimization problem with the objective to minimize the maximum link load ratio. First, we solve the TAMP problem when the flow paths are predetermined, such as the case in a tree. For a single flow, we propose the least-first-greatest-last (LFGL) rule and prove its optimality; for multiple flows, we first show the NP-hardness of the problem, and then propose an efficient heuristic. Next, for the general TAMP problem without predetermined flow paths, we prove that it is NP-hard even for a single flow, and propose the LFGL based MinMax routing algorithm by integrating LFGL with MinMax routing. We use a joint emulation and simulation approach to evaluate the proposed solutions, and present extensive experimental and simulation results to demonstrate the effectiveness of our design. 2004-2012 IEEE.",,
Pan,"Jo E., Pan D., Liu J., Butler L.","The fat tree topology with multipath capability has been used in many recent data center networks (DCNs) for increased bandwidth and fault tolerance. Traditional routing protocols have only limited support for multipath routing, and cannot fully utilize the available bandwidth in such networks. In this paper, we study multipath routing for fat tree networks. We formulate the problem as a linear program and prove its NP-completeness. We propose a practical solution, which takes advantage of the emerging software-defined networking paradigm. Our algorithm relies on a central controller to collect necessary network state information in order to make optimized routing decisions. We implemented the algorithm as an OpenFlow controller module and validated it with Mininet emulation. We also developed a fluid-based DCN simulator and conducted experiments, which show that our algorithm outperforms the traditional multipath algorithm based on random assignments, both in terms of increased throughput and in reduced end-to-end delay. 2014 IEEE.",,
Pan,"Vawter I., Pan D., Ma W.","Modern networks require robust traffic handling policies in order to minimize unwanted traffic and maximize performance. These policies are enforced by the placement of rules on network devices such as routers and switches. For high-speed processing, the rules are stored in Ternary Content Addressable Memory (TCAM) [3]. Because it is expensive, there is only a limited amount of TCAM on each network device. Software Defined Networking (SDN) [2] and the OpenFlow [1] protocol provide the capability to control the way rules are generated and placed within a network. This allows researchers to design algorithms that control TCAM usage while optimizing performance. The goal of this project is to develop a test bed for researchers to easily implement and evaluate their performance-optimizing algorithms. In this paper, we describe a test bed that emulates SDN activity and captures network performance data for rule placement algorithm evaluation. 2014 IEEE.",,
Pan,"Ma W., Medina C., Pan D.","Network Function Virtualization (NFV) enables flex- ible deployment of middleboxes as Virtual Machines (VMs) running on general hardware. Different types of middleboxes have the potential to either increase or decrease the volume of processed traffic. In this paper, we investigate the traffic changing effects of middleboxes, and study efficient deployment of NFV middleboxes in Software-Defined Networks (SDNs). To begin with, we formulate the Traffic-Aware Middlebox Placement (TAMP) problem as a graph optimization problem, and show that it is NP-hard when there are multiple flows to consider. Next, by observing that in reality flows arrive one at a time, we leverage the SDN central control mechanism, and propose an optimal solution for the TAMP problem with a single flow. We develop the solution in two steps. First, when the flow path has been determined, we present the Least-First-Greatest- Last (LFGL) rule to place middleboxes. Second, we integrate the LFGL rule with widest-path routing to propose the LFGL based MinMax routing algorithm. Further, we have implemented the proposed algorithm as a module running on top of the Floodlight SDN controller, and conducted experiments in the Mininet emulation system. The experiment results fully demonstrate the superiority of our algorithm over other benchmark solutions. 2015 IEEE.",,
Pan,"Fatmi O., Pan D.","Modern data center networks often adopt multipath topologies for greater bisection bandwidth and better fault tolerance. However, traditional distributed routing algorithms make routing decisions based on only packet destinations, and cannot readily utilize the multipath feature. In this paper, we study distributed multipath routing for data center networks. First, to capture the time varying and non-deterministic nature of data center network traffic, we present a stochastic traffic model based on the log normal distribution. Then, we formulate the stochastic load-balanced multipath routing problem, and prove that it is NP hard for typical data center network topologies, including the fat tree, VL2, DCell, and BCube. Next, we propose our distributed multipath routing algorithm, which balances traffic among multiple links by minimizing the probability of each link to face congestion. Finally, we implement the proposed algorithm in the NS2 simulator, and provide simulation results to demonstrate the effectiveness of our design. 2014 IEEE.",,
Pan,"Jin H., Cheocherngngarn T., Levy D., Smith A., Pan D., Liu J., Pissinou N.","Data centers consume significant amounts of energy. As severs become more energy efficient with various energy saving techniques, the data center network (DCN) has been accounting for 20% or more of the energy consumed by the entire data center. While DCNs are typically provisioned with full bisection bandwidth, DCN traffic demonstrates fluctuating patterns. The objective of this work is to improve the energy efficiency of DCNs during off-peak traffic time by powering off idle devices. Although there exist a number of energy optimization solutions for DCNs, they consider only either the hosts or network, but not both. In this paper, we propose a joint optimization scheme that simultaneously optimizes virtual machine (VM) placement and network flow routing to maximize energy savings, and we also build an Open Flow based prototype to experimentally demonstrate the effectiveness of our design. First, we formulate the joint optimization problem as an integer linear program, but it is not a practical solution due to high complexity. To practically and effectively combine host and network based optimization, we present a unified representation method that converts the VM placement problem to a routing problem. In addition, to accelerate processing the large number of servers and an even larger number of VMs, we describe a parallelization approach that divides the DCN into clusters for parallel processing. Further, to quickly find efficient paths for flows, we propose a fast topology oriented multipath routing algorithm that uses depth-first search to quickly traverse between hierarchical switch layers and uses the best-fit criterion to maximize flow consolidation. Finally, we have conducted extensive simulations and experiments to compare our design with existing ones. The simulation and experiment results fully demonstrate that our design outperforms existing host-or network-only optimization solutions, and well approximates the ideal linear program. 2013 IEEE.",,
Pan,"Jin H., Pan D., Liu J., Pissinou N.","Flow-level bandwidth provisioning (FBP) achieves fine-grained bandwidth assurance for individual flows. It is especially important for virtualization-based computing environments such as data centers. However, existing flow-level bandwidth provisioning solutions suffer from a number of drawbacks, including high implementation complexity, poor performance guarantees, and inefficiency to process variable length packets. In this paper, we study flow-level bandwidth provisioning for Combined Input Crosspoint Queued (CICQ) switches in the OpenFlow context. First, we propose the Flow-level Bandwidth Provisioning algorithm for CICQ switches, which reduces the switch scheduling problem to multiple instances of fair queuing problems, each utilizing a well-studied fair queuing algorithm. We theoretically prove that FBP can closely emulate the ideal Generalized Processing Sharing model, and accurately guarantee the provisioned bandwidth. Furthermore, we implement FBP in the OpenFlow software switch to obtain realistic performance data by a prototype. Leveraging the capability of OpenFlow to define and manipulate flows, we experimentally demonstrate a practical flow-level bandwidth provisioning solution. Finally, we conduct extensive simulations and experiments to evaluate the design. The simulation data verify the correctness of the analytical results, and show that FBP achieves tight performance guarantees. The experiment results demonstrate that our OpenFlow-based prototype can conveniently and accurately provision bandwidth at the flow level. 1968-2012 IEEE.",,
Pan,"Pumpichet S., Pissinou N., Jin X., Pan D.","The imprecision in data streams received at the base station is common in mobile wireless sensor networks. The movement of sensors leads to dynamic spatio-temporal relationships among sensors and invalidates the data cleaning techniques designed for stationary networks. As one of the first methods designed for mobile environments, we introduce a novel online method to clean the imprecise or dirty data in mobile wireless sensor networks. Our method deploys a belief parameter to select the helpful neighboring sensors to clean data. The belief parameter is based on sensor trajectories and the consistency of their streaming data correctly received at the base station. The evaluation over multiple mobility models shows that the proposed method outperforms the existing data cleaning algorithms, especially in sparse environments where the node density in the system is low. 2012 IEEE.",,
Pan,"Zhang S., Bhattacharya A., Pan D., Yang Z.","P2P is successful in various multimedia applications such as On-demand/live streaming due to the efficient upload bandwidth usage among participating peers which offloads server request thereby saving bandwidth as system size scales up. Many designs were proposed for P2P multimedia streaming systems, including the most promising tree/mesh overlays. In this paper, we propose MERIT as an integrated framework for scalable mesh-based P2P multi-streaming whose design objective is to preserve content diversity as well as optimizing start-up delay while satisfying the in-/out- bound bandwidth constraints. We formulate our design goals as an optimization problem and start with a centralized heuristic exploiting the global knowledge of peers. We then present a decentralized version of our algorithm which is scalable and follows similar design principles as the centralized one. Simulation results indicate that our heuristics outperform state-of-the-art approaches by improving streaming quality and start-up delay with efficient utilization of bandwidth resources at each peer.",,
Pan,"Jin H., Pan D., Xu J., Pissinou N.","Virtual machines (VMs) may significantly improve the efficiency of data center infrastructure by sharing resources of physical servers. This benefit relies on an efficient VM placement scheme to minimize the number of required servers. Existing VM placement algorithms usually assume that VMs' demands for resources are deterministic and stable. However, for certain resources, such as network bandwidth, VMs' demands are bursty and time varying, and demonstrate stochastic nature. In this paper, we study efficient VM placement in data centers with multiple deterministic and stochastic resources. First, we formulate the Multidimensional Stochastic VM Placement (MSVP) problem, with the objective to minimize the number of required servers and at the same time satisfy a predefined resource availability guarantee. Then, we show that the problem is NP-hard, and propose a polynomial time algorithm called Max-Min Multidimensional Stochastic Bin Packing (M3SBP). The basic idea is to maximize the minimum utilization ratio of all the resources of a server, while satisfying the demands of VMs for both deterministic and stochastic resources. Next, we conduct simulations to evaluate the performance of M3SBP. The results demonstrate that M3SBP guarantees the availability requirement for stochastic resources, and M3SBP needs the smallest number of servers to provide the guarantee among the benchmark algorithms. 2012 IEEE.",,
Pan,"Cheocherngngarn T., Jin H., Andrian J., Pan D., Liu J.","Modern data center networks (DCNs) often use multi-rooted topologies, which offer multipath capability, for increased bandwidth and fault tolerance. However, traditional routing algorithms for the Internet have no or limited support for multipath routing, and cannot fully utilize available bandwidth in such DCNs. In this paper, we study the multipath routing problem for DCNs. We first formulate the problem as an integer linear program, but it is not suitable for fast on-the-fly route calculation. For a practical solution, we propose the Depth-First Worst-Fit Search based multipath routing algorithm. The main idea is to use depth-first search to find a sequence of worst-fit links to connect the source and destination of a flow. Since DCN topologies are usually hierarchical, our algorithm uses depth-first search to quickly traverse between hierarchical layers to find a path. When there are multiple links to a neighboring layer, the worst-fit link selection criterion enables the algorithm to make the selection decision with constant time complexity by leveraging the max-heap data structure, and use a small number of selections to find all the links of a path. Further, worst-fit also achieves load balancing, and thus generates low queueing delay, which is a major component of the end-to-end delay. We have evaluated the proposed algorithm by extensive simulations, and compared its average number of link selections and average end-to-end delay with competing solutions. The simulation results fully demonstrate the superiority of our algorithm and validate the effectiveness of our designs. 2012 IEEE.",,
Pan,"Jin X., Pissinou N., Chesneau C., Pumpichet S., Pan D.","The rapid development in micro-computing has allowed implementations of complex mobile Wireless Sensor Networks (mWSNs). Privacy invasion is becoming an indispensable issue along with the increasing range of applications of mWSNs. Private trajectory information not only indicates the movements of mobile sensors, but also reveals personal preferences and habits of users. In this paper, we propose the distributed Basic Trajectory Privacy (BTPriv) and Secondary Trajectory Privacy (STPriv) preservation algorithms to hide trajectory of data source nodes online. We set up various simulation environments for different applications. The effectiveness of our proposed algorithms is evaluated by the software implementation in simulation experiments. 2012 IEEE.",,
Pan,"Pan D., Yang Y.","Buffered crossbar switches are a special type of crossbar switches with a small buffer at each crosspoint of the crossbar. Existing research results indicate that they can provide port based performance guarantees with speedup of two, but require significant hardware complexity to provide flow based performance guarantees. In this paper, we present scheduling algorithms for buffered crossbar switches to achieve flow based performance guarantees with speedup of two and one buffer per crosspoint. When there is no crosspoint blocking, only simple and distributed input scheduling and output scheduling are needed. Otherwise, a special urgent matching procedure is necessary to guarantee on-time delivery of crosspoint blocked cells. For urgent matching, we present both sequential and parallel matching algorithms. The parallel version significantly reduces the average number of iterations for convergence, which is verified by simulation. With the proposed algorithms, buffered crossbar switches can provide flow based performance guarantees by emulating push-in-first-out output-queued switches, and we use the counting method to prove the perfect emulation. Finally, we discuss an alternative backup-buffer implementation design to the bypass path, and compare our scheme with existing solutions. 2012 IEEE.",,
Pan,"Jin H., Pan D., Pissinou N.","The ever increasing demand for more bandwidth at core routers has been a challenge for switch design. To address the challenge, parallel packet switches (PPSs) combine multiple parallel switching fabrics and provide huge aggregate bandwidth. However, most existing PPSs handle only fixed length packets, also called cells, mainly because traditional switching fabrics can process only cells. Since packets in the Internet are of variable length, existing PPSs need segmentation-and-reassembly (SAR) to process such packets, which will introduce padding bits and waste precious bandwidth. In this paper, we propose a PPS to directly handle variable-length packets without SAR. First, we present a simplified 1 x 1 variable-length PPS. We design the packet distribution and collection algorithms, and show that input and output conversion buffers are bounded by 2L, where L is the maximum packet length. Next, we present a general N x N variable-length PPS, and propose the packet scheduling algorithms. We then prove our main result that such a PPS can emulate a first-in-first-out (FIFO) output queued (OQ) switch with speedup of two, i.e. emulating an FIFO OQ switch with bandwidth R by 2K-1 parallel switching fabrics each with bandwidth r, where r=R/K. 2011 IEEE.",,
Pan,"Bhattacharya A., Yang Z., Pan D.","Application-layer overlay networks are receiving considerable popularity due to its flexibility and readily deployable nature thereby providing support for a plethora of Peer-to-Peer (P2P) applications. Currently, the real-world deployment of Internet-scale P2P media streaming systems involve the usage of tracker server for content discovery in on-demand model with asynchronous interactivity. The inherent drawbacks of tracker-server based approach are obvious due to scalability and bottleneck issues, which prompted us to pursue a structured P2P based proposition such as Distributed Hash Tables (DHT) which are already proved to be stable substrates. The challenging issue of accommodating a large number of update operations with the continuous change of user's playing position in DHT-based overlay is addressed in our previous work by the concept of Temporal-DHT which exploits the temporal dynamics of the content to estimate playing position. In this paper, we incorporate the notion of popularity awareness in the Temporal-DHT framework which will help to adapt the query resolution mechanism by addressing the skew ness of content popularity typically found in real multimedia user access patterns. The essential objective of popularity awareness mechanism is to increase the overall performance of Temporal-DHT by optimizing the search cost among the entire content set within the system. We formulate the problem and provide practical solutions with extensive simulation results that demonstrates the effectiveness of popularity-aware Temporal-DHT by achieving optimized query resolution cost and high streaming quality for on-demand systems in a dynamic network environment where user's are free to asynchronously join/leave the system. 2011 IEEE.",,
Pan,"Cheocherngngarn T., Andrian J., Yang Z., Pan D.","A group of switch schedulers make packet scheduling decisions based on predefined bandwidth allocation for each flow. Allocating bandwidth for best effort flows is challenging due to lack of allocation criteria and fairness principles. In this paper, we propose sequential and parallel algorithms to allocate bandwidth for best effort flows in a switch, to achieve fairness and efficiency. The proposed algorithms use the queue length proportional allocation criterion, which allocates bandwidth to a best effort flow proportional to its queue length, giving more bandwidth to congested flows. In addition, the algorithms adopt the max-min fairness principle, which maximizes bandwidth utilization and maintains fairness among flows. We first formulate the problem based on the allocation criterion and fairness principle. Then, we present a sequential algorithm and prove that it achieves max-min fairness. To accelerate the allocation process, we propose a parallel version of the algorithm, which allows different input ports and output ports to conduct calculation in parallel, resulting in fast convergence. Finally, we present simulation data to demonstrate that the parallel algorithm is effective in reducing the convergence iterations. 2011 IEEE.",,
Pan,"Jin H., Pan D., Liu J., Pissinou N.","Flow level bandwidth provisioning offers fine granularity bandwidth assurance for individual flows. It is especially important for virtual network based experiment environments, to isolate traffic of different experiments or different types, which may be fed to the same switch or router port. Existing flow level bandwidth provisioning solutions suffer from a number of drawbacks, including high implementation complexity, poor performance guarantees, and inefficiency to process variable length packets. In this paper, we study flow level bandwidth provisioning for combined-input-crosspoint-queued switches in the OpenFlow context. We propose the FEBR (Flow lEvel Bandwidth pRovisioning) algorithm, which reduces the switch scheduling problem to multiple instances of fair queueing problems, each employing a well studied fair queueing algorithm. FEBR can tightly emulate the ideal Generalized Processing Sharing model, and accurately guarantee the provisioned bandwidth. Further, we implement FEBR in the OpenFlow version 1.0 software switch. In conjunction with the capability of OpenFlow to flexibly define and manipulate flows, we thus provide a practical flow level bandwidth provisioning solution. Finally, we present extensive simulation and experiment data to validate the analytical results and evaluate our design. 2011 IEEE.",,
Pan,"Bhattacharya A., Yang Z., Pan D.","Various distributed multi-source applications such as 3D Virtual Immersive Systems (3DVIS), 2D/3D Videoconferencing, Multi-party games, etc. require the construction of a multicast overlay through which the video blocks are streamed from source to receiver. The most general multicast model involves a single source and a set of receiver nodes also known as multicast set with a single stream to each receiver. In this paper, we are interested to investigate a different multicast model consisting of multiple sources and receivers which is quite common in 3DVIS/video conferencing applications where each node serves both as a source and a receiver. The problem of constructing a multicast overlay for multi-source is hard and we are interested to guarantee certain QoS constraints such as end-to-end latency and latency variations. Latency variation constraints are very important in collaborative applications for maintaining real-time interactivity and multi-stream synchronization. We present an efficient heuristic solution for the muti-source multicast construction problem with an economical time-complexity. ©2010 IEEE.",,
Pan,"Karimi M., Sun Z., Pan D., Yang Z.","Buffered crossbar switches are special crossbar switches with an exclusive buffer at each crosspoint. They demonstrate unique advantages over traditional unbuffered crossbar switches, such as asynchronous scheduling and variable length packet handling. However, since crosspoint buffers are expensive on-chip memories, it is desired that each crosspoint has only a small buffer. In this paper, we propose a scheduling algorithm called Fair Asynchronous Segment Scheduling (FASS) for buffered crossbar switches, which reduces the crosspoint buffer size by dividing packets into shorter segments before transmission. FASS also provides tight constant performance guarantees by emulating the ideal Generalized Processor Sharing (GPS) model. Furthermore, FASS requires no speedup for the crossbar, lowering the hardware cost and improving the switch capacity. By theoretical analysis, we prove that FASS is strongly stable and therefore achieves 100% throughput. We also calculate the size bound for the crosspoint buffers. Moreover, we show that FASS provides bounded delay guarantees. Finally, we present simulation data to verify the analytical results. ©2010 IEEE.",,
Pan,"Karimi M., Sun Z., Pan D.","Generalized Processor Sharing (GPS) is a powerful fluid model and there are practical scheduling algorithms that can perfectly emulate it. GPS has been widely used as the reference model to schedule guaranteed performance traffic. However, there has not been a way to apply GPS to best effort traffic. In this paper, we propose a bandwidth allocation scheme called Queue Length Proportional (QLP) for crossbar switches without speedup, so as to use GPS to schedule best effort traffic. QLP dynamically obtains a feasible bandwidth matrix as the GPS scheduling criteria. In QLP, the amount of service that each flow receives is proportional to the length of its backlogged queue. We analytically prove that QLP is strongly stable and hence provides 100% throughput for any admissible traffic, no matter whether the traffic distribution is uniform or nonuniform. Moreover, we show that QLP is feasible, which means the allocated bandwidth does not exceed the available capacity. Finally, we perform simulations to verify the theoretical results and to measure the performance of QLP. ©2010 IEEE.",,
Pan,"Jin H., Pan D., Pissinou N., Makki K.","Performance guarantees provided by switches can be at different granularity: port level and flow level. As a trade-off, it is usually more expensive to provide performance guarantees at finer granularity. Existing solutions for switches to provide flow level performance guarantees require either expensive hardware support or centralized scheduling algorithms with multiple iterations. In this paper, we present the Flow-level Fair Scheduling (FFS) algorithm to provide flow level performance guarantees for Combined-Input-Crosspoint-Queued (CICQ) switches, which are special crossbar switches with a small exclusive buffer at each crosspoint of the crossbar. FFS uses hierarchical and multidimensional fair queueing to emulate the ideal Generalized Processing Sharing (GPS) model. The main features of FFS include: constant performance guarantees, bounded crosspoint buffer sizes, no speedup requirement, and distributed operation. We theoretically analyze the performance of FFS, and conduct simulations to verify the analytical results. ©2010 IEEE.",,
Pan,"Jin X., Putthapipat P., Pan D., Pissinou N., Makki S.K.","Recent advancements in micro-computing have provided an exponential increase in the capabilities of a wide range of devices and have allowed the implementation of complex mobile wireless sensor networks (mWSNs). Common battery-powered sensor nodes require security techniques that eliminate redundant processing overhead for resource conservation, without compromising the overall network performance. To address this issue, this paper presents USAS: Unpredictable Software-based Attestation Solution, a node compromise detection algorithm in mWSNs. USAS deploys dynamic node attestation chains to decrease checksum computation time by almost 48% for selective attested nodes. By decentralizing the network, the attestation is unpredictable, preventing malicious data injection. The performance of USAS is estimated in terms of node compromise detection rate. ©2010 IEEE.",,
Pan,"Sun Z., Karimi M., Pan D., Yang Z., Pissinou N.","A parallel packet switch (PPS) provides huge aggregate bandwidth by combining the capacities of multiple switching fabrics. Most existing PPSs use output queued switches as the switching fabrics, which require speedup and result in high implementation cost. In this paper, we present a buffered crossbar based parallel packet switch (BCB-PPS), whose switching fabrics need no speedup. We propose the Batch-WF2Q algorithm to dispatch packets to the parallel switching fabrics, and leverage the sMUX algorithm in [7] to schedule packet transmission for the switching fabrics. Such a design enables a simple round-robin algorithm to efficiently collect packets from the switching fabrics. In addition, our design requires no packet resequencing, and thus needs no buffers at either external or internal outputs. We show that BCB-PPS has tight delay guarantees and bounded buffer sizes. Finally, we present simulation data to verify the analytical results and to evaluate the performance of our design. ©2010 IEEE.",,
Pan,"Pan D., Makki K., Pissinou N.","Buffered crossbar switches are special crossbar switches with a small exclusive buffer at each crosspoint of the crossbar. They demonstrate unique advantages, such as variable length packet handling and distributed scheduling, over traditional unbuffered crossbar switches. The current main approach for buffered crossbar switches to provide performance guarantees is to emulate push-in-first-out output queued switches. However, such an approach has several drawbacks, and in particular it has difficulty in providing tight constant performance guarantees. To address the issue, we propose in this paper the guaranteed-performance asynchronous packet scheduling (GAPS) algorithm for buffered crossbar switches. GAPS intends to provide tight performance guarantees, and requires no speedup. It directly handles variable length packets without segmentation and reassembly, and makes scheduling decisions in a distributed manner. We show by theoretical analysis that GAPS achieves constant performance guarantees. We also prove that GAPS has a bounded crosspoint buffer size of 3L, where L is the maximum packet length. Finally, we present simulation data to verify the analytical results and show the effectiveness of GAPS.",,
Pan,"Pan D., Yang Y.","Multicast enables efficient data transmission from one source to multiple destinations, and has been playing an important role in Internet multimedia applications. Although several multicast scheduling schemes for packet switches have been proposed in the literature, they usually aim to achieve only short multicast latency and high throughput without considering bandwidth guarantees. However, fair bandwidth allocation is critical for the quality of service (QoS) of the network, and is necessary to support multicast applications requiring guaranteed performance services, such as online audio and video streaming. This paper addresses the issue of bandwidth guaranteed multicast scheduling on virtual output queued (VOQ) switches. We propose the Credit based Multicast Fair scheduling (CMF) algorithm, which aims at achieving not only short multicast latency but also fair bandwidth allocation. CMF uses a credit based strategy to guarantee the reserved bandwidth of an input port on each output port of the switch. It keeps track of the difference between the reserved bandwidth and actually received bandwidth, and minimizes the difference to ensure fairness. Moreover, in order to fully utilize the multicast capability provided by the switch, CMF lets a multicast packet simultaneously send transmission requests to multiple output ports. In this way, a multicast packet has more chances to be delivered to multiple destination output ports in the same time slot and thus to achieve short multicast latency. Extensive simulations are conducted to evaluate the performance of CMF, and the results demonstrate that CMF achieves the two design goals: fair bandwidth allocation and short multicast latency. 2009 Elsevier Inc. All rights reserved.",,
Pan,"Bhattacharya A., Yang Z., Pan D.","Peer-to-Peer (P2P) approaches are gaining increasing popularity for video streaming applications due to their potential for Internet-level scalability. P2P VoD (Video On-Demand) applications pose more technical challenges than P2P live streaming since the peers are less synchronized over time as the playing position varies widely across the total video length along with the requirement to support VCR operations such as random seek, Fast-Forward and Backward (FF/FB). We propose COCONET in this paper, which uses a distributed cache partly contributed by each participant thereby achieving a random content distribution pattern independent of the playing position. It also achieves an ?(1) search efficiency for any random seek and FF/FB operation to any video segment across the total video stream with very low maintenance cost through any streaming overlay size. Performance evaluation by simulation indicates the effectiveness of COCONET to support our claim. Institute for Computer Science, Social-Informatics and Telecommunications Engineering 2009.",,
Pan,"Karimi M., Sun Z., Pan D., Chen Z.","Traditional crossbar switches use centralized scheduling algorithms with high time complexity. In contrast, buffered crossbar switches are capable of distributed scheduling due to crosspoint buffers, which decouple the dependency between inputs and outputs. However, crosspoint buffers are expensive on-chip memories. To reduce the hardware cost of buffered crossbar switches and make them scalable, we consider partially-buffered crossbar switches, whose crosspoint buffers can be of an arbitrarily small size and store only part of a packet instead of the entire packet. In this paper, we propose the Packet-mode Asynchronous Scheduling Algorithm (PASA) for partially buffered crossbar switches. PASA combines the features of both distributed and centralized scheduling algorithms. It works in an asynchronous mode and can directly handle variable length packets without Segmentation And Reassembly (SAR). We theoretically prove that, with a speedup of two, PASA achieves 100% throughput for any admissible traffic. We also show that outputs in PASA have a large probability to avoid the more time-consuming centralized scheduling process, and thus make fast scheduling decisions. Finally, we present simulation data to verify the analytical results and evaluate the performance of PASA.",,
Pan,"Pan D., Makki K., Pissinou N.","Recent development in VLSI technology makes it feasible to integrate on-chip memories to crossbar switching fabrics. Switches using such crossbars are called buffered crossbar switches, in which each crosspoint has a small exclusive buffer. The crosspoint buffers decouple input ports and output ports, and reduce the switch scheduling problem to the fair queueing problem. In this paper, we present the fair queueing based packet scheduling scheme for buffered crossbar switches, which requires no speedup and directly handles variable length packets without segmentation and reassembly (SAR). The presented scheme makes scheduling decisions in a distributed manner, and provides performance guarantees. Given the properties of the actual fair queueing algorithm used in the scheduling scheme, we calculate the crosspoint buffer size bound to avoid overflow, and analyze the fairness and delay guarantees provided by the scheduling scheme. In addition, we use WF2Q, the fair queueing algorithm with the tightest performance guarantees, as a case study, and present simulation data to verify the analytical results.",,
Pan,"Pan D., Yang Z., Makki K., Pissinou N.","Buffered crossbar switches are special crossbar switches with each crosspoint equipped with a small exclusive buffer. The crosspoint buffers decouple input ports and output ports, and simplify switch scheduling. In this paper, we propose a scheduling algorithm called Fair and Localized Asynchronous Packet Scheduling (FLAPS) for buffered crossbar switches, to provide tight performance guarantees. FLAPS needs no speedup for the crossbar and handles variable length packets without segmentation and reassembly (SAR). With FLAPS, each input port and output port independently make scheduling decisions and rely on only local queue statuses. We theoretically show that a crosspoint buffer size of 4L is sufficient for FLAPS to avoid buffer overflow, where L is the maximum packet length. In addition, we prove that FLAPS achieves strong stability, and provides bounded delay guarantees. Finally, we present simulation data to verify the analytical results. Institute for Computer Science, Social-Informatics and Telecommunications Engineering 2009.",,
Pan,"Karimi M., Pan D.","Next generation of wireless communication systems are engineered to service independent mobile users. These independent mobile users (nodes) are connected by wireless links build a Mobile Ad-hoc Network (MANET). The nodes in this system work together only based on mutual agreement without knowing about the network topology around themselves. Maintaining appropriate Quality of Service (QoS) for these networks, Mobile Ad-hoc Networks (MANETs) is a complex task because of the dynamic behavior of the network topology. Moreover, the size ofthe ad-hoc network is related to the QoS of the network. In addition, Mobile ad-hoc networks have a significant role in the future operation of wireless communication systems. Consequently, these networks should be able to provide the required QoS for the users. In this paper we discuss and analyze the dynamic nature of MANETs, and special attention is paid on fundamental problems and issues that occur when trying to provide QoS in this network environment. ©2009 IEEE.",,
Pan,"Labrador Y., Karimi M., Pan D., Miller J.","In this paper we focus our attention in the main two methods of Cooperative Communications: Decode and Forward, and Amplify and Forward, and how they can be used in a new concept of Cooperative Satellite Communications. We present an analysis of both in terms of Symbol Error Rate and Power Allocation and analyze which would be more efficient when relaying information from the satellite to a mobile node in the terrestrial network. We propose a protocol that combines Selective and Incremental Relaying to optimize the cooperative scheme. 2009 ACADEMY PUBLISHER.",,
Pan,"Pan D., Yang Y.","Fair scheduling and buffer management are two typical approaches to provide differentiated service. Fair scheduling algorithms usually need to keep a separate queue and maintain associated state variables for each incoming flow, which makes them difficult to operate and scale. On the contrary, buffer management (in conjunction with FIFO scheduling) needs only a constant amount of state information and processing, and can be efficiently implemented. In this paper, we consider using buffer management to provide lossless service for guaranteed performance flows in shared buffer switches. We study the buffer size requirement and buffer allocation strategies by starting with the single output switch and then extending the analytical results to the general multiple output switch. We present a universally applicable buffer allocation method for assuring lossless service, and validate the correctness of the theoretical results through simulations. 2009 Elsevier Inc. All rights reserved.",,
Pan,"Pan D., Yang Y.","Buffered crossbar switches are a special type of crossbar switches. In such a switch, besides normal input queues and output queues, a small buffer is associated with each crosspoint. Due to the introduction of crosspoint buffers, output and input contention is eliminated, and the scheduling process for buffered crossbar switches is greatly simplified. Moreover, since different input ports and output ports work independently, the switch can easily schedule and transmit variable length packets. Compared with fixed length packet scheduling, variable length packet scheduling has some unique advantages: higher throughput, shorter packet latency, and lower hardware cost. In this paper, we present a fast and practical scheduling scheme for buffered crossbar switches called Localized Independent Packet Scheduling (LIPS). With LIPS, an input port or output port makes scheduling decisions solely based on the state information of its local crosspoint buffers, i.e., the crosspoint buffers where the input port sends packets to or the output port retrieves packets from. The localization feature makes LIPS suitable for a distributed implementation and thus highly scalable. Since no comparison operation is required in LIPS, scheduling arbiters can be efficiently implemented using priority encoders, which can make arbitration decisions quickly in hardware. Another advantage of LIPS is that each crosspoint needs only L (the maximum packet length) buffer space, which minimizes the hardware cost of the switches. We theoretically analyze the performance of LIPS and, in particular, prove that LIPS achieves 100 percent throughput for any admissible traffic with speedup of two. We also discuss in detail the implementation architecture of LIPS and analyze the packet transmission timing in different scenarios. Finally, simulations are conducted to verify the analytical results and measure the performance of LIPS. 2006 IEEE.",,
Pan,"Pan D., Yang Y.","Buffered crossbar switches are a special type of combined input-output queued switches with each crosspoint of the crossbar having small on-chip buffers. The introduction of crosspoint buffers greatly simplifies the scheduling process of buffered crossbar switches, and furthermore enables buffered crossbar switches with speedup of two to easily provide port based performance guarantees. However, recent research results have indicated that, in order to provide flow based performance guarantees, buffered crossbar switches have to either increase the speedup of the crossbar to three or greatly increase the total number of crosspoint buffers, both adding significant hardware complexity. In this paper, we present scheduling algorithms for buffered crossbar switches to achieve flow based performance guarantees with speedup of two and with only one or two buffers at each crosspoint. When there is no crosspoint blocking in a specific time slot, only the simple and distributed input scheduling and output scheduling are necessary. Otherwise, the special urgentmatching is introduced to guarantee the on-time delivery of crosspoint blocked cells. With the proposed algorithms, buffered crossbar switches can provide flow based performance guarantees by emulating push-in-first-out output queued switches, and we use the counting method to formally prove the perfect emulation. For the special urgent matching, we present sequential and parallel matching algorithms. Both algorithms converge with N iterations in the worst case, and the latter needs less iterations in the average case. Finally, we discuss an alternative backup-buffer implementation scheme to the bypass path, and compare our algorithms with existing algorithms in the literature. ©2008 IEEE.",,
Pan,"Pan D., Yang Y.","With the rapid development of broadband applications, the capability of networks to provide quality of service (QoS) has become an important issue. Fair scheduling algorithms are a common approach for switches and routers to support QoS. All fair scheduling algorithms are running based on a bandwidth allocation scheme. The scheme should be feasible in order to be applied in practice, and should be efficient to fully utilize available bandwidth and allocate bandwidth in a fair manner. However, since a single input port or output port of a switch has only the bandwidth information of its local flows (i.e., the flows traversing itself), it is difficult to obtain a globally feasible and efficient bandwidth allocation scheme. In this paper, we show how to fairly allocate bandwidth in packet switches based on the max-min fairness principle. We first formulate the problem, and give the definitions of feasibility and max-min fairness for bandwidth allocation in packet switches. As the first step to solve the problem, we consider the simpler unicast scenarios, and present the max-min fair bandwidth allocation algorithm for unicast traffic. We then extend the analysis to the more general multicast scenarios, and present the max-min fair bandwidth allocation algorithm for multicast traffic. We prove that both algorithms achieve max-min fairness, and analyze their complexity. The proposed algorithms are universally applicable to any type of switches and scheduling algorithms. 2007 IEEE.",,
Pan,"Pan D., Yang Y.","Virtual output queued (VOQ) crossbar switches have been demonstrating advantages as high speed interconnects. They eliminate the Head of Line (HOL) blocking, which limits the maximum throughput of single input queued switches, and do not require switching fabrics with speedup capability, which prevents output queued switches from being cheaply implementable. Existing practical VOQ scheduling algorithms work in an iterative manner, and each iteration usually includes three steps: request, grant and accept. By incorporating arbitration into the request step, the accept step can be eliminated, and two step iterative matching can be achieved. While two step algorithms achieve almost identical performance as three step algorithms, they have extra advantages, such as simpler hardware implementation, shorter scheduling time, and less data exchange. As examples of two step iterative matching algorithms, we present Two Step Parallel Iterative Matching (PIM2) and Two Step iSLIP (iSLIP2), and theoretically analyze the convergence property of PIM2. Furthermore, because the request step and grant step perform similar operations, and the two steps always progress in a sequential manner, we propose a hardware efficient implementation for two step iterative matching algorithms which requires only one set of arbitration logic. We conduct extensive simulations, and the results demonstrate that our analytical result on the average convergence iterations, In N + e/(e -1), is more accurate than the classical result, log2 N + 4/3, and that two step algorithms and three step algorithms have almost identical performance. 2006 IEEE.",,
Pan,"Pan D., Yang Y.","Fair scheduling and buffer management are two typical approaches to providing differentiated service. Fair scheduling algorithms usually need to keep a separate queue and maintain associated state variables for each incoming flow, which make them difficult to operate and scale in high speed networks. On the contrary, buffer management and FIFO scheduling need only a constant amount of state information and processing, and can be efficiently implemented. In this paper, we consider using buffer management to provide lossless service for guaranteed performance flows in network processors. We investigate the buffer size requirement and buffer allocation strategies by starting with the single output network processor and then extending the analytical results to the general multiple output network processor. A universally applicable buffer allocation method for assuring lossless service is obtained, and the correctness of the theoretical results is verified through simulations. 2006 IEEE.",,
Pan,"Pan D., Yang Y.","Buffered crossbar switches are a special type of crossbar switches. In such a switch, besides normal input queues and output queues, a small buffer is associated with each crosspoint. Due to the introduction of crosspoint buffers, output and input contention is eliminated, and the scheduling process for buffered crossbar switches is greatly simplified. Moreover, crosspoint buffers enable the switch to work in an asynchronous mode and easily schedule and transmit variable length packets. Compared with fixed length packet scheduling or cell scheduling, variable length packet scheduling, or packet scheduling for short, has some unique advantages: higher throughput, shorter packet latency and lower hardware cost. In this paper, we present a fast and practical scheduling scheme for buffered crossbar switches called Localized Asynchronous Packet Scheduling (LAPS). With LAPS, an input port or output port makes scheduling decisions solely based on the state information of its local crosspoint buffers, i.e., the crosspoint buffers where the input port sends packets to or the output port retrieves packets from. The localization property makes LAPS suitable for a distributed implementation and thus highly scalable. Since no comparison operation is required in LAPS, scheduling arbiters can be efficiently implemented using priority encoders, which can make arbitration decisions quickly in hardware. Another advantage of LAPS is that each crosspoint needs only L (the maximum packet length) buffer space, which minimizes the hardware cost of the switches. We also theoretically analyze the performance of LAPS, and in particular we prove that LAPS achieves 100% throughput for any admissible traffic with speedup of two. Finally, simulations are conducted to verify the analytical results and measure the performance of LAPS. Copyright 2006 ACM.",,
Pan,"Pan D., Yang Y.","Traditional iterative matching algorithms for VOQ switches need three steps, i.e., request, grant and accept. By incorporating arbitration into the request step, two step iterative matching can be achieved. This enables simpler implementation and shorter scheduling time, while maintaining almost identical performance. As an example of the two step iterative matching algorithms, in this paper we present Two Step Parallel Iterative Matching (PIM2), and theoretically prove that its average convergence iterations are less than ln N + e/(e - 1) for an N _ N switch. Furthermore, two step iterative matching algorithms can be efficiently pipelined on CIOQ switches so that two matchings can be obtained in each time slot. We propose a scheme called Second of Line (SOL) matching to provide two independent virtual switches, with which the pipelining can be achieved without additional scheduling time and arbitration hardware. More importantly, the pipelined algorithms are theoretically guaranteed to achieve 100% throughput for any admissible traffic. Extensive simulations are conducted to show that our analytical result on the average convergence iterations lnN +e/(e-1) is more accurate than the classical result log2 N + 4/3, and to test the performance of different pipelined algorithms on CIOQ switches.",,
Pan,"Pan D., Yang Y.","Multicast enables efficient data transmission from one source to multiple destinations, and has been playing an important role in Internet multimedia applications. Although several multicast scheduling schemes for packet switches have been proposed, they usually consider only short delay and high throughput but not bandwidth guarantees. However, fair bandwidth allocation is critical for the quality of service (QoS) of the network, and is necessary to support multicast applications requiring guaranteed performance services, such as online audio and video streaming. This paper addresses the issue of bandwidth guaranteed multicast scheduling on virtual output queued (VOQ) switches. We propose the Credit based Multicast Fair scheduling (CMF) algorithm, which aims at achieving not only short multicast latency but also fair bandwidth allocation. CMF uses a credit/balance based strategy to guarantee the reserved bandwidth of an input port on each output port of the switch. It keeps track of the difference between the reserved bandwidth and actually received bandwidth, and minimizes the difference to ensure fairness. Moreover, CMF supports multicast scheduling by allowing a multicast packet to send transmission requests to multiple output ports simultaneously. As a result, a multicast packet has more chances to be delivered to all its destinations in the same time slot, and thus shortens its multicast latency. Extensive simulations are conducted to compare the performance of CMF with other existing scheduling algorithms, and the results demonstrate that CMF achieves the two design goals: short multicast latency and fair bandwidth allocation. 2005 IEEE.",,
Pan,"Pan D., Yang Y.","With the rapid development of Internet multimedia applications, the next generation of networks is required to schedule not only the best effort traffic but also the traffic with bandwidth and delay guarantees. Currently, there are two types of fair scheduling algorithms in the literature. The time stamp based schedulers achieve very good fairness and delay guarantees but have high O(log N) time complexity, where N is the number of flows. While the round robin based schedulers reach O(1) time complexity, their delay guarantees are O(N). This paper aims at a fair scheduling algorithm with constant time complexity as well as good fairness and delay guarantees. We first present a credit/balance based fair scheduling algorithm called Most Credit First (MCF). We theoretically prove that MCF can provide O(log N) fairness, delay and delay jitter guarantees, and demonstrate experimentally that it actually can achieve O(1) guarantees. In order to reduce the O(log N) time complexity of MCF, we further present a more efficient variant of MCF, called Fast Most Credit First (FMCF). FMCF achieves O(1) time complexity by utilizing approximation and synchronization, and at the same time preserves the O(log N) theoretical fairness, delay and delay jitter guarantees of MCF. We also implemented MCF and FMCF in NS2 simulator to compare the end to end delay performance with other fair scheduling algorithms. Our experimental results demonstrate that MCF outperforms two commonly used fair schedulers, and FMCF is able to closely match the performance of MCF with reduced time complexity. 2005 IEEE.",,
Pan,"Pan D., Yang Y.","Many networking/computing applications require high speed switching for multicast traffic at the switch/router level to save network bandwidth. However, existing queueing based packet switches and scheduling algorithms cannot perform well under multicast traffic. While the speedup requirement makes the output queued switch difficult to scale, the single input queued switch suffers from the head of line (HOL) blocking, which severely limits the network throughput An efficient yet simple buffering strategy to remove the HOL blocking is to use the virtual output queueing (VOQ), which has been shown to perform well under unicast traffic. However, it is impractical to use the traditional virtual output queued (VOQ) switches for multicast traffic, because a VOQ multicast switch has to maintain an exponential number of queues in each input port. In this paper, we give a novel queue structure for the input buffers of a VOQ multicast switch by separately storing the address information and data information of a packet, so that an input port only needs to manage a linear number of queues. In conjunction with the multicast VOQ switch, we present a first-in-first-out based multicast scheduling algorithm, FIFO Multicast Scheduling (FIFOMS), and conduct extensive simulations to compare FIFOMS with other popular scheduling algorithms. Our results fully demonstrate the superiority of FIFOMS in both multicast latency and queue space requirement.",,
Pissinou,"Sanchez Aleman C., Pissinou N., Alemany S., Kamhoua G.","Data accuracy and low energy consumption in mobile wireless sensor networks (MWSN) are crucial attributes for real-time applications. Although there are many existing methods to reconstruct data for wireless sensor networks, there are few developed for highly mobile environments. We propose Dynamic Trust Weight Allocation Technique (DTWA), a novel in-network data reconstruction method that determines the trust level in the data accuracy of each candidate node by evaluating spatio-temporal correlations, trajectory behavior, quantity and quality of data, and the number of hops traveled by the received data from the source. DTWA is capable of evaluating second-hand data when there is no first-hand data available and selecting second-hand data when this last is more accurate than the first-hand data. Our results demonstrate that data reconstructed using DTWA depicts significantly lower Root Mean Square Error (RMSE) compared to the IMC method when tested for both low and high incomplete dataset scenarios. 2018 IEEE.",,
Pissinou,"Tasnim S., Caldas J., Pissinou N., Iyengar S.S., Ding Z.","The rapid development of mobile sensing technologies (like GPS, RFID, accelerometer, gyroscope etc. various sensors in smart phones) has caused a rise in the large-scale capture of positioning data. These mobility data generated by heterogeneous mobile devices with embedded sensors are mostly known as trajectories. There are different algorithms to process the large amounts of mobility data for identifying mobility patterns. Even though few of these algorithms consider the use of semantic annotations on the data, none of the existing research has considered semantic annotations for online sub-trajectory clustering-based movement behavior analysis. In this paper, we incorporate semantics annotation in the raw trajectory data in order to discover various movement relationships between subtrajectories of mobile devices. We conduct experiment on a realworld data set. Along with the added advantage of semanticaware movement behavior analysis, our method is able to identify outliers in the clustering process with almost similar performance (average recall 0.92) as classic density-based clustering algorithm DBSCAN. 2018 IEEE.",,
Pissinou,"Aleman C.S., Pissinou N., Alemany S., Boroojeni K., Miller J., Ding Z.","In mobile wireless sensor networks (MWSN), data imprecision is a common problem. Decision making in real time applications may be greatly affected by a minor error. Even though there are many existing techniques that take advantage of the spatio-temporal characteristics exhibited in mobile environments, few measure the trustworthiness of sensor data accuracy. We propose a unique online context-aware data cleaning method that measures trustworthiness by employing an initial candidate reduction through the analysis of trust parameters used in financial markets theory. Sensors with similar trajectory behaviors are assigned trust scores estimated through the calculation of 'betas' for finding the most accurate data to trust. Instead of devoting all the trust into a single candidate sensor's data to perform the cleaning, a Diversified Trust Portfolio (DTP) is generated based on the selected set of spatially autocorrelated candidate sensors. Our results show that samples cleaned by the proposed method exhibit lower percent error when compared to two well-known and effective data cleaning algorithms in tested outdoor and indoor scenarios. 2018 IEEE.",,
Pissinou,"Kamhoua G.A., Pissinou N., Iyengar S.S., Beltran J., Miller J., Kamhoua C.A., Njilla L.L.","Crowdsourcing services have become one of the most common ways organizations can gather ideas for new products and services from large crowds of consumers by offering monetary rewards depending on the tasks. However, this monetary reward has begun to attract malicious crowds of users who wish to complete the task with minimal effort through collaboration. For instance, a task based on reviews of a product can be degraded when malicious users copy each other with minimal edits of the review, giving a misrepresentation of the true quality of the product. More specifically, we investigate the case where different malicious crowd sizes cooperate on different tasks, known as overlapping groups. Such sophisticated and hard to detect malicious crowds provide unfair evaluations and misleading results to the crowdsourcers. To overcome this type of attack, we propose two methods to point out such groups with high accuracy. The first method detects similar reviews by including a new proposed similarity between review texts and show the results outperform the vectorial similarity measures used in prior works. The second method is based on community detection on networks and exploits the semantic similarity of the reviews. The experiments were conducted on reviews from Ott dataset on Amazon Mechanical Turk. 2017 IEEE.",,
Pissinou,"Ma W., Sandoval O., Beltran J., Pan D., Pissinou N.","Network function virtualization enables flexible implementation of network functions, or middleboxes, as virtual machines running on standard servers. However, the flexibility also creates a challenge for efficiently placing such middleboxes, due to the availability of multiple hosting servers, capability of middleboxes to change traffic volumes, and dependency between middleboxes. In this paper, we address the optimal placement challenge of NFV middleboxes, and propose solutions for middleboxes of different traffic changing effects and with different dependency relations. First, we formulate the Traffic Aware Placement of Interdependent Middleboxes problem as a graph optimization problem. When the flow path is predetermined, we design optimal algorithms to place a non-ordered or totally-ordered middlebox set, and propose an efficient heuristic for the general scenario of a partially-ordered middlebox set after proving its NP-hardness. When the flow path is not predetermined, we show that the problem is NP-hard even for a non-ordered middlebox set, and propose a traffic and space aware routing heuristic. We have evaluated the proposed algorithms using large scale simulations and prototype experiments, and present extensive evaluation results to demonstrate the effectiveness of our design. 2017 IEEE.",,
Pissinou,"Ma W., Beltran J., Pan Z., Pan D., Pissinou N.","Network function virtualization (NFV) enables flexible deployment of middleboxes as virtual machines running on general hardware. Since different middleboxes may change the volume of processed traffic in different ways, improper deployment of NFV middleboxes will result in hot spots and congestion. In this paper, we study the traffic changing effects of middleboxes, and propose software-defined networking based middlebox placement solutions to achieve optimal load balancing. We formulate the traffic aware middlebox placement (TAMP) problem as a graph optimization problem with the objective to minimize the maximum link load ratio. First, we solve the TAMP problem when the flow paths are predetermined, such as the case in a tree. For a single flow, we propose the least-first-greatest-last (LFGL) rule and prove its optimality; for multiple flows, we first show the NP-hardness of the problem, and then propose an efficient heuristic. Next, for the general TAMP problem without predetermined flow paths, we prove that it is NP-hard even for a single flow, and propose the LFGL based MinMax routing algorithm by integrating LFGL with MinMax routing. We use a joint emulation and simulation approach to evaluate the proposed solutions, and present extensive experimental and simulation results to demonstrate the effectiveness of our design. 2004-2012 IEEE.",,
Pissinou,"Tasnim S., Pissinou N., Iyengar S.S.","With recent widespread usage of state-of-the-art technology (e.g., various mobile devices), environmental sensing is getting popular. The sensors used for sensing are small and due to the mobility they become more error-prone, which results in data corruption or loss from sensor. Therefore, cleaning of the sensed data is of high importance to recover the lost or corrupted data. In this paper, we propose a novel data cleaning mechanism to ensure better accuracy in environmental sensing applications. Based on the sensed data and the context relationship of each sensor, we update the credibility (or alternatively reliability) of the sensed data. We consider mobility pattern of the mobile sensor nodes while selecting the candidate sensor nodes for data stream cleaning. Through simulations, we evaluate the performance of our proposed approach. We compare our proposed sensor data stream cleaning approach with Influence Mean Cleaning (IMC) (a recent algorithm in data stream cleaning) and Mean-based cleaning. Simulation results show up to 24% reduction in root mean square error (RMSE) over IMC and up to 30% over Mean-based cleaning. 2017 IEEE.",,
Pissinou,"Kamhoua G.A., Pissinou N., Iyengar S.S., Beltran J., Kamhoua C., Hernandez B.L., Njilla L., Makki A.P.","Nowadays, Online Social Networks (OSNs) has become one of the most common ways among people to facilitate communication. This has made it a target for attackers to steal information from influential users and has brought new forms of customized attacks for OSNs. Attackers take advantage of the user's trustworthiness when using OSN. This exploitation leads to attacks with a combination of both classical and modern threats. Specifically, colluding attackers have been taken advantage of many OSNs by creating fake profiles of friends of the target in the same OSN or others. Colluders impersonate their victims and ask friend requests to the target in the aim to infiltrate her private circle to steal information. These types of attacks are difficult to detect in OSNs because multiple malicious users may have a similar purpose to gain information from their targeted user. The purpose of this paper is to overcome this type of attack by addressing the problem of matching user profiles across multiple OSNs. Then, we will extract both features and text from a user's profile and build a classifier based on supervised learning techniques. Simulation and experimental results are provided to validate the accuracy of our findings. 2017 IEEE.",,
Pissinou,"Njilla L., Ouete H., Pissinou N., Makki K.","A connection through a mobile node may not be available because of the greediness of selfish nodes. In this paper, we address the issue of dynamic packet forwarding by a set of wireless autonomous ad hoc nodes. Wireless nodes acting in a selfish manner try to use the resources of other nodes without their own participation. We model the dynamic packet forwarding problem as a negotiation game with an arbitrator. In our model, a group of mobile nodes requesting to forward packets negotiates with a mobile arbitrator until an agreement at least by simple majority is reached on a resource allocation. The mobile arbitrator submits offers to each mobile device in the group, and the mobile nodes decide to agree or disagree on the offer. The ultimate decision is made by simple majority. We investigate and solve the negotiation by finding the optimal Nash Equilibrium strategies of the game. We consider offers generated from Dirichlet's distribution for an ensemble of mobile devices over a finite and sporadic time limitation. The solution obtained from negotiation ensures that a mobile device always finds a peer or arbitrator to help forward packets and keep the network flowing. Mathematical proofs and MATLAB simulations support our model. 2017 IEEE.",,
Pissinou,"Njilla L.L., Kamhoua C.A., Kwiat K.A., Hurley P., Pissinou N.","An effective defense-in-depth in cyber security applies multiple layers of defense throughout a system. The goalis to defend a system against cyber-Attack using severalindependent methods. Therefore, a cyber-Attack that is able to penetrate one layer of defense may be unsuccessful in other layers. Common layers of cyber defense include: Attack avoidance, prevention, detection, survivability and recovery. It follows that in security-conscious organizations, the cyber security investment portfolio is divided into different layers of defense. For instance, a two-way division is agility and recovery. Cyber agility pursues attack avoidance techniques such that cyber-Attacks are rendered as ineffective, whereas cyber recovery seeks to fight-Through successful attacks. We show that even when the primary focus is on the agility of a system, recovery should be an essential point during implementation because the frequency of attacks will degrade the system and a quick and fast recovery is necessary. However, there is not yet an optimum mechanism to allocate limited cyber security resourcesinto the different layers. We propose an approach using theMarkov Decision Process (MDP) framework for resourcesallocation between the two end layers: Agility and recovery. 2017 IEEE.",,
Pissinou,"Shahid A.R., Jeukeng L., Zeng W., Pissinou N., Iyengar S.S., Sahni S., Varela-Conover M.","The proliferation of GPS-enabled mobile devices has made location-based services (LBSs) very popular in mobile networks and protecting user's location information has become a critical issue of it. In a typical location-based service framework, the LBS requires the user to provide precise location information to assure better Quality of Services(QoS), while the user wants to hide this information as much as possible. This dilemma of privacy and quality of service trade-off has been studied in literature extensively. In this paper, we propose a spatial obfuscation framework, Privacy Preserving Voronoi Cell (PPVC) based on Voronoi diagram. In this framework user's region of interest (ROI) is divided into n sectors and each sector is concealed with an irregular quadrilateral, generated from a n-gon Voronoi cell and user's location is confounded within a convex region called Anonymity Zone. We evaluate PPVC with simulated environment, showing the efficiency and efficacy of the proposed framework. Finally, we implement it on Android phone with a user centric privacy scale. 2017 IEEE.",,
Pissinou,"Njilla L.Y., Echual P., Pissinou N., Makki K.","Prevalent concerns with dynamic networks typically involve security. Especially with resource constraints in dynamic networks such as mobile ad-hoc networks (MANETs), security needs to be of particular consideration. In this paper, we first analyze the solution concept involved in optimizing resource allocation and data packet forwarding. In a MANET, the availability of having data packets forwarded may be insubstantial due to the presence of selfish nodes. Nodes may not want to participate in the network to preserve their own resources. We propose a packet-forwarding problem model with a negotiation game, where an arbitrator acts as a cluster head and initiates a bargaining game. Thereafter, we consider the possibility of having some group of nodes exhibit malicious behavior and collude to subvert the MANET. We investigate the problem by finding the optimal Nash Equilibrium (NE) strategies of the negotiation game. Then, we simulate the effect of the coalition of malicious nodes in a mobile environment. Simulation results support our model. 2016 IEEE.",,
Pissinou,"Njilla L.Y., Pissinou N., Makki K.","Today, online network services have evolved as the highest-emergent medium, enabling various online activities to be lucrative. However, these lucrative activities also bring new forms of privacy threats to the community. In a reliable e-business service, users should be able to trust the providers of the service to protect their customers' privacy. The service providers should not risk the personal and private information about their customers in cyberspace. There is an economic gain for a business provider when users trust the service provider. Despite those benefits, cyber security concern is the main reason some large organization may go bankrupted. Unfortunately, attackers may attempt to breach a provider's database and expose customers' private information. Therefore, in this paper, we propose a game theoretic framework for security and trust relationship in cyberspace for users, service providers, and attackers. Mathematical proofs and evaluations support our model. Service providers may use the model to see how important and dissuasive against attackers is when investing in cybersecurity. We propose a three-player game-theoretic approach framework for security and trust relationship in cyberspace for users, service providers, and attackers. The model implemented has a positive impact on users trust relation with their service provider while introducing some dissuasive factors to an attacker with intention to breach a provider's infrastructure in case a provider invests in cybersecurity. Copyright 2016 John Wiley & Sons, Ltd.",,
Pissinou,"Guo M., Pissinou N., Iyengar S.S.","Multiple sensors with different functions on sensor-enabled devices on a mobile vehicle are generating large amount of data in real time which consists of a vehicular user's instant context. Context-aware services and applications utilize users' contexts to provide personalized services in return. However, most of the context-aware applications are selfish and very curious of users' sensitive information. In this work, we propose a framework for privacy preserving mobile sensing by caching on mobile vehicles. The framework combines a context releasing component with sensitive data caching capability to achieve strong contextual privacy for mobile vehicles. The main strength of our approach is to improve the privacy level by data caching for sensitive contexts as well as the quality of query results. Finally, we conduct simulations to evaluate our framework for effectiveness and efficiency. 2016 IEEE.",,
Pissinou,"Guo M., Jin X., Pissinou N., Zanlongo S., Carbunar B., Iyengar S.S.","Recent advances in mobile device, wireless networking, and positional technologies have helped locationaware applications become pervasive. However, location trajectory privacy concerns hinder the adoptability of such applications. In this article, we survey existing trajectory privacy work in the context of wireless sensor networks, location-based services, and geosocial networks. In each context, we categorize and summarize the main techniques according to their own feathers. Furthermore, we discuss future trajectory privacy research challenges and directions. 2015 ACM.",,
Pissinou,"Njilla L.Y., Pissinou N.","In this paper, we address the problem of dynamic packet forwarding with a set of wireless autonomous ad hoc network nodes, where each node acting in a selfish manner tries to use the resources of other nodes. We model the dynamic packet forwarding problem as a modified Rubinstein-Stëhl bargaining game. In our model, a mobile node (player) negotiates with the other mobile node to obtain an agreeable and respectable sharing rule of packet forwarding based on its own resource available, such that a node should not agree to forward packets without the energy or storage capacity to do so. We investigate and solve this bargaining by finding the Subgame Perfect Nash Equilibrium (SPNE) strategies of the game. We consider finite horizon of the bargaining game and examine its SPNE. The solution obtained from bargaining ensures that a mobile device always finds a peer to help forward packets in order to keep the network at flow. Extensive simulations using OMNET++ simulation frameworks are conducted to evaluate how the level of participation of each mobile node impact the network overall performance. Simulation results show that our proposed bargaining game scheme performs better than other resource shared algorithms, namely the technique for order preference by similarity to ideal solution (TOPSIS) and the bargaining game based access network selection for heterogeneous network. 2015 IEEE.",,
Pissinou,"Punjala S.S., Pissinou N., Makki K.",A novel broadband reconfigurable antenna design that can cover different frequency bands is presented. This antenna has multiple resonant frequencies. The reflection coefficient graphs for this antenna are presented in this paper. The new proposed design was investigated along with RF MEMS switches and the results are also presented. Investigations were carried out to check the efficiency of the antenna in the wireless powering domain. The antenna was placed in a concrete block and its result comparison to that of a dipole antenna is also presented in this paper. 2015 Shishir Shanker Punjala et al.,,
Pissinou,"Guo M., Pissinou N., Iyengar S.S.","The popularity of location-aware mobile devices and the advances of wireless networking have seriously pushed location-based services into the IT market. However, moving users need to report their coordinates to an application service provider to utilize interested services that may compromise user privacy. In this paper, we propose an online personalized scheme for generating anonymity zones to protect users with mobile devices while on the move. We also introduce a strong adversary model, which can conduct inference attacks in the system. Our design combines a geometric transformation algorithm with a dynamic pseudonyms-changing mechanism and user-controlled personalized dummy generation to achieve strong trajectory privacy preservation. Our proposal does not involve any trusted third-party and will not affect the existing LBS system architecture. Simulations are performed to show the effectiveness and efficiency of our approach. 2015 IEEE.",,
Pissinou,"Nguyen H., Wahman E., Pissinou N., Iyengar S.S., Makki K.","Mobile learning can augment formal education and bridge the gap between formal and informal education by creating extended learning communities using any digital technology in connected or infrastructure-less environments. With the use of ad hoc networks and mobile authoring tools, we can now create an 'on-the-fly' learning scenario, where learners can create, share, and view content from their mobile devices without the need for server-client or infrastructure-based liaisons. Mobile learning, however, is not just about sharing content or learning using mobile, wireless, and portable devices. Rather, it is learning across spatiotemporal contexts that enables learners to form knowledge and understanding in different scenarios. When facing limited resources stemming from the characteristics of mobility and wireless technologies, determining the best practices for content creation and delivery becomes a challenge. This paper describes the architecture of a mobile-focused learning network designed so that learning tools, activities, contexts, and interactions are created as necessary over time and space while adhering to traditional learning object standards. Copyright 2015 John Wiley & Sons, Ltd.",,
Pissinou,"Islam M.A., Ren S., Pissinou N., Mahmud A.H., Vasilakos A.V.","While resource consolidation enables energy efficiency in virtualized data centers, it results in increased power density and causes excessive heat generation. To prevent servers from overheating and avoid potential damage and/or service outages, data centers need to incorporate temperature awareness in resource provisioning decisions. Moreover, data centers are subject to various peak power constraints (such as peak server power) that have to be satisfied at all times for reliability concerns. In this paper, we propose a novel resource management algorithm, called DREAM (Distributed REsource mAnagement with teMperature constraint), to optimally control the server capacity provisioning (via power adjustment), virtual machine (VM) CPU allocation and load distribution for minimizing the data center power consumption while satisfying the Quality of Service (QoS), IT peak power and maximum server temperature constraints. By using DREAM, each server can autonomously adjust its discrete processing speed (and hence, power consumption, too), and optimally decide the VM CPU allocation as well as amount of workloads to process in the hosted VMs, in order to minimize the total power consumption which incorporates both server power and cooling power. We formally prove that DREAM can yield the minimum power with an arbitrarily high probability while satisfying the peak power and server temperature constraints. To complement the analysis, we perform a simulation study and show that DREAM can significantly reduce the power consumption compared to the optimal temperature-unaware algorithm (by up to 33%) and equal load distribution (by up to 86%). 2014 Elsevier Inc. All rights reserved.",,
Pissinou,"Terry I.M., Wu A., Ramirez S., Makki A.P., Bobadilla L., Pissinou N., Iyengar S.S., Carbunar B.","The tight integration of mobile devices and apps into our daily routine impacts our approach to health. While existing solutions propose to take advantage of recent developments in mobile and sensor technologies to encourage users to lead healthier lives, we still lack a viable approach that motivates user participation. In this paper we aim to integrate fitness challenges into the daily routine of users, in order to motivate them to participate more frequently. We develop GeoFit, a mobile app that enables users to discover and add novel fitness challenges in their proximity. In addition to achievement badges, GeoFit relies on top-k lists to motivate users to become top performers. Since participation incentives may also raise cheating concerns, GeoFit leverages GPS and accelerometer sensors of the mobile device to verify the authenticity of fitness challenges performed by users. We have implemented and tested GeoFit on Android devices. Our experimental results show that the GeoFit client imposes only little overhead on the user device, while the server side can support hundreds of client interactions per second. 2014 IEEE.",,
Pissinou,"Gooden G., Pissinou N., Kamhoua C.A., Kwiat K.A.","In times of generation shortage relative to load demand, the only available way to ensure system stability is to shed load equivalent to the amount that demand exceeds supply. This load shedding procedure usually takes place in a series of 3 - 5 steps where each step has a quantity of load assigned to it. The process is carried out with each subsequent steps adding to the total load disconnected from supply until the amount of load/demand is less than the present supply capacity. Presently there is no method that gauges the percentage of load to be disconnected, then equitably allocates this loss of supply to the non-critical load on multiple feeders. We investigate using the game theory method of bargaining games for resources, within the setting where available supply needs to be allocated to the loading on feeders appropriately to their respective level of demand. 2014 IEEE.",,
Pissinou,"Nguyen H., Pissinou N., Iyengar S.S.","Due to the rapid increase in the number of mobile devices available to consumers, there is a need to evaluate these devices and their various features to see where they fit into the current idea of mobile learning. The emergence of lifelong learners has created a need for a new approach to learning so that learning can occur easily in non-standard classroom environments. Mobile learning is a new idea for learning because of the ubiquitous presence of mobile devices in people's lives and the possibility of learning anywhere from anyone it allows access to. Many problems exist with m-learning that have prevented its widespread adoption, outside of tests and limited classroom use that have been researched but never resolved. This paper offers an analysis of a few categories of questions that are important to mobile learning and critiques a few papers that investigate these areas. It also presents a new methodology to enable on-the-fly learning using emerging technology such as ad-hoc networks. 2014 IEEE.",,
Pissinou,"Tasnim S., Chowdhury M.A.R., Ahmed K., Pissinou N., Iyengar S.S.","Mobile applications can be enhanced to a great extent by using the offloading mechanism in an energy efficient manner to the bounty resourceful clouds. Due to the huge demand of smart phones, the issue of providing more processing capability to this resource constraint device is getting more concern now-a-days. In this paper, a method level offloading mechanism has been proposed where no prior image of the mobile device is needed to be transferred to the cloud. The application is partitioned at different points where the migration of the execution thread is performed from mobile device to nearby resourceful cloud to get the best execution performance in optimal energy cost. The mobile can complete the execution after the partitioned thread returns back from the cloud to the device. This mechanism increases scalability as well as performance in the form of faster execution speed of the mobile devices. Moreover, we consider the mobility of the mobile device and propose a solution to find the best cloud instance on the move. To find out which cloud to offload, the communication latency, capacity, and current load at individual clouds are considered to find out the best cloud to offload to ensure better service for the mobile device. The proposed solution has been simulated and compared against CloneCloud in two different simulation scenarios where we show that our method performs superior to CloneCloud. 2014 IEEE.",,
Pissinou,"Carbunar B., Rahman M., Pissinou N., Vasilakos A.V.","Geosocial networks (GSNs) are a popular extension of the growing online social networking phenomenon. The wealth of personal information voluntarily revealed by subscribers, however, exposes them to a wide range of privacy vulnerabilities. In this article we describe basic GSN features and show how external attackers can exploit them in order to gain access to sensitive user information. We introduce several properties that need to be satisfied by a viable, privacy preserving GSN solution. We survey the body of work addressing GSN attacks and privacy issues, and compare their ability to satisfy the crystallized properties. Finally, we propose several open research directions. 2013 IEEE.",,
Pissinou,"Jin H., Cheocherngngarn T., Levy D., Smith A., Pan D., Liu J., Pissinou N.","Data centers consume significant amounts of energy. As severs become more energy efficient with various energy saving techniques, the data center network (DCN) has been accounting for 20% or more of the energy consumed by the entire data center. While DCNs are typically provisioned with full bisection bandwidth, DCN traffic demonstrates fluctuating patterns. The objective of this work is to improve the energy efficiency of DCNs during off-peak traffic time by powering off idle devices. Although there exist a number of energy optimization solutions for DCNs, they consider only either the hosts or network, but not both. In this paper, we propose a joint optimization scheme that simultaneously optimizes virtual machine (VM) placement and network flow routing to maximize energy savings, and we also build an Open Flow based prototype to experimentally demonstrate the effectiveness of our design. First, we formulate the joint optimization problem as an integer linear program, but it is not a practical solution due to high complexity. To practically and effectively combine host and network based optimization, we present a unified representation method that converts the VM placement problem to a routing problem. In addition, to accelerate processing the large number of servers and an even larger number of VMs, we describe a parallelization approach that divides the DCN into clusters for parallel processing. Further, to quickly find efficient paths for flows, we propose a fast topology oriented multipath routing algorithm that uses depth-first search to quickly traverse between hierarchical switch layers and uses the best-fit criterion to maximize flow consolidation. Finally, we have conducted extensive simulations and experiments to compare our design with existing ones. The simulation and experiment results fully demonstrate that our design outperforms existing host-or network-only optimization solutions, and well approximates the ideal linear program. 2013 IEEE.",,
Pissinou,"Iyer V., Iyengar S., Pissinou N., Ren S.","The process of inversion, estimation and reconstruction of the sensor quality matrix, allows modeling the precision and accuracy, and in general the reliability of the model. When the sensor data ranges are not known a priori, current systems do not train on new data samples, rather they approximate based on the parameter's global average value, losing most of the spatial and temporal features. The proposed model, which we call SPOTLESS, checks the spatial integrity and temporal plausibility of streams generated by mobility patterns due to varying channel conditions. We define a minimum quality of the measured sensor data as local stream (QoD) requirements to give high precision by using distributed labeled training. In our SPOTLESS data-cleaning steps, to account for packet errors due to varying channel conditions, a soft-phy based decoding is selected for various Bit Error Rates (BER), minimizing packet loss at the mobile receiver. Numerical experiments for Rayleigh fading channels and mobile BER model examples are compared with large deployment of ground sensor collecting static data streams and Data MULE collecting multi-hop temporal data from the sensor to provide hypothetical parameter accuracy. Our results were obtained in the context of provisioning a minimum precision and accuracy stream (QoD) required for 802.15.4 mobile services. SPOTLESS data-cleaning algorithm coding provides 90% precision for static streams, and increases the plausible relevance of multi-hop mobile streams by 85% for task-based learning. 2013 IEEE.",,
Pissinou,"Jin H., Pan D., Liu J., Pissinou N.","Flow-level bandwidth provisioning (FBP) achieves fine-grained bandwidth assurance for individual flows. It is especially important for virtualization-based computing environments such as data centers. However, existing flow-level bandwidth provisioning solutions suffer from a number of drawbacks, including high implementation complexity, poor performance guarantees, and inefficiency to process variable length packets. In this paper, we study flow-level bandwidth provisioning for Combined Input Crosspoint Queued (CICQ) switches in the OpenFlow context. First, we propose the Flow-level Bandwidth Provisioning algorithm for CICQ switches, which reduces the switch scheduling problem to multiple instances of fair queuing problems, each utilizing a well-studied fair queuing algorithm. We theoretically prove that FBP can closely emulate the ideal Generalized Processing Sharing model, and accurately guarantee the provisioned bandwidth. Furthermore, we implement FBP in the OpenFlow software switch to obtain realistic performance data by a prototype. Leveraging the capability of OpenFlow to define and manipulate flows, we experimentally demonstrate a practical flow-level bandwidth provisioning solution. Finally, we conduct extensive simulations and experiments to evaluate the design. The simulation data verify the correctness of the analytical results, and show that FBP achieves tight performance guarantees. The experiment results demonstrate that our OpenFlow-based prototype can conveniently and accurately provision bandwidth at the flow level. 1968-2012 IEEE.",,
Pissinou,"Munavalli S.C., Pissinou N., Lagos L.E., Jin X.","Civil structures undergo many physical loads and environmental effects which can cause damage. Failing to identify the damage may lead to irreparable property and life loss. Thus, investments and efforts are being taken to develop an efficient structural health monitoring (SHM) technology. The objective of this research was to implement an embedded thermocouple sensor network under controlled and variable conditions to assess the location of the damage inside the structure. In this study, a civil structure embedded with thermocouple sensors at 4 different levels was analyzed with statistical tools to detect possible damage locations. This research is the first to implement a statistical pattern recognition methodology to detect damage in nuclear structures. 2013 IEEE.",,
Pissinou,"Pumpichet S., Jin X., Pissinou N.","The data imprecision received at a base station is common in mobile wireless sensor networks. In scenarios, data cleaning based on spatio-temporal relationships among sensors is not practical due to the unique, but commonly found, characteristics of sensor networks. As one of the first methods to clean sensor data in such environments, our proposed method deploys a sketch technique to periodically summarize N sensor samples into a fixed size array of memory and manage to recover values of missing or corrupted sensor samples at the base station. Our evaluation demonstrates that, with a small fixed portion of additional data transmission compared to original N data, the proposed method outperforms the existing data cleaning methods which assume the spatio-temporal relationship among sensors. 2013 IEEE.",,
Pissinou,"Islam M.A., Ren S., Pissinou N., Mahmud H., Vasilakos A.V.","With the ever-increasing power density generating an excessive amount of heat that potentially induces server damage and outages, data center operation must be judiciously optimized to prevent server overheating. Moreover, data center is subject to various peak power constraints (such as peak server power) that have to be satisfied at all times for reliability concerns. In this paper, we propose a novel resource management algorithm, called DREAM (Distributed REsource mAnagement with teMperature constraint), to optimally control the server capacity provisioning (via power adjustment) and load distribution for minimizing the data center operational cost while satisfying the IT peak power and maximum server inlet temperature constraints. By using DREAM, each server can autonomously adjust its discrete processing speed (and hence, power consumption, too) and optimally decide the amount of workloads to process, in order to minimize the total cost that incorporates both power consumption and service delay. We rigorously prove that DREAM can yield the minimum cost with an arbitrarily high probability while satisfying the peak power and inlet temperature constraints. To complement the analysis, we perform a simulation study and show that DREAM can significantly reduce the cost compared to the optimal temperature-unaware algorithm (by up to 23%) and equal load distribution (by up to 69%). 2013 IEEE.",,
Pissinou,"Jin X., Pissinou N., Pumpichet S., Kamhoua C.A., Kwiat K.","As new mobile Wireless Sensor Networks (mWSNs) for location-aware applications are emerging, trajectory privacy invasion is becoming an indispensable issue. Many promising techniques are under development. Considering the decentralized network architecture, most of Trajectory Privacy Preservation (TPP) techniques rely on the cooperation from peer nodes, cluster headers, or a third party. However, only a few works have addressed the issue of selfish behaviors in such cooperation required techniques. Nevertheless, the problem of facing selfish and compromised nodes in the noncooperative and hostile environment is rarely touched. In this paper, we apply Bayesian game theory to model cooperative, selfish and malicious behaviors of autonomous mobile nodes in decentralized mWSNs. We formulate and analyze the TPP game among peer nodes in both strategic and dynamic forms. The equilibrium strategies for users to evaluate the degree of trust in participating in in-network TPP activities are provided and analyzed in theoretical and simulation results. 2013 IEEE.",,
Pissinou,"Pumpichet S., Pissinou N., Jin X., Pan D.","The imprecision in data streams received at the base station is common in mobile wireless sensor networks. The movement of sensors leads to dynamic spatio-temporal relationships among sensors and invalidates the data cleaning techniques designed for stationary networks. As one of the first methods designed for mobile environments, we introduce a novel online method to clean the imprecise or dirty data in mobile wireless sensor networks. Our method deploys a belief parameter to select the helpful neighboring sensors to clean data. The belief parameter is based on sensor trajectories and the consistency of their streaming data correctly received at the base station. The evaluation over multiple mobility models shows that the proposed method outperforms the existing data cleaning algorithms, especially in sparse environments where the node density in the system is low. 2012 IEEE.",,
Pissinou,"Jin H., Pan D., Xu J., Pissinou N.","Virtual machines (VMs) may significantly improve the efficiency of data center infrastructure by sharing resources of physical servers. This benefit relies on an efficient VM placement scheme to minimize the number of required servers. Existing VM placement algorithms usually assume that VMs' demands for resources are deterministic and stable. However, for certain resources, such as network bandwidth, VMs' demands are bursty and time varying, and demonstrate stochastic nature. In this paper, we study efficient VM placement in data centers with multiple deterministic and stochastic resources. First, we formulate the Multidimensional Stochastic VM Placement (MSVP) problem, with the objective to minimize the number of required servers and at the same time satisfy a predefined resource availability guarantee. Then, we show that the problem is NP-hard, and propose a polynomial time algorithm called Max-Min Multidimensional Stochastic Bin Packing (M3SBP). The basic idea is to maximize the minimum utilization ratio of all the resources of a server, while satisfying the demands of VMs for both deterministic and stochastic resources. Next, we conduct simulations to evaluate the performance of M3SBP. The results demonstrate that M3SBP guarantees the availability requirement for stochastic resources, and M3SBP needs the smallest number of servers to provide the guarantee among the benchmark algorithms. 2012 IEEE.",,
Pissinou,"Jin X., Pissinou N., Chesneau C., Pumpichet S., Pan D.","The rapid development in micro-computing has allowed implementations of complex mobile Wireless Sensor Networks (mWSNs). Privacy invasion is becoming an indispensable issue along with the increasing range of applications of mWSNs. Private trajectory information not only indicates the movements of mobile sensors, but also reveals personal preferences and habits of users. In this paper, we propose the distributed Basic Trajectory Privacy (BTPriv) and Secondary Trajectory Privacy (STPriv) preservation algorithms to hide trajectory of data source nodes online. We set up various simulation environments for different applications. The effectiveness of our proposed algorithms is evaluated by the software implementation in simulation experiments. 2012 IEEE.",,
Pissinou,"Kamhoua C.A., Pissinou N., Makki K., Kwiat K., Iyengar S.S.","The demand on mobile data usage is exponentially increasing since the introduction of iPhones in 2007. The network became congested as millions of users tried to browse website and social networks, send e-mail, stream multimedia, and transfer file simultaneously. An immediate solution for the providers will be to change their pricing strategy with the goal of slowing down heavy users and decreasing the bandwidth demand. From the users' standpoint, network providers must constantly upgrade their infrastructure to accommodate new applications and devises. However, upgrading the infrastructure will be costly for the provider. A provider will prefer a minimum investment to upgrade the network while attracting the maximum number of customers. On the other hand, without regular upgrade of the network from the provider, there may be more congestion, more delays, and generally a low QoS at the user's dissatisfaction. Moreover, users that experience bad connection will be tempted to switch providers. We analyze the dynamic communication market and the users and providers' interactions in the framework of repeated game theory. We consider noise in user monitoring. We also compare two scenarios: individual and independent actions of users as opposed to the collective actions of users. For a collective action, a database aggregating users' QoS through a binary vote (good or bad QoS) needs to be implemented. The users keep their provider if and only if their majority reports a good QoS. This research shows that if the users collaborate, their bargaining power is increased. 2012 IEEE.",,
Pissinou,"Ali B.Q., Pissinou N., Makki K.","Wireless sensor network (WSN) data is often subjected to corruption and losses due to wireless medium of communication and presence of hardware inaccuracies in the nodes. For a WSN application to deduce an appropriate result it is necessary that the data received is clean, accurate, and lossless. WSN data cleaning systems exploit contextual associations existing in the received data to suppress data inconsistencies and anomalies. In this work we attempt to clean the data gathered from WSN by capturing the influence of changing dynamics of the environment on the contextual associations existing in the sensor nodes. Specifically, our work validates the extent of similarities among the sensed observations from contextually (spatio-temporally) associated nodes and considers the time of arrival of data at the sink to educate the cleaning process about the WSN's behavior. We term the data cleaning technique proposed in this work as time of arrival for data cleaning (TOAD). TOAD establishes belief on spatially related nodes to identify potential nodes that can contribute to data cleaning. By using information theory concepts and experiments on data sets from a real-time scenario we demonstrate and establish that validation of contextual associations among the sensor nodes significantly contributes to data cleaning. 2010 John Wiley & Sons, Ltd.",,
Pissinou,"Ganapathy V., Pissinou N., Makki S.K., Ali B.Q.","Existing pervasive applications are based on time series data that possess the form of time-ordered series of events. Such applications also embody the need to handle large volumes of unexpected events, often modified on-the-fly, containing conflicting information, and dealing with rapidly changing contexts while producing results with low latency. Correlating events across contextual dimensions holds the key to expanding the capabilities and improving the performance of these applications. In this paper we analyze complex-event semantic correlation that examines epistemic uncertainty in computer networks by using Dempster-Shafer theory to support a high-volume, event-based, in-network and non-deterministic pervasive network management. We consider imprecision and uncertainty when an event is detected and associate a belief parameter with the semantics and the detection of composite events. The approach taps into in-network processing capabilities of pervasive computer networks and can withstand missing or conflicting information gathered from multiple participating entities. In the end, we establish that a lightweight, distributed, large-volume, event-based technique which exploits epistemic uncertainty to correlate events along contextual dimensions provides a successful technique for enabling management of large-scale and pervasive contemporary and future computer networks. Copyright 2011 John Wiley & Sons, Ltd.",,
Pissinou,"Chen X., Makki K., Yen K., Pissinou N., Liu Z.","Providing a secure routing algorithm that defend against node compromise is a challenging task in sensor networks. In this paper, we propose a proactive secure routing algorithm (PSR) to conquer the undetected compromised nodes-the compromised nodes that the system has not detected them yet. Compared with other secure routing algorithms or ideas, routing paths in PSR not only bypass the detected compromised nodes but also bypass the nodes with larger probabilities of being compromised. To estimate the probability of node compromise, we introduce a node compromise distribution model. Simulations show that PSR is effective to prevent routing path from passing compromised nodes whether detected or not. ©2008 IEEE.",,
Pissinou,"Fan S., Fan J., Makki K., Pissinou N.","In this paper, we have proposed a framework of systems-on-chips clustering in application to complicated sensor networks. The framework can be applied to address the communication issues in distributed and large-scaled sensor nodes in wireless sensor network application. There are two communication categories under consideration, i.e. intra-nodes and inter-nodes. Due to the potentially higher frequency in the signal propagation within the sensor node, the characteristics of the interconnect among various systems-on-chips cannot be described in the traditionally lumped R, L, C components. We adapt a distributed transmission line model to address such issues and possibly improve the reliability in the intra-nodes communication. Furthermore, based on the bandwidth requirements of each sensor node, the large-scaled senor network is proposed to be transformed into a maze diagram by a user defined threshold bandwidth, so that many existing approaches may be applied to determine the routing paths in the inter-nodes communication to improve the efficiency of the overall network. 2008 IEEE.",,
Pissinou,"Wang Q., Chen Z., Makki K., Pissinou N., Chen C.","Internet worm attacks pose a significant threat to network security. In this work, we coin the term Internet worm tomography as inferring the characteristics of Internet worms from the observations of Darknet or network telescopes that are routable but unused IP addresses. Under the framework of Internet worm tomography, we attempt to infer worm temporal behaviors such as the host infection time and the worm infection sequence, and thus pinpoint patient zero. Specifically, we introduce statistical estimation techniques and propose method of moments, maximum likelihood, and linear regression estimators. We show analytically and empirically that our proposed estimators can better infer worm temporal characteristics than a naive estimator that has been used in the previous work. 2008 IEEE.",,
Pissinou,"Ali B.Q., Pissinou N., Makki K.","Approximate replication of data is a technique commonly used in distributed systems where the exact value from the source (client) is not required at the destination (server), rather a fairly accurate estimate of the actual data is sufficient for the end application to infer a result. A Wireless Sensor Network (WSN) is a special case of distributed system where energy is the prime concern governing its life span. Approximate replication when deployed in WSN causes heavy energy savings if the application at the sink is intelligent enough to deduce the result from an estimate of actual sensed data. The crux of approximate replication is the adaptation and prediction mechanisms that cause the sensing node to transmit only a subset of actual sensed data. In this paper, we have modified the traditional approach of approximate replication through coordinating filters, by deploying different adaptive algorithms at the sink and the node. We have also modified the existing model by properly initializing the filter parameters to facilitate faster convergence, whenever there is a change in the working mode of the model. We also bring about some changes in the LMS algorithm to make sure that it suits to the prediction conditions of the approximate replication platform. In the end we have demonstrated by means of simulation that the new modifications achieve better energy savings than the existing peer models by reducing the size of the subset of the information transmitted. 2008 IEEE.",,
Pissinou,"Mo Z., Zhu H., Makki K., Pissinou N.","Vehicular Ad-hoc Networks (VANETs) are gaining importance for inter-vehicle communication, because they allow for the local communication between vehicles without any infrastructure, configuration effort, and without the expensive cellular networks. As geographic routing can be used to achieve efficient data delivery in VANETs, how to provide location management service in VANETs to facilitate geographic routing remains a fundamental issue. In this paper we will present a novel location management protocol, call MALM, to provide location service to vehicles in VANETs. In MALM, a vehicle calculates the current location of other vehicles by using Kalman filtering based on the historical location information of other nodes. Theoretical analysis is provided to show that MALM is able to achieve high location information availability in the network. We evaluate the performance of MALM via extensive simulations. The simulation results show that MALM works efficiently in VANETs.",,
Pissinou,"Wan Z., Zhang J., Zhu H., Makki K., Pissinou N.","In wireless sensor networks, one of the big challenges is to achieve a satisfactory network lifetime while meeting quality of service (QoS) requirements. In this paper, we propose a novel MAC layer protocol which provides energy efficiency, latency guarantee, adaptability and scalability for wireless sensor networks carrying on delay-sensitive applications. Different from existing schemes, our protocol applies an opportunistic sleep scheme to adaptively balance the tradeoff between energy efficiency and the end-to-end delay. Each sensor node maintains its own wakeup probability which is adaptively adjusted according to the local network conditions to reduce the energy consumption without the increased transmission latency. In addition, an energyaware antcast-based data forwarding scheme is introduced to further reduce sleep latency influence on the end-to-end delay and improve the communication reliability. We implement our protocol in the ns-2 simulator and compare with other protocols. Simulation results show that our protocol outperforms other protocols in terms of balancing the tradeoff between energy efficiency and the end-to-end delay.",,
Pissinou,"Zhao X., Ganapathy V., Pissinou N., Makki K.","A distributed hash table (DHT) based approach for supporting forensic capability in Mobile Ad Hoc Networks (MANETs) is presented. The DHT-based approach has been modified to inhibit recursive increase in bandwidth consumption due to forensic activity - the process of logging is associated with that of packet delivery via customizable decreasing functions. Simulation has revealed that this approach limits the bandwidth requirement for forensic activities, although it requires a trade-off between bandwidth consumption and effective logging. The focus is to design a self-organized logging system over networks with dynamic topology. ©2008 IEEE.",,
Pissinou,"El Moutia A., Makki K., Pissinou N.","A Uniform Latin square of order k = m2 is an k x k square matrix that consists of k symbols from 0 to k-1 such that no symbol appears more than once in any row or in any column. This property is also maintained in any m x m area of main subsquares in a k x k Latin square. The uniqueness of each symbol in the main subsquares presents very attractive characteristic in applying Uniform Latin squares to time slot allocation problem in sensor networks. In this paper, we propose a space division multiple access (SDMA) scheme for wireless sensor networks based on Uniform Latin squares. The SDMA divides the geographical area into space divisions, where there is one-to-one map between space divisions and time slots. Because of the uniqueness of the symbol value in any main subsquares, the mapping of time slots into space divisions guaranties a collision-free medium access to sensor nodes. We also study the effect of the use of multiple transmission power levels and corresponding packet lengths on the system throughput. To do so, a self-controlled multiple power level algorithm has been proposed to improve the throughput of a multiple power level system. ©2008 IEEE.",,
Pissinou,"Chen X., Makki K., Yen K., Pissinou N.","Defending against attack is the key successful factor for sensor network security. There are many approaches that can be used to detect and defend against attacks, yet few are focused on modeling attack distribution. Knowing the distribution models of attacks can help system estimate the attack probability and thus defend against them effectively and efficiently. In this paper, we use probability theory to develop a basic uniform model, a basic gradient model, an intelligent uniform model and an intelligent gradient model of attack distribution in order to adapt to different application environments. These models allow systems to estimate the attack probability of each node under a given position and time. Applying these models in system security designs can improve system security performance and decrease the overheads in nearly every security area. Based on these models, we describe a novel probability secure routing algorithm that is effective to defend against attacks whether they are detected or not. Besides this application, we also introduce some other applications, such as secure routing that can save systems available energy and resources while still providing enough security, detecting attack, and key management.Rishe",,
Rangaswami,"Tarasov V., Rupprecht L., Skourtis D., Warke A., Hildebrand D., Mohamed M., Mandagere N., Li W., Rangaswami R., Zhao M.","Containers are a widely successful technology today popularized by Docker. Containers improve system utilization by increasing workload density. Docker containers enable seamless deployment of workloads across development, test, and production environments. Docker's unique approach to data management, which involves frequent snapshot creation and removal, presents a new set of exciting challenges for storage systems. At the same time, storage management for Docker containers has remained largely unexplored with a dizzying array of solution choices and configuration options. In this paper we unravel the multi-faceted nature of Docker storage and demonstrate its impact on system and workload performance. As we uncover new properties of the popular Docker storage drivers, this is a sobering reminder that widespread use of new technologies can often precede their careful evaluation. 2017 IEEE.",,
Rangaswami,"Tian D., Bates A., Butler K.R.B., Rangaswami R.","Defenders of enterprise networks have a critical need to quickly identify the root causes of malware and data leakage. Increasingly, USB storage devices are the media of choice for data exfiltration, malware propagation, and even cyber-warfare. We observe that a critical aspect of explaining and preventing such attacks is understanding the provenance of data (i.e., the lineage of data from its creation to current state) on USB devices as a means of ensuring their safe usage. Unfortunately, provenance tracking is not offered by even sophisticated modern devices. This work presents ProvUSB, an architecture for fine-grained provenance collection and tracking on smart USB devices. ProvUSB maintains data provenance by recording reads and writes at the block layer and reliably identifying hosts editing those blocks through attestation over the USB channel. Our evaluation finds that ProvUSB imposes a one-time 850 ms overhead during USB enumeration, but approaches nearly-bare-metal runtime performance (90% of throughput) on larger files during normal execution, and less than 0.1% storage overhead for provenance in real-world workloads. ProvUSB thus provides essential new techniques in the defense of computer systems and USB storage devices. 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,
Rangaswami,"Mishra D., Kulkarni P., Rangaswami R.","A hierarchical namespace is a common abstraction used for data organization within modern file systems. Fast translation of namespace objects to physical locations is necessary to carry out efficient file system operations. For reasons attributed to modularity, security, and to some extent legacy, namespace translations involves iterative translation of intervening directory objects from the root of the namespace. Namespace resolution is typically a multi-step process, potentially involving serialized I/O operations at each step. In this paper, we propose a rethink of the strategy to fetch pathname entries. Our technique, StepAhead, proactively utilizes hints about namespace translation lookup failures to enable parallel and just-in-time fetching of necessary path translation data into memory to increase cache hits significantly. With StepAhead, we measure an increase in cache hit rates for path translation data across a set of six workloads by as much as 51%, which in turn results in application speed-up of as much as 20%. Copyright 2016 ACM.",,
Rangaswami,"Santan R., Rangaswami R., Tarasov V., Hildebrand D.","There is a vast number and variety of file systems currently available, each optimizing for an ever growing number of storage devices and workloads. Users have an unprecedented, and somewhat overwhelming, number of data management options. At the same time, the fastest storage devices are only getting faster, and it is unclear on how well the existing file systems will adapt. Using emulation techniques, we evaluate five popular Linux file systems across a range of storage device latencies typical to low-end hard drives, latest high-performance persistent memory block devices, and in between. Our findings are often surprising. Depending on the workload, we find that some file systems can clearly scale with faster storage devices much better than others. Further, as storage device latency decreases, we find unexpected performance inversions across file systems.Finally, file system scalability in the higher device latency range is not representative of scalability in the lower, submillisecond, latency range. We then focus on Nilfs2 as an especially alarming example of an unexpectedly poor scalability and present detailed instructions for identifying bottlenecks in the I/O stack. 2015 ACM.",,
Rangaswami,"Santana R., Rangaswami R., Tarasov V., Hildebrand D.","There is a vast number and variety of file systems currently available, each optimizing for an ever growing number of storage devices and workloads. Users have an unprecedented, and somewhat overwhelming, number of data management options. At the same time, the fastest storage devices are only getting faster, and it is unclear on how well the existing file systems will adapt. Using emulation techniques, we evaluate five popular Linux file systems across a range of storage device latencies typical to low-end hard drives, latest high-performance persistent memory block devices, and in between. Our findings are often surprising. Depending on the workload, we find that some file systems can clearly scale with faster storage devices much better than others. Further, as storage device latency decreases, we find unexpected performance inversions across file systems. Finally, file system scalability in the higher device latency range is not representative of scalability in the lower, submillisecond, latency range. We then focus on Nilfs2 as an especially alarming example of an unexpectedly poor scalability and present detailed instructions for identifying bottlenecks in the I/O stack. Copyright 2015 ACM.",,
Rangaswami,"Koller R., Mashtizadeh A.J., Rangaswami R.","Host-side SSD caches represent a powerful knob for improving and controlling storage performance and improve performance isolation. We present Centaur, as a host-side SSD caching solution that uses cache sizing as a control knob to achieve storage performance goals. Centaur implements dynamically partitioned per-VM caches with per-partition local replacement to provide both lower cache miss rate, better performance isolation and performance control for VM workloads. It uses SSD cache sizing as a universal knob for meeting a variety of workload-specific goals including per-VM latency and IOPS reservations, proportional share fairness, and aggregate optimizations such as minimizing the average latency across VMs. We implemented Centaur for the VMware ESX hyper visor. With Centaur, times for simultaneously booting 28 virtual desktops improve by 42% relative to a non-caching system and by 18% relative to a unified caching system. Centaur also implements per-VM shares for latency with less than 5% error when running micro benchmarks, and enforces latency and IOPS reservations on OLTP workloads with less than 10% error. 2015 IEEE.",,
Rangaswami,"Kundu S., Rangaswami R., Zhao M., Gulati A., Dutta K.","The increasing VM density in cloud hosting services makes careful management of physical resources such as CPU, memory, and I/O bandwidth within individual virtualized servers a priority. To maximize cost-efficiency, resource management needs to be coupled with the revenue generating mechanisms of cloud hosting: the service level agreements (SLAs) of hosted client applications. In this paper, we develop a server resource management framework that reduces data center resource management complexity substantially. Our solution implements revenue-driven dynamic resource allocation which continuously steers the resource distribution across hosted VMs within a server such as to maximize the SLA-generated revenue from the server. Our experimental evaluation for a VMware ESX hyper visor highlights the importance of both resource isolation and resource sharing across VMs. The empirical data shows a 7%-54% increase in total revenue generated for a mix of 10-25 VMs hosting either similar or diverse workloads when compared to using the currently available resource distribution mechanisms in ESX. 2015 IEEE.",,
Rangaswami,"Consuegra M.E., Narasimhan G., Rangaswami R.","In this paper we experiment with practical algorithms for the vector repacking problem and its variants. Vector repacking, like vector packing, aims to pack a set of input vectors such that the number of bins used is minimized, while minimizing the changes from the previous packing. We also consider a variant of vector repacking that stores additional copies of items with the goal of improving the performance of vector repacking algorithms. In addition, our algorithms are parameterized so that they can be effectively optimized for a variety of resource allocation applications with different input characteristics and different cost functions. 2013 IEEE.",,
Rangaswami,"Kundu S., Rangaswami R., Gulati A., Zhao M., Dutta K.","With the growing adoption of virtualized datacenters and cloud hosting services, the allocation and sizing of resources such as CPU, memory, and I/O bandwidth for virtual machines (VMs) is becoming increasingly important. Accurate performance modeling of an application would help users in better VM sizing, thus reducing costs. It can also benefit cloud service providers who can offer a new charging model based on the VMs' performance instead of their configured sizes. In this paper, we present techniques to model the performance of a VM-hosted application as a function of the resources allocated to the VM and the resource contention it experiences. To address this multi-dimensional modeling problem, we propose and refine the use of two machine learning techniques: artificial neural network (ANN) and support vector machine (SVM). We evaluate these modeling techniques using five virtualized applications from the RUBiS and Filebench suite of benchmarks and demonstrate that their median and 90th percentile prediction errors are within 4.36% and 29.17% respectively. These results are substantially better than regression based approaches as well as direct applications of machine learning techniques without our refinements. We also present a simple and effective approach to VM sizing and empirically demonstrate that it can deliver optimal results for 65% of the sizing problems that we studied and produces close-to-optimal sizes for the remaining 35%. Copyright 2012 ACM.",,
Rangaswami,"Kundu S., Rangaswami R., Gulati A., Zhao M., Dutta K.","With the growing adoption of virtualized datacenters and cloud hosting services, the allocation and sizing of resources such as CPU, memory, and I/O bandwidth for virtual machines (VMs) is becoming increasingly important. Accurate performance modeling of an application would help users in better VM sizing, thus reducing costs. It can also benefit cloud service providers who can offer a new charging model based on the VMs' performance instead of their configured sizes. In this paper, we present techniques to model the performance of a VM-hosted application as a function of the resources allocated to the VM and the resource contention it experiences. To address this multi-dimensional modeling problem, we propose and refine the use of two machine learning techniques: artificial neural network (ANN) and support vector machine (SVM). We evaluate these modeling techniques using five virtualized applications from the RUBiS and Filebench suite of benchmarks and demonstrate that their median and 90th percentile prediction errors are within 4.36% and 29.17% respectively. These results are substantially better than regression based approaches as well as direct applications of machine learning techniques without our refinements. We also present a simple and effective approach to VM sizing and empirically demonstrate that it can deliver optimal results for 65% of the sizing problems that we studied and produces close-to-optimal sizes for the remaining 35%. 2012 ACM.",,
Rangaswami,"Koller R., Verma A., Rangaswami R.","Miss rate curves (MRCs) are a fundamental concept in determining the impact of caches on an application's performance. In our research, we use MRCs to provision caches for applications in a consolidated environment. Current techniques for building MRCs at the CPU caches level require changes to the applications and are restricted to a few processor architectures [7], [22]. In this work, we investigate two techniques to partition shared L2 and L3 caches in a server and build MRCs for the VMs. These techniques make different trade-offs across accuracy, flexibility, and intrusiveness dimensions. The first technique is based on operating system (OS) page coloring and does not require change in commodity hardware or application. We improve upon existing page-coloring based approaches by identifying and overcoming a subtle but real problem of unequal associative cache sets loading to implement accurate cache allocation. Our second technique called Cache Grabber is even less intrusive and requires no changes in hardware, OS, or application. We present a comprehensive evaluation of the relative merits of these and other techniques to estimate MRCs. Our evaluation study enables a data center administrator to select the technique most suitable to his (her) specific data center to provision caches for consolidated applications. 2011 IEEE.",,
Rangaswami,"Liu J., Rangaswami R., Zhao M.","We present VENICE, a project that aims at developing a high-fidelity, high-performance, and highly-controllable experimental platform on commodity computing infrastructure to facilitate innovation in existing and futuristic network systems. VENICE employs a novel model-driven network emulation approach that combines simulation of large-scale network models and virtual-machine-based emulation of real distributed applications. To accurately emulate the target system and meet the computation and communication requirements of its individual elements, VENICE adopts a holistic machine and network virtualization technique, called virtual time machine, in which the time advancement of simulated and emulated components are regulated in complete transparency to the test applications. In this paper, we outline the challenges and solutions to realizing the vision of VENICE. ©2010 IEEE.",,
Rangaswami,"Koller R., Verma A., Rangaswami R.","Accurately characterizing the resource usage of an application at various levels in the memory hierarchy has been a long-standing research problem. Existing characterization studies are either motivated by specific allocation problems (e.g., memory page allocation) or they characterize a specific memory resource (e.g., L2 cache). The studies thus far have also implicitly assumed that there is no contention for the resource under consideration. The inevitable future of virtualization driven consolidation necessitates the sharing of physical resources at all levels of the memory hierarchy by multiple virtual machines (VMs). Given the lack of resource isolation mechanisms at several levels of the memory hierarchy within current commodity systems, provisioning resources for a virtualized application not only requires a precise characterization of its resource usage but must also account for the impact of resource contention due to other co-located applications during its lifetime. In this paper, we present a unifying Generalized ERSS Tree Model that characterizes the resource usage at all levels of the memory hierarchy during the entire lifetime of an application. Our model characterizes capacity requirements, the rate of use, and the impact of resource contention, at each level of memory. We present a methodology to build the model and demonstrate how it can be used for the accurate provisioning of the memory hierarchy in a consolidated environment. Empirical results suggest that the Generalized ERSS Tree Model is effective at characterizing applications with a wide variety of resource usage behaviors. 2010 Elsevier B.V. All rights reserved.",,
Rangaswami,"Zhang C., Sadjadi S.M., Sun W., Rangaswami R., Deng Y.","The development of collaborative multimedia applications today follows a vertical development approach, where each application is built on top of low-level network abstractions such as the socket interface. This stovepipe development process is a major inhibitor that drives up the cost of development and slows down the innovation pace of new generations of communication applications. In this paper, we propose a network communication broker (NCB) that provides a unified higher-level abstraction for the class of multimedia collaborative applications. We demonstrate how NCB encapsulates the complexity of network-level communication control and media delivery, and expedites the development of applications with various communication logics. We investigate the minimum necessary requirements for the NCB abstraction. We identify that the concept of user-level sessions involving multiple parties and multiple media, is critical to designing a reusable NCB to facilitate next-generation multimedia communications. Furthermore, the internal design of NCB decouples the user-level sessions from network-level sessions, so that the NCB framework can accommodate heterogeneous networks, and applications can be easily ported to new network environments. In addition, we demonstrate how theextensible and self-managing design of NCB supports dynamic adaptation in response to changes in network conditions and user requirements. Springer Science + Business Media, LLC 2009.",,
Rangaswami,"Koller R., Rangaswami R.","Duplication of data in storage systems is becoming increasingly common. We introduce I/O Deduplication,a storage optimization that utilizes content similarity for improving I/O performance by eliminating I/O operations and reducing the mechanical delays during I/O operations. I/O Deduplication consists of three main techniques: content-based caching, dynamic replica retrieval, and selective duplication. Each of these techniques is motivated by our observations with I/O workload traces obtained from actively-used production storage systems, all of which revealed surprisingly high levels of content similarity for both stored and accessed data. Evaluation of a prototype implementation using these workloads showed an overall improvement in disk I/O performance of 28 to 47% across these workloads. Further breakdown also showed that each of the three techniques contributed significantly to the overall performance improvement. 2010 ACM.",,
Rangaswami,"Shimizu S., Rangaswami R., Duran-Limon H.A., Corona-Perez M.","Application resource usage models can be used in the decision making process for ensuring quality-of-service as well as for capacity planning, apart from their general use in performance modeling, optimization, and systems management. Current solutions for modeling application resource usage tend to address parts of the problem by either focusing on a specific application, or a specific platform, or on a small subset of system resources. We propose a simple and flexible approach for modeling application resource usage in a platform-independent manner that enables the prediction of application resource usage on unseen platforms. The technique proposed is application agnostic, requiring no modification to the application (binary or source) and no knowledge of application-semantics. We implement a Linux-based prototype and evaluate it using four different workloads including real-world applications and benchmarks. Our experiments reveal prediction errors that are bound within 6-24% of the observed for these workloads when using the proposed approach. 2009 Elsevier Inc. All rights reserved.",,
Rangaswami,"Milani M., Sadjadi S.M., Rangaswami R., Clarke P.J., Li T.","According to Computing Research Association, during each year between 2003 and 2007, fewer than 3% of the US's Ph.D.s graduates in computer science and computer engineering were Hispanic or African American and fewer than 20% were women. Such an under-representation precludes the benefits of diversity in computer sciences research and industry and consequently compromises the competitiveness of the US economy. It is therefore imperative that undergraduate institutions introduce students from these groups to research at an early stage of their academic careers and to provide them with the tools necessary for success in graduate school. The School of Computing and Information Sciences (SCIS) at Florida International University (FIU) has been working to strengthen the pipeline of underrepresented students to graduate work in computer science by hosting an NSF sponsored Research Experiences for Undergraduates (REU) site for the past three years. Our REU site has hosted 30 undergraduate students, 23 of them were underrepresented including 8 females, 16 Hispanics, and 4 African Americans, who published 13 technical papers. Six of the ten students who have already graduated, have started their graduate studies. Copyright 2009 ACM.",,
Rangaswami,"Li Y., Liu J., Rangaswami R.","This paper describes a new software infrastructure that combines the scalability and flexibility benefits of real-time network simulation with the realism of open-source routing protocol implementations. The infrastructure seamlessly integrates the open-source XORP router implementation with a real-time large-scale network simulation engine. The design uses a novel forwarding plane offloading approach that decouples routing from forwarding and confines the more resource consuming forwarding operations inside the simulation engine to reduce I/ O overhead. Experiments demonstrate superior performance of the software routing infrastructure without impairing accuracy. The infrastructure is shown to be able to support large-scale routing experiments on light-weight virtual machines. Copyright 2009, Inderscience Publishers.",,
Rangaswami,"Bhadkamkar M., Farfan F., Hristidis V., Rangaswami R.","Applications that manage semi-structured data are becoming increasingly commonplace. Current approaches for storing semi-structured data use existing storage machinery; they either map the data to relational databases, or use a combination of flat files and indexes. While employing these existing storage mechanisms provides readily available solutions, there is a need to more closely examine their suitability to this class of data. Particularly, retrofitting existing solutions for semi-structured data can result in a mismatch between the tree structure of the data and the access characteristics of the underlying storage device (disk drive). This study explores various possibilities in the design space of native storage solutions for semi-structured data by exploring alternative approaches that match application data access characteristics to those of the underlying disk drive. For evaluating the effectiveness of the proposed native techniques in relation to the existing solution, we experiment with XML data using the XPathMark benchmark. Extensive evaluation reveals the strengths and weaknesses of the proposed native data layout techniques. While the existing solutions work really well for deep-focused queries into a semi-structured document (those that result in retrieving entire subtrees), the proposed native solutions substantially outperform for the non-deep-focused queries, which we demonstrate are at least as important as the deep-focused. We believe that native data layout techniques offer a unique direction for improving the performance of semi-structured data stores for a variety of important workloads. However, given that the proposed native techniques require circumventing current storage stack abstractions, further investigation is warranted before they can be applied to general-purpose storage systems.",,
Rangaswami,"Farfán F., Hristidis V., Rangaswami R.","XML is acknowledged as the most effective format for data encoding and exchange over domains ranging from the World Wide Web to desktop applications. However, large-scale adoption into actual system implementations is being slowed down due to the inefficiency of its document-parsing methods. The recent development of lazy parsing techniques is a major step towards improving this situation, but lazy parsers still have a key drawback-they must load the entire XML document in order to extract the overall document structure before document parsing can be performed. We have developed a framework for efficient parsing based on the idea of placing internal physical pointers within the XML document that allow the navigation process to skip large portions of the document during parsing. We show how to generate such internal pointers in a way that optimizes parsing using constructs supported by the current W3C XML standard. A double-lazy parser (2LP) exploits these internal pointers to efficiently parse the document. The usage of supported W3C constructs to create internal pointers allows 2LP to be backward compatible-i.e., the pointer-augmented documents can be parsed by current XML parsers. We also implemented a mechanism to efficiently parse large documents with limited main memory, thereby overcoming a major limitation in current solutions. We study our pointer generation and parsing algorithms both theoretically and experimentally, and show that they perform considerably better than existing approaches. 2008 Elsevier B.V. All rights reserved.",,
Rangaswami,"Useche L., Guerra J., Bhadkamkar M., Alarcon M., Rangaswami R.","Power consumption within the disk-based storage subsystem forms a substantial portion of the overall energy footprint in commodity systems. Researchers have proposed external caching on a persistent, low-power storage device, which we term external caching device (ECD), to minimize disk activity and conserve energy. While recent simulation-based studies have argued in favor of this approach, the lack of an actual system implementation has precluded answering several key questions about external caching systems. We present the design and implementation of EXCES, an external caching system that employs prefetching, caching, and buffering of disk data for reducing disk activity. EXCES addresses important questions related to external caching, including the estimation of future data popularity, I/O indirection, continuous reconfiguration of the ECD contents, and data consistency. We evaluated EXCES with both micro- and macro- benchmarks that address idle, I/O intensive, and real-world workloads. Overall system energy savings was found to lie in the modest 2-14% range, depending on the workload, in somewhat of a contrast to the higher values predicted by earlier studies. Furthermore, while the CPU and memory overheads of EXCES were well within acceptable limits, we found that flash-based external caching can substantially degrade I/O performance. We believe that external caching systems hold promise. Further improvements in ECD technology, both in terms of their power consumption and performance characteristics can help realize the full potential of such systems. ©2008 IEEE.",,
Rangaswami,"Sadjadi S.M., Fong L., Badia R.M., Figueroa J., Delgado J., Collazo-Mojica X.J., Saleem K., Rangaswami R., Shimizu S., Limon H.A.D., Welsh P., Pattnaik S., Praino A., Villegas D., Kalayci S., Dasgupta G., Ezenwoye O., Martinez J.C., Rodero I., Chen S., Muñoz J., Lopez D., Corbalan J., Willoughby H., McFail M., Lisetti C., Adjouadi M.","The impact of hurricanes is so devastating throughout different levels of society that there is a pressing need to provide a range of users with accurate and timely information that can enable effective planning for and response to potential hurricane landfalls. The Weather Research and Forecasting (WRF) code is the latest numerical model that has been adopted by meteorological services worldwide. The current version of WRF has not been designed to scale out of a single organization's local computing resources. However, the high resource requirements of WRF for fine-resolution and ensemble forecasting demand a large number of computing nodes, which typically cannot be found within one organization. Therefore, there is a pressing need for the Grid-enablement of the WRF code such that it can utilize resources available in partner organizations. In this paper, we present our research on Grid enablement of WRF by leveraging our work in transparent shaping, GRID superscalar, profiling, code inspection, code modeling, meta-scheduling, and job flow management. Copyright 2008 ACM.",,
Rangaswami,"Guerra J., Useche L., Bhadkamkar M., Koller R., Rangaswami R.","Self-managing storage systems have recently received attention from the research community due to their promised ability of continuously adapting to best reflect high-level system goal specifications. However, this eventuality is currently being met by both conceptual and practical challenges that threaten to slow down the pace of innovation. We argue that two fundamental directions will help evolve the state of self-managing storage systems: (i) a standardized development environment for self-management extensions that also addresses ease of deployment, and (ii) a theoretical framework for reasoning about behavioral properties of individual and collective self-management extensions. We propose Active Block Layer Extensions (ABLE), an operating system infrastructure that aids the development and manages the deployed instances of self-management extensions within the storage stack. ABLE develops a theory behind block layer extensions that helps address key questions about overall storage stack behavior, data consistency, and reliability. We exemplify specific storage self-management solutions that can be built as stackable extensions using ABLE. Our initial experience with ABLE and few block layer extensions that we have been building, leads to believe that the ABLE infrastructure can substantially simplify the development and deployment of robust, self-managing, storage systems.",,
Rangaswami,"Deng Y., Masoud Sadjadi S., Clarke P.J., Hristidis V., Rangaswami R., Wang Y.","The convergence of data, voice, and multimedia communication over digital networks, coupled with continuous improvement in network capacity and reliability has resulted in a proliferation of communication technologies. Unfortunately, despite these new developments, there is no easy way to build new application-specific communication services. The stovepipe approach used today for building new communication services results in rigid technology, limited utility, lengthy and costly development cycle, and difficulty in integration. In this paper, we introduce communication virtual machine (CVM) that supports rapid conception, specification, and automatic realization of new application-specific communication services through a user-centric, model-driven approach. We present the concept, architecture, modeling language, prototypical design, and implementation of CVM in the context of a healthcare application.",,
Rangaswami,"Koller R., Rangaswami R., Marrero J., Hernandez I., Smith G., Barsilai M., Necula S., Sadjadi S.M., Li T., Merrill K.","Host intrusion prevention systems for both servers and end-hosts must address the dual challenges of accuracy and performance. Researchers have mostly focused on addressing the former challenge, suggesting solutions based either on exploitbased penetration detection or anomaly-based misbehavior detection, but yet stopping short of comprehensive solutions that leverage merits of both approaches. The second challenge, however, is rarely addressed; doing so comprehensively is important since these systems can introduce substantial overhead and cause system slowdown, more so when the system load is high. We present Rootsense, a holistic and real-time intrusion prevention system that combines the merits of misbehaviorbased and anomaly-based detection. Four principles govern the design and implementation of Rootsense. First, Rootsense audits events within different subsystems of the host operating system and correlates them to comprehensively capture the global system state. Second, Rootsense restricts the detection domain to root compromises only; doing so reduces run-time overhead and increases detection accuracy (root behavior is more easily modeled than user behavior). Third, Rootsense adopts a dual approach to intrusion detection - a root penetration detector detects activities that exploit system vulnerabilities to penetrate the security perimeter, and a root misbehavior detector tracks misbehavior by root processes. Fourth, Rootsense is designed to be configurable for overhead management allowing the system administrator to tune the overhead characteristics of the intrusion prevention system that affect foreground task performance. A Linux implementation of Rootsense is analyzed for both accuracy and performance, using several real-world exploits and a range of end-host and server benchmarks.",,
Rangaswami,"Sadjadi S.M., Shimizu S., Figueroa J., Rangaswami R., Delgado J., Duran H., Collazo-Mojica X.J.","In a Grid computing environment, resources are shared among a large number of applications. Brokers and schedulers find matching resources and schedule the execution of the applications by monitoring dynamic resource availability and employing policies such as first-come-first-served and back-filling. To support applications with timeliness requirements in such an environment, brokering and scheduling algorithms must address an additional problem - they must be able to estimate the execution time of the application on the currently available resources. In this paper, we present a modeling approach to estimating the execution time of long-running scientific applications. The modeling approach we propose is generic; models can be constructed by merely observing the application execution ""externally"" without using intrusive techniques such as code inspection or instrumentation. The model is cross-platform; it enables prediction without the need for the application to be profiled first on the target hardware. To show the feasibility and effectiveness of this approach, we developed a resource usage model that estimates the execution time of a weather forecasting application in a multi-cluster Grid computing environment. We validated the model through extensive benchmarking and profiling experiments and observed prediction errors that were within 10% of the measured values. Based on our initial experience, we believe that our approach can be used to model the execution time of other time-sensitive scientific applications; thereby, enabling the development of more intelligent brokering and scheduling algorithms. ©2008 IEEE.",,
Rangaswami,"Li Y., Liu J., Rangaswami R.","The ability to conduct accurate and realistic experiments is critical in furthering the research and development of network routing protocols. Existing framework for routing experiments is found to be lacking in one or more of the three required features: realism, scalability, and flexibility. We develop a new software infrastructure that combines the scalability and flexibility benefits of real-time network simulation with the realism of open-source routing protocol implementations. The infrastructure seamlessly integrates the open-source XORP router software with a previously developed real-time network simulation engine. Our design of the infrastructure uses a novel forwarding plane offloading approach that decouples routing from forwarding and confines the more resource consuming forwarding operations inside the simulation engine to reduce I/O overhead. Experiments demonstrate superior performance of the experimental infrastructure without impairing accuracy. 2008 IEEE.",,
Rangaswami,"Dutta K., Rangaswami R., Kundu S.","Database storage management at data centers is a manual, time-consuming, and error-prone task. Such management involves regular movement of database objects across storage nodes in an attempt to balance the I/O bandwidth utilization across disk drives. Achieving such balance is critical for avoiding I/O bottlenecks and thereby maximizing the utilization of the storage system. However, manual management of the aforesaid task, apart from increasing administrative costs, encumbers the greater risks of untimely and erroneous operations. We address the preceding concerns with STORM, an automated approach that combines low-overhead information gathering of database access and storage usage patterns with efficient analysis to generate accurate and timely hints for the administrator regarding data movement operations. STORM's primary objective is minimizing the volume of data movement required (to minimize potential down-time or reduction in performance) during the reconfiguration operation, with the secondary constraints of space and balanced I/O-bandwidth-utilization across the storage devices. We analyze and evaluate STORM theoretically, using a simulation framework, as well as experimentally. We show that the dynamic data layout reconfiguration problem is NP-hard and we present a heuristic that provides an approximate solution in O(Nlog(N/M) + (N/M)2) time, where M is the number of storage devices and N is the total number of database objects residing in the storage devices. A simulation study shows that the heuristic converges to an acceptable solution that is successful in balancing storage utilization with an accuracy that lies within 7% of the ideal solution. Finally, an experimental study demonstrates that the STORM approach can improve the overall performance of the TPC-C benchmark by as much as 22%, by reconfiguring an initial random, but evenly distributed, placement of database objects. 2008 ACM.",,
Rangaswami,"Farfán F., Hristidis V., Rangaswami R.","XML has become the standard format for data representation and exchange in domains ranging from Web to desktop applications. However, wide adoption of XML is hindered by inefficient document-parsing methods. Recent work on lazy parsing is a major step towards alleviating this problem. However, lazy parsers must still read the entire XML document in order to extract the overall document structure, due to the lack of internal navigation pointers inside XML documents. Further, these parsers must load and parse the entire virtual document tree into memory during XML query processing. These overheads significantly degrade the performance of navigation operations. We have developed a framework for efficient XML parsing based on the idea of placing internal physical pointers within the document, which allows skipping large portions of the document during parsing. The internal pointers are generated in a way that optimizes parsing for common navigation patterns. A double-Lazy Parser (2LP) is then used to parse the document that exploits the internal pointers. To create the internal pointers, we use constructs supported by the current W3C XML standard. We study our pointer generation and parsing algorithms both theoretically and experimentally, and show that they perform considerably better than existing approaches. Springer-Verlag Berlin Heidelberg 2007.",,
Rangaswami,"Rangaswami R., Masoud Sadjadi S., Prabakar N., Deng Y.","Multimedia communication services today are conceived, designed, and developed in isolation, following a stovepipe approach This has resulted in a fragmented and incompatible set of technologies and products. Building new communication services requires a lengthy and costly development cycle, which severely limits the pace of innovation. In this paper, we address the fundamental problem of automating the development of multimedia communication services, We propose a new paradigm for creating such services through declarative specification and generation, rather than through traditional design and development. Further, the proposed paradigm pays special attention to how the end-user specifies his/her communication needs, an important requirement largely ignored in existing approaches. 2007 IEEE.",,
Rangaswami,"Dutta K., Rangaswami R.","Database storage management in clustered storage environments is a manual, time-consuming, and error-prone task. Such management involves regular movement of database objects across nodes in the storage cluster so that storage utilization is maximized. We present STORM, an automated approach that guides this task by combining low-overhead information gathering about database access and storage usage patterns, efficient analysis of gathered information, and effective decision-making for reconfiguring data layout. The reconfiguration process is guided by the primary optimization objective of minimizing the total data movement required for the reconfiguration, with the secondary constraints of space and balanced I/O bandwidth utilizations across the storage nodes in the cluster. We model the reconfiguration decision-making as a multiconstraint optimization problem which is NP-hard. We then present a heuristic that provides an approximate solution in O(Nlog(N/M) + (N/M)2) time, where M is the number of storage nodes and N is the total number of database objects. A simulation study shows that the heuristic converges to an acceptable solution that is successful in balancing storage utilization with an accuracy that lies within 7% of the ideal solution. 2007 IEEE.",,
Rangaswami,"Rangaswami R., Dimitrijevi? Z., Chang E., Schauser K.","The performance of streaming media servers has been limited by the dual requirements of high disk throughput (to service more clients simultaneously) and low memory use (to decrease system cost). To achieve high disk throughput, disk drives must be accessed with large IOs to amortize disk access overhead. Large IOs imply an increased requirement of expensive DRAM, and, consequently, greater overall system cost. MEMS-based storage, an emerging storage technology, is predicted to offer a price-performance point between those of DRAM and disk drives. In this study, we propose storage architectures that use the relatively inexpensive MEMS-based storage devices as an intermediate layer (between DRAM and disk drives) for temporarily staging large disk IOs at a significantly lower cost. We present data layout mechanisms and synchronized IO scheduling algorithms for the real-time storage and retrieval of streaming data within such an augmented storage system. Analytical evaluation suggests that MEMS-augmented storage hierarchies can reduce the cost and improve the throughput of streaming servers significantly. 2007 ACM.",,
Rangaswami,"Zhang C., Sadjadi S.M., Sun W., Rangaswami R., Deng Y.","The development of collaborative multimedia applications today follows a vertical development approach, which is a major inhibitor that drives up the cost of development and slows down the pace of innovation of new generations of collaborative applications. In this paper, we propose a network communication broker (NCB) that provides a unified higher-level abstraction that encapsulates the complexity of network-level communication control and media delivery for the class of multimedia collaborative applications. NCB expedites the development of next-generation applications with diverse communication logics. Furthermore, NCB-based applications can be easily ported to new network environments. In addition, the self-managing design of NCB supports dynamic adaptation in response to changes in network conditions and user requirements. 2006 IEEE.",,
Rangaswami,"Yi D., Sadjadi S.M., Clarke P.J., Chi Z., Hristidis V., Rangaswami R., Prabakar N.","The convergence of data, voice and multimedia communication over digital networks, coupled with continuous improvement in network capacity and reliability has significantly enriched the ways we communicate. However, the stovepipe approach used to develop today's communication applications and tools results in rigid technology, limited utility, lengthy and costly development cycle, difficulty in integration, and hinders innovation. In this paper, we present a fundamentally different approach, which we call Communication Virtual Machine (CVM) to address these problems. CVM provides a user-centric, model-driven approach for conceiving, synthesizing and delivering communication solutions across application domains. We argue that CVM represents a far more effective paradigm for engineering communication solutions. The concept, architecture, modeling language, prototypical design and implementation of CVM are discussed. 2006 IEEE.",,
Rangaswami,"Dimitrijevi? Z., Rangaswami R., Chang E.Y.","Allowing higher-priority requests to preempt ongoing disk lOs is of particular benefit to delay-sensitive and real-time systems. In this paper, we present Semi-preemptible IO, which divides disk IO requests into small temporal units of disk commands to improve the preemptibility of disk access. We first lay out main design strategies to allow preemption of each component of a disk access - seek, rotation, and data transfer, namely, seek-splitting, JIT-seek, and chunking. We then present the preemption mechanisms for single and multidisk systems - JIT-preemption and JIT-migration. The evaluation of our prototype system showed that Semi-preemptible IO substantially improved the preemptibility of disk access with little loss in disk throughput and that preemptive disk scheduling could improve the response time for high-priority interactive requests. 2005 IEEE.",,
Rangaswami,"Rangaswami R., Dimitrijevic Z., Chang E., Gary Chan S.-H.","The use of interactive media has already gained considerable popularity. Interactivity gives viewers VCR controls like slow-motion, pause, fast-forward, and instant replay. However, traditional server-based or client-based approaches for supporting interactivity either consume too much network bandwidth or require large client buffering; and hence they are economically unattractive. In this paper, we propose the architecture and design of an interactive media proxy (IMP) server that transforms noninteractive broadcast or multicast streams into interactive ones for servicing a large number of end users. For IMP to work cost-effectively, it must carefully manage its storage devices, which are needed for caching voluminous media data. In this regard, we propose a fine-grained device management strategy consisting of three complementary components: disk profiler, data placement, and IO scheduler. Through quantitative analysis and experiments, we show that these fine-grained strategies considerably improve device throughput under various workload scenarios.",,
Rangaswami,"Rangaswami R., Dimitrijevi? Z., Chang E., Schauser K.E.","The performance of streaming media servers has been limited due to the dual requirements of high throughput and low memory use. Although disk throughput has been enjoying a 40% annual increase, slower improvements in disk access times necessitate the use of large DRAM buffers to improve the overall streaming throughput. MEMS-based storage is an exciting new technology that promises to bridge the widening performance gap between DRAM and disk-drives in the memory hierarchy. This paper explores the impact of integrating these devices into the memory hierarchy on the class of streaming media applications. We evaluate the use of MEMS-based storage for buffering and caching streaming data. We also show how a bank of k MEMS devices can be managed in either configuration and that they can provide a k-fold improvement in both throughput and access latency. An extensive analytical study shows that using MEMS storage can reduce the buffering cost and improve the throughput of streaming servers significantly.",,
Rangaswami,"Dimitrijevi? Z., Rangaswami R., Chang E.","Supporting preemptible disk access is essential for interactive multimedia applications that require short response time. In this study, we propose Virtual IO, an abstraction for disk IO, that transforms a non-preemptible IO request into a preemptible one. In order to achieve its objective efficiently, Virtual IO uses disk profiling to obtain accurate and detailed knowledge about the disk. Upon implementation of Virtual IO, we show that not only does Virtual IO enable highly preemptible disk access, but it does so with little or no loss in disk throughput.",,
Rangaswami,"Dimitrijevi? Z., Rangaswami R., Chang E.","This paper presents the architecture and implementation of XTREAM, a high-performance streaming multimedia system. XTREAM is supported by its three core components: IO Scheduler, Request Handler, and Admission Controller. Via extensive experiments, we show that, thanks to these core components, XTREAM can achieve a low response time as well as high throughput and high-quality service to simultaneous clients. 2002 IEEE.",,
Rangaswami,"Rangaswami R., Chang E., Li C., Chen M.","In this paper, we propose an interactive DTV design that converts non-interactive broadcast DTV streams into interactive ones for multiple simultaneous viewers. To enable viewing interactivity, we show that it is critical to organize data intelligently for improving IO resolution, reducing disk latency and minimizing storage cost. We propose three data placement schemes that offer different tradeoffs between IO resolution, disk latency and storage. By employing different schemes under different workload scenarios, an intelligent system can minimize memory use and hence the system cost. 2001 IEEE.",,
Rishe,"Lôckerath D., Ullrich O., Rishe N., Speckenmeyer E.","Timetable regularity, that is, equability of headways, is an important measure for service quality in high frequency public transit systems, assuring an evenly distributed passenger load as well as improving product attractiveness. However, to be feasible during daily operation a timetable may also have to adhere to other planning requirements, such as departure time coordination with other service providers or deliberately short headways to reduce the passenger load of follow-up vehicles. In this article, a disjunctive program formulation combining aspects of two previous optimization models is proposed, to generate regular public transit timetables adhering to planning requirements. The modeled requirements not only allow for the consideration of feasibility constraints from daily operations, but also for the consideration of simultaneous departures for transfer connections, an objective traditionally opposed to regularity. To show its applicability the approach is applied to two models of artificial transit networks as well as to models of the public transit network of Cologne, Germany. The results show that the proposed formulation can be used to generate timetables for network instances of realistic size in acceptable time. For networks consisting of multiple connected components it is shown that a decomposition approach can significantly reduce run times. 2018 Wiley Periodicals, Inc.",,
Rishe,"Izquierdo W., Martin H., Cabrerizo M., Barreto A., Andrian J., Rishe N., Gonzalez-Arias S., Loewenstein D., Duara R., Adjouadi M.","Predicting future cognitive status from current and past scores on objective cognitive tests and imaging measures would be useful in diagnosing Alzheimer's disease (AD) and to assess the progression of the disease. We used stochastic gradient boosting of decision trees on over 1,141 individuals whose clinical and imaging studies were available from the Alzheimer's disease Neuroimaging Initiative (ADNI) database. The proposed method outperformed all the algorithms tested in all five cognitive scores (MMSE, CDRS, RAVLT, ADAS11 and ADAS13), outranking all other state-of-the-art algorithms in terms of both Pearson's correlation coefficient and root mean square error. All correlation measures between predicted and actual cognitive scores were higher than 0.9. Given the large number of subjects included in this study, all correlations were statistically significant. For the subset of MCI patients, we compared the proposed method with state of the art algorithms. Here, the proposed method outperformed all the algorithms tested in all five cognitive scores. 2017 IEEE.",,
Rishe,"Bolivar S., Ortega F.R., Zock-Obregon M., Rishe N.D.","We propose a 3D environment in the form of a video game where the main idea is to increase Computer Science (CS) interest. We believe that by providing software that can be used by everyone, we can spark more interest in CS. We created a simple prototype emulating an Escape Room with the idea to attract individuals of any age range with a fun learning activity, but our primary focus is on teenagers and young adults. The puzzles in the game engage the player by giving them challenges that can be completed optimally by using computer science concepts. However, the game is presented as a typical puzzle game to avoid scaring away players who may have preconceived notions of computer science. The aim is to engage players through the puzzles to promote further interest in CS concepts. Springer International Publishing AG, part of Springer Nature 2018.",,
Rishe,"Torres N., Ortega F.R., Bernal J., Barreto A., Rishe N.D.","We present a Multi-Modal Interactive Paint application. Our work is intended to illustrate shortcomings in current multi-modal interaction and to present design strategies to address and alleviate these issues. In particular, from an input perspective use in a regular desktop environment. A serious of challenges are listed and addressed individually with their corresponding strategies in our discussion of design practices for multi-modality. We also identify areas which we will improve for future iterations of similar multi-modal interaction applications due to the findings identified in this paper. These improvements should alleviate shortcomings with our current design and provide further opportunities to research multi-modal interaction. Springer International Publishing AG, part of Springer Nature 2018.",,
Rishe,"Janeja V.P., Gholap J., Walkikar P., Yesha Y., Rishe N., Grasso M.A.","Clinical research and drug development trials generate large amounts of data. Due to the dispersed nature of clinical trial data across multiple sites and heterogeneous databases, it remains a challenge to harness these trial data for analytics to gain more understanding about the implementation of studies as well as disease processes. Moreover, the veracity of the results from analytics is difficult to establish in such datasets. We make a two-fold contribution in this paper: First, we provide a mechanism to extract task-relevant data using Master Data Management (MDM) from a clinical trial database with data spread over several domain datasets. Second, we provide a method for validating findings by collaborative utilization of multiple data mining techniques, namely: classification, clustering, and association rule mining. Overall, our approach aims at extracting useful knowledge from data collected during clinical trials to enable the development of faster and cheaper clinical trials that more accurate and impactful. For a demonstration of the efficacy of our proposed methods, we utilized the following datasets: (1) the National Institute on Drug Abuse (NIDA) data share repository and (2) the data from the Osteoarthritis initiative (OAI), where we found real-world implications in validating the findings using multiple data mining methods in a collaborative manner. The comparative results with existing state of the art techniques show the usefulness and high accuracy of our methods. 2018-IOS Press and the authors. All rights reserved.",,
Rishe,"Kench A., Janeja V.P., Yesha Y., Rishe N., Grasso M.A., Niskar A.","Patient data can be present in clinical notes, lab results, genomic data sources, environmental and geospatial data sources and tissue banks to name a few. A holistic view of the patient's health can be achieved when relevant data from multiple heterogeneous sources are extracted and analyzed in a personalized manner. Moreover, comparative analysis of patients can be performed when multiple patient records are viewed across these heterogeneous data sources. To address this need, we propose clinico-genomic data analytics to enhance personalized medicine treatment decisions using heterogeneous, high dimensional, sparse and massive datasets. We utilize this framework to discover similar patients and overlaps among patients in a set of features towards the goals of: (1) better cohort discovery for clinical trials, (2) better disease management by studying peer group of patients with similar diagnosis but better prognosis, (3) early disease diagnosis by identifying similar features in patients with the existing diagnosis. We propose novel approach in two areas: (1) integrating clinical and genomic data of patients and (2) combined data analytics in such heterogeneous datasets. Our approach is modeled as a unified clustering algorithm for finding correlations among clinical and genomic factors of patients. We integrate data containing risk causing Single Nucleotide Polymorphism's (SNP's) known from literature with clinical records of patients. In such heterogeneous data, we propose a combined similarity measure for numeric and nominal data attributes, which we use in our clustering algorithm. Our results show compelling overlaps among patients in the same cluster. These patients had high pair wise similarity and emulated the real world similarities between patients with co-morbid diseases. 2015 IEEE.",,
Rishe,"Robinson M., Rishe N.","We present a data processing framework aimed at facilitating the discovery of unknown genes in any genome, extracting DNA, RNA or Protein sub-sequences of any length. Using our GenomePro framework, we process raw input data files, of any size, in multiple formats such as NGS, fasta, and GBK, extracting all sub-sequences, of lengths selected by the end user. The only limitations are the computer storage size and/or operating system restrictions. Our framework can be applied to any life form genome. 2015 IEEE.",,
Rishe,"Yuanyuan F., Janeja V.P., Yesha Y., Rishe N., Grasso M.A., Niskar A.","Early prediction of treatment outcomes in RA clinical trials is critical for both patient safety and trial success. We hypothesize that an approach employing metadata of clinical trials could provide accurate classification of primary outcomes before trial implementation. We retrieved RA clinical trials metadata from ClinicalTrials.gov. Four quantitative outcome measures that are frequently used in RA trials, i.e., ACR20, DAS28, and AE/SAE, were the classification targets in the model. Classification rules were applied to make the prediction and were evaluated. The results confirmed our hypothesis. We concluded that the metadata in clinical trials could be used to make early prediction of the study outcomes with acceptable accuracy. 2015 IEEE.",,
Rishe,"Zhao G., Zhang M., Li T., Chen S.-C., Rishe N.","Dashboard cameras are increasingly used these days worldwide to record driving videos. These devices generate a vast amount of geo-referenced videos. However, most of this valuable data is lost due to loop recording. We present the City Recorder, a platform to provide street level video tours based on user-uploaded driving videos. We demonstrate here how to get a smooth route previewing experience using the best available videos. We show several use cases in a real urban environment. 2015 IEEE.",,
Rishe,"Zhang M., Wang H., Lu Y., Li T., Guang Y., Liu C., Edrosa E., Li H., Rishe N.","With the exponential growth of the usage of web map services, geo-data analysis has become more and more popular. This article develops an online spatial data analysis and visualization system, TerraFly GeoCloud, which helps end-users visualize and analyze spatial data and share the analysis results. Built on the TerraFly Geo spatial database, TerraFly GeoCloud is an extra layer running upon the TerraFly map and can efficiently support many different visualization functions and spatial data analysis models. Furthermore, users can create unique URLs to visualize and share the analysis results. TerraFly GeoCloud also enables the MapQL technology to customize map visualization using SQL-like statements. The system is available at http://terrafly.fiu.edu/GeoCloud/. 2015 ACM.",,
Rishe,"Fodor A., Rishe N., Karnieli E.","The current article outlines the main issues presented during the 4th Up Close and Personalized (UPCP), International Congress on Personalized Medicine, which took place in Tel Aviv (Israel) on 17-20 June 2015. The main topics presented included: phenotypic and 'omics' data in the personalized medicine of diabetes, obesity and oncology; updated reports on large-scale human genetics applied to drug discovery and precision medicine; new advances in disease targeted nano delivery systems, lessons derived from big clinical data and barriers for implementing personalized medicine in at the point of care. 2015 Future Medicine Ltd.",,
Rishe,"Tamir D.E., Rishe N.D., Kandel A.","Fuzzy Logic, introduced by Zadeh along with his introduction of fuzzy sets, is a continuous multi-valued logic system. Hence, it is a generalization of the classical logic and the classical discrete multi-valued logic (e.g. ?ukasiewiczê three/many-valued logic). Throughout the years Zadeh and other researches have introduced extensions to the theory of fuzzy setts and fuzzy logic. Notable extensions include linguistic variables, type-2 fuzzy sets, complex fuzzy numbers, and Z-numbers. Another important extension to the theory, namely the concepts of complex fuzzy logic and complex fuzzy sets, has been investigated by Kandel et al. This extension provides the basis for control and inference systems relating to complex phenomena that cannot be readily formalized via type-1 or type-2 fuzzy sets. Hence, in recent years, several researchers have used the new formalism, often in the context of hybrid neuro-fuzzy systems, to develop advanced complex fuzzy logic-based inference applications. In this chapter we reintroduce the concept of complex fuzzy sets and complex fuzzy logic and survey the current state of complex fuzzy logic, complex fuzzy sets theory, and related applications. Springer International Publishing Switzerland 2015.",,
Rishe,"Xue W., Li T., Rishe N.","Today, a large volume of hotel reviews is available on many websites, such as TripAdvisor (http://www.tripadvisor.com) and Orbitz (http://www.orbitz.com). A typical review contains an overall rating and several aspect ratings along with text. The rating is perceived as an abstraction of reviewersê satisfaction in terms of points. Although the amount of reviews having aspect ratings is growing, there are plenty of reviews including only an overall rating. Extracting aspect-specific opinions hidden in these reviews can help users quickly digest them without actually reading through them. The task mainly consists of two parts: aspect identification and rating inference. Most existing studies cannot utilize aspect ratings which are becoming abundant in the last few years. In this paper, we propose two topic models which explicitly model aspect ratings as observed variables to improve the performance of aspect rating inference over unrated reviews. Specifically, we consider sentiment distributions in the aspect level, which generate sentiment words and aspect ratings. The experiment results show our approaches outperform other existing methods on the data set crawled from Trip Advisor. Springer International Publishing Switzerland 2015.",,
Rishe,"Tamir D.E., Rishe N.D., Last M., Kandel A.","Epidemical crisis prediction is one of the most challenging examples of decision making with uncertain information. As in many other types of crises, epidemic outbreaks may pose various degrees of surprise as well as various degrees of –derivatives” of the surprise (i.e., the speed and acceleration of the surprise). Often, crises such as epidemic outbreaks are accompanied by a secondary set of crises, which might pose a more challenging prediction problem. One of the unique features of epidemic crises is the amount of fuzzy data related to the outbreak that spreads through numerous communication channels, including media and social networks. Hence, the key for improving epidemic crises prediction capabilities is in employing sound techniques for data collection, information processing, and decision making under uncertainty and exploiting the modalities and media of the spread of the fuzzy information related to the outbreak. Fuzzy logic-based techniques are some of the most promising approaches for crisis management. Furthermore, complex fuzzy graphs can be used to formalize the techniques and methods used for the datamining. Another advantage of the fuzzy-based approach is that it enables keeping account of events with perceived low possibility of occurrence via low fuzzy membership/truth-values and updating these values as information is accumulated or changed. In this chapter we introduce several soft computing based methods and tools for epidemic crises prediction. In addition to classical fuzzy techniques, the use of complex fuzzy graphs as well as incremental fuzzy clustering in the context of complex and high order fuzzy logic system is presented. Springer International Publishing Switzerland 2015.",,
Rishe,"Ortega F.R., Rishe N., Barreto A., Abyarjoo F., Adjouadi M.","We are motivated to find a multi-touch gesture detection algorithm that is efficient, easy to implement, and scalable to real-time applications using 3D environments. Our approach tries to solve the recognition for gestures with the use of feature extraction without the need of any previous learning samples. Before showing our proposed solution, we describe some algorithms that attempt to solve similar problems. Finally, we describe our code to accomplish off-line gesture recognition. Springer International Publishing Switzerland 2015.",,
Rishe,"Grasso M.A., Comer A.C., DiRenzo D.D., Yesha Y., Rishe N.D.",An association between periodontal disease and rheumatoid arthritis is believed to exist. Most investigations into a possible relationship have been case-control studies with relatively low sample sizes. The advent of very large clinical repositories has created new opportunities for data-driven research. We conducted a retrospective cohort study to measure the association between periodontal disease and rheumatoid arthritis in a population of 25 million patients. We demonstrated that subjects with periodontal disease were roughly 1.4 times more likely to have rheumatoid arthritis. These results compare favorably with those of previous studies on smaller cohorts. Additional work is needed to identify the mechanisms behind this association and to determine if aggressive treatment of periodontal disease can alter the course of rheumatoid arthritis.,,
Rishe,"Zhao G., Zhang M., Li T., Chen S.-C., Wolfson O., Rishe N.","There has been growing interest in correlating co-visualizing a video stream to the dynamic geospatial attributes of the moving camera. Moving videos comes from various GPS-enabled video recording devices and can be uploaded to video-sharing websites. Such public website do not presently display dynamic spatial features correlated to a video player. Although some systems include map based playback products, there has been no unified platform for users to share geo-referenced videos that takes spatial characteristics into account. We present here Moving Video Mapper, which integrates both historical and live georeferenced videos to give users an immersive experience in multidimensional perspectives. The platform has been evaluated using real data in an urban environment through several use cases. Springer International Publishing Switzerland 2015.",,
Rishe,"Ortega F.R., Barreto A., Rishe N., O-Larnnithipong N., Adjouadi M., Abyarjoo F.","We present GyroTouch, a multi-modal approach to the use of a digital gyroscope in a watch form-factor and a multi-touch desktop display with the aim to find properties that can yield better navigation in 3D virtual environments. GyroTouch was created to augment multitouch gestures with other devices. Our approach addressed 3D rotations and 3D Translation used in navigation of virtual environments. This work also includes an algorithm for estimating angular velocity for any given axis, using only one previous sample. Springer International Publishing Switzerland 2015.",,
Rishe,"Yasavur U., Amini R., Lisetti C., Rishe N.","Named-Entity Recognizers (NERs) are an important part of information extraction systems in annotation tasks. Although substantial progress has been made in recognizing domain-independent named entities (e.g. location, organization and person), there is a need to recognize named entities for domain-specific applications in order to extract relevant concepts. Due to the growing need for smart health applications in order to address some of the latest worldwide epidemics of behavioral issues (e.g. over eating, lack of exercise, alcohol and drug consumption), we focused on the domain of behavior change, especially lifestyle change. To the best of our knowledge, there is no named-entity recognizer designed for the lifestyle change domain to enable applications to recognize relevant concepts. We describe the design of an ontology for behavioral health based on which we developed a NER augmented with lexical resources. Our NER automatically tags words and phrases in sentences with relevant (lifestyle) domain-specific tags (e.g. [un/]healthy food, potentially-risky/healthy activity, drug, tobacco and alcoholic beverage). We discuss the evaluation that we conducted with with manually collected test data. In addition, we discuss how our ontology enables systems to make further information acquisition for the recognized named entities by using semantic reasoners. Copyright 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,
Rishe,"Lu Y., Zhang M., Li T., Liu C., Edrosa E., Rishe N.","With the exponential growth of the usage of web map services, the geo data analysis has become more and more popular. This paper develops an online Spatial Data Analysis System, TerraFly GeoCloud, which facilitates the end user to visualize and analyze spatial data, and to share the analysis results. Built on the TerraFly Geo spatial database, TerraFly GeoCloud is an extra layer running upon TerraFly map supporting many different visualization functions and spatial data analysis models. TerraFly GeoCloud also enables the MapQL technology to create maps using SQLlike statements. The TerraFly GeoCloud system is available at http://terrafly.fiu.edu/GeoCloud/. Copyright 2013 ACM.",,
Rishe,"Amini R., Lisetti C., Yasavur U., Rishe N.","In this paper, we discuss a novel approach for the computer-delivery of Brief Motivational Interventions (BMIs) for health behavior change. We describe the basic elements of our system architecture, and focus on enabling a multimodal Embodied Conversational Agent (ECA) to deliver the health behavior change interventions empathetically by adapting, in real-time, its verbal and non-verbal communication messages to those of its clients. The designed empathy model integrates a cognitive component and an affective components. We then discuss the evaluation experiment that we designed and conducted to evaluate the impact of empathy model on users' experience with the empathic character. Results indicate that, in comparison with the non-empathic counselor, the empathic one is better accepted (e.g., more enjoyable, empathizing, engaging, and likable) and some users might be willing to disclose more private information (e.g., drinking habits) to the counselor endowed with empathic abilities than the one without. 2013 IEEE.",,
Rishe,"Ballesteros J., Carbunar B., Rahman M., Rishe N.","Review based geosocial networks are online social networks centered on the location of venues and users as well as on reviews left by users for visited venues. The popularity and impact of reviews makes them an ideal tool for influencing public opinion. In this paper we study the effects of Yelp Elite events, organized for the benefit of Elite reviewers, on the image of the hosting venues. To this end, we introduce tools for identifying venues receiving abnormally large numbers of reviews in a short time and use them to detect correlations between events and hosting venues. We have implemented a browser plugin that makes users aware of Yelp event manipulations. We use data we collected from Yelp to show that Elite events have a noticeable short-term impact on the rating of hosting venues. 2013 IEEE.",,
Rishe,"Lisetti C., Amini R., Yasavur U., Rishe N.","We discuss our approach to developing a novel modality for the computer-delivery of Brief Motivational Interventions (BMIs) for behavior change in the form of a personalized On-Demand VIrtual Counselor (ODVIC), accessed over the internet. ODVIC is a multimodal Embodied Conversational Agent (ECA) that empathically delivers an evidence-based behavior change intervention by adapting, in real-time, its verbal and nonverbal communication messages to those of the user's during their interaction. We currently focus our work on excessive alcohol consumption as a target behavior, and our approach is adaptable to other target behaviors (e.g., overeating, lack of exercise, narcotic drug use, non-adherence to treatment). We based our current approach on a successful existing patient-centered brief motivational intervention for behavior change-the Drinker's Check-Up (DCU)-whose computer-delivery with a text-only interface has been found effective in reducing alcohol consumption in problem drinkers. We discuss the results of users' evaluation of the computer-based DCU intervention delivered with a text-only interface compared to the same intervention delivered with two different ECAs (a neutral one and one with some empathic abilities). Users rate the three systems in terms of acceptance, perceived enjoyment, and intention to use the system, among other dimensions. We conclude with a discussion of how our positive results encourage our long-term goals of on-demand conversations, anytime, anywhere, with virtual agents as personal health and well-being helpers. 2013 ACM.",,
Rishe,"Rahman M., Ballesteros J., Carbunar B., Rishe N., Vasilakos A.V.","Storing user friend lists, preferences and messages, online social networks have become a significant source of sensitive personal information. A recent addition to this space, geosocial networks (GSNs) such as Yelp [1] or Foursquare [2], collect even user locations, through check-ins performed by users at visited venues. Overtly, personal information allows GSN providers to offer a variety of applications, including personalized recommendations and targeted advertising, and venue owners to promote their businesses through spatio-temporal incentives (e.g., rewarding frequent customers through accumulated badges). Providing personal information exposes however users to significant risks, as social networks have been shown to leak [3] and even sell [4] user data to third parties. There exists therefore a conflict. Without privacy people may be reluctant to use geosocial networks; without user information the provider and venues cannot support applications and have no incentive to participate. 2013 by the Association for Computing Machinery, Inc.",,
Rishe,"Guzman A.M., Goryawala M., Wang J., Barreto A., Andrian J., Rishe N., Adjouadi M.","A new thermal imaging framework with unique feature extraction and similarity measurements for face recognition is presented. The research premise is to design specialized algorithms that would extract vasculature information, create a thermal facial signature, and identify the individual. The proposed algorithm is fully integrated and consolidates the critical steps of feature extraction through the use of morphological operators, registration using the Linear Image Registration Tool, and matching through unique similarity measures designed for this task. The novel approach at developing a thermal signature template using four images taken at various instants of time ensured that unforeseen changes in the vasculature over time did not affect the biometric matching process as the authentication process relied only on consistent thermal features. Thirteen subjects were used for testing the developed technique on an in-house thermal imaging system. The matching using the similarity measures showed an average accuracy of 88.46% for skeletonized signatures and 90.39% for anisotropically diffused signatures. The highly accurate results obtained in the matching process clearly demonstrate the ability of the thermal infrared system to extend in application to other thermal-imaging-based systems. Empirical results applying this approach to an existing database of thermal images prove this assertion. 2012 IEEE.",,
Rishe,"Ortega F.R., Barreto A., Rishe N., Adjouadi M., Abyarjoo F.","We are motivated to seek a fast and accurate multi-touch gesture detection algorithm that can be utilized for 3D navigation. Our current approach tries to solve the online gesture detection for multi-touch devices for translation, rotation and zooming gestures. 2013 IEEE.",,
Rishe,"Yasavur U., Lisetti C., Rishe N.","This paper describes the design of a multimodal spoken dialogue system using Markov Decision Processes (MDPs) to enable embodied conversational virtual health coach agents to deliver brief interventions for lifestyle behavior change - in particular excessive alcohol consumption. Its contribution is two fold. First, it is the first attempt to-date to study stochastic dialogue policy optimization techniques in the health dialogue domain. Second, it provides a model for longer branching dialogues (in terms of number of dialogue turns and number of slots) than the usual slot filling dialogue interactions currently available (e.g. tourist information domain). In addition, the model forms the basis for the generation of a richly annotated dialogue corpus, which is essential for applying optimization methods based on reinforcement learning. 2013 Springer-Verlag.",,
Rishe,"Ortega F.R., Barreto A., Rishe N.",We describe two approaches to augment multi-touch user input with commodity devices (Kinect and wiiMote). Copyright ACM 2013.,,
Rishe,"Ortego F., Rishe N., Barreto A., Adjouadi M.","The increase in availability of multi-touch devices has motivated us to consider interaction approaches outside the limitations associated with the use of a mouse. The problem that we try to solve is how to interact in a 3D world using a 2D surface multi-touch display. Before showing our proposed solution, we briefly review previous work in related fields that provided a framework for the development of our approach. Finally, we propose a set of multi-touch gestures and outline an experiment design for the evaluation of these forms of interaction. 2013 Springer Science+Business Media.",,
Rishe,"Lu Y., Zhang M., Li T., Guang Y., Rishe N.","With the exponential growth of the usage of web map services, the geo data analysis has become more and more popular. This paper develops an online spatial data analysis and visualization system, TerraFly GeoCloud, which facilitates end users to visualize and analyze spatial data, and to share the analysis results. Built on the TerraFly Geo spatial database, TerraFly GeoCloud is an extra layer running upon the TerraFly map and can efficiently support many different visualization functions and spatial data analysis models. Furthermore, users can create unique URLs to visualize and share the analysis results. TerraFly GeoCloud also enables the MapQL technology to customize map visualization using SQL-like statements. The system is available at http://terrafly.fiu.edu/GeoCloud/. Copyright 2013 ACM.",,
Rishe,"Lu Y., Zhao M., Zhao G., Wang L., Rishe N.","GIS application hosts are becoming more and more complicated. Thus, their management is more time consuming, and reliability decreases with the complexity of GIS applications increasing. We have designed, implemented, and evaluated, a virtualized whole Large Scale Distributed Spatial Data Visualization System for optimizing maintainability and performance when handling large amount of GIS data. We employ the virtual machines (VMs) technique, load balance cluster techniques, and autonomic resource management to improve the system's performance. The proposed system was prototyped on TerraFly [1], a production web map service, and evaluated using actual TerraFly workloads. The results show that the virtual TerraFly system has both good performance and much better maintainability. Our experiments show that the proposed Virtual TerraFly Geo-database system has doubled the reliability, and saved 20-30% computing resources cost compared to current static peak-load physical machine node allocations. 2013 IEEE.",,
Rishe,"Lu Y., Zhang M., Witherspoon S., Yesha Y., Yesha Y., Rishe N.","With the fast growing use of web-based map services, the performance of indexing and querying of location-based data is becoming a critical quality of service aspect. Spatial indexing is typically time-consuming and is not available to end-users. To address this challenge, we have developed and open-sourced an Online Indexing and Querying System for Big Geospatial Data, sksOpen. Integrated with the TerraFly Geospatial database [1], TerraFly sksOpen is an efficient indexing and query engine for processing Top-k Spatial Boolean Queries. Further, we provide ergonomic visualization of query results on interactive maps to facilitate the user's data analysis. 2013 IEEE.",,
Rishe,"Wang H., Lu Y., Guang Y., Edrosa E., Zhang M., Camarca R., Yesha Y., Lucic T., Rishe N.","GIS systems and online services are growing at a very fast pace, however, there are few online services for the analysis of geospatial epidemiology and their functionality is limited. We present a geospatial epidemiology analysis system on the TerraFly Geo-spatial Cloud platform. The system provides comprehensive spatial analysis methods and visualization. In this system, the user is not required to program in order to employ the functionality. All the datasets are stored in the Geo-spatial Cloud. This system is accessible at http://terrafly.fiu.edu/GeoCloud/. The system API algorithms adapted to geospatial epidemiology. The application utilizes the GeoCloud distributed storage system for the Big Data to be analyzed, it utilizes an interactive mapping API to display results. 2013 IEEE.",,
Rishe,"Ballesteros J., Cary A., Rishe N.","A spatial similarity join of two geospatial datasets finds pairs of records that are simultaneously similar on spatial and textual attributes. Such join is useful for a variety of applications, like data cleansing, record linkage, duplications detection and geocoding enhancement. Efficient techniques exist for the individual joins on either spatial or textual attributes. However, the combined problem has received much less research attention. This paper presents the SpSJoin (Spatial Similarity join) system to fill in this need. SpSJoin is a platform that merges geospatial and text processing techniques for efficiently performing spatial similarity joins. The platform leverages parallel computing with MapReduce to tackle scalability issues in joining large datasets. The efficiency of the proposed techniques are experimentally validated with a join case for improving the geolocation of entities in a real geospatial dataset with referential entities of another dataset. 2011 Authors.",,
Rishe,"Lisetti C.L., Yasavur U., Visser U., Rishe N.","In this article we describe work-in-progress about the development of avatar-based personalized assistants that can delivered motivational interviewing health behavior change interventions, tailored to its specific users Our approach combines the latest progress in Embodied Conversational Agents (ECAs), believable agents, and dialog systems. We discuss how we use different platforms to aim at providing accessibility of personalized health assistant, anytime anywhere. 2011 ICST.",,
Rishe,"Wang J., Barreto A., Rishe N., Andrian J., Adjouadi M.","This study establishes the mathematical foundation for a fast incremental multilinear method which combines the traditional sequential Karhunen-Loeve (SKL) algorithm with the newly developed incremental modified fast Principal Component Analysis algorithm (IMFPCA). In accordance with the characteristics of the data structure, the proposed algorithm achieves both computational efficiency and high accuracy for incremental subspace updating. Moreover, the theoretical foundation is analyzed in detail as to the competing aspects of IMFPCA and SKL with respect to the different data unfolding schemes. Besides the general experiments designed to test the performance of the proposed algorithm, incremental face recognition system was developed as a real-world application for the proposed algorithm. ICIC International 2011.",,
Rishe,"Sistla A.P., Wolfson O., Xu B., Rishe N.","We consider the problem of evaluating the continuous query of finding the k nearest objects with respect to a given moving point-object Oq among a set of n moving point-objects. The query returns a sequence of answer-pairs, namely pairs of the form (I, S) such that I is a time interval and S is the set of objects that are closest to Oq during I. Existing work on this problem lacks complexity analysis due to limited understanding of the maximum number of answer-pairs. In this paper we analyze the lower bound and the upper bound on the maximum number of answer-pairs. Then we consider two different types of algorithms. The first is off-line algorithms that compute a priori all the answer-pairs. The second type is on-line algorithms that at any time return the current answer-pair. We present the algorithms and analyze their complexity using the maximum number of answer-pairs. 2011 ACM.",,
Rishe,"You X., Adjouadi M., Guillen M.R., Ayala M., Barreto A., Rishe N., Sullivan J., Dlugos D., Vanmeter J., Morris D., Donner E., Bjornson B., Smith M.L., Bernal B., Berl M., Gaillard W.D.","To study the neural networks reorganization in pediatric epilepsy, a consortium of imaging centers was established to collect functional imaging data. Common paradigms and similar acquisition parameters were used. We studied 122 children (64 control and 58 LRE patients) across five sites using EPI BOLD fMRI and an auditory description decision task. After normalization to the MNI atlas, activation maps generated by FSL were separated into three sub-groups using a distance method in the principal component analysis (PCA)-based decisional space. Three activation patterns were identified: (1) the typical distributed network expected for task in left inferior frontal gyrus (Broca's) and along left superior temporal gyrus (Wernicke's) (60 controls, 35 patients); (2) a variant left dominant pattern with greater activation in IFG, mesial left frontal lobe, and right cerebellum (three controls, 15 patients); and (3) activation in the right counterparts of the first pattern in Broca's area (one control, eight patients). Patients were over represented in Groups 2 and 3 (P < 0.0004). There were no scanner (P = 0.4) or site effects (P = 0.6). Our data-driven method for fMRI activation pattern separation is independent of a priori notions and bias inherent in region of interest and visual analyses. In addition to the anticipated atypical right dominant activation pattern, a sub-pattern was identified that involved intensity and extent differences of activation within the distributed left hemisphere language processing network. These findings suggest a different, perhaps less efficient, cognitive strategy for LRE group to perform the task. 2010 Wiley-Liss, Inc.",,
Rishe,"Faller II K.J., Barreto A., Gupta N., Rishe N.","Head-Related Impulse Responses (HRIRs) are used in signal processing to model the synthesis of spatialized audio which is used in a wide variety of applications, from computer games to aids for the vision impaired. They represent the modification to sound due to the listener's torso, shoulders, head and pinnae, or outer ears. As such, HRIRs are somewhat different for each listener and require expensive specialized equipment for their measurement. Therefore, the development of a method to obtain customized HRIRs without specialized equipment is extremely desirable. In previous research on this topic, Prony's modeling method was used to obtain an appropriate set of time delays and a resonant frequency to approximate measured HRIRs. During several recent experimental attempts to improve on this previous method, a noticeable increase in percent fit was obtained using the Steiglitz-McBride iterative approximation method. In this paper we report on the comparison between these two methods and the statistically significant advantage found in using the Steiglitz-McBride method for the modeling of most HRIRs. 2006 Springer.",,
Rishe,"Teng W., Rishe N., Rui H.","NASA satellite data form a rich resource that is largely untapped by the applications user community, in part because of the complexity of using, and the cost of learning how to use, such data. These users are generally not interested in the data per se, but rather in one or more specific measurements (e.g., surface rain) from the data, which can then be seamlessly infused in their own environment (e.g., decision support systems). The Goddard Earth Sciences Data and Information Services Center Distributed Active Archive Center (GES DISC DAAC) has collaborated with the Florida International University's High Performance Database Research Center (FIU HPDRC) on an initial prototype effort, which has demonstrated the feasibility of making NASA satellite data more easily and seamlessly accessible, as a Web service, from the FIU's TerraFly environment. The latter is a Web-enabled system designed to aid in the visualization of spatial and remotely sensed data, by ""flying"" over the Earth's surface, via standard Web browsers. The other part of the collaboration is the GES DISC Giovanni, an online visualization and analysis system, which relieves the users of much of the data preparation work and provides a tool for easily and quickly obtaining information from the data, without having to download and handle large amounts of data. The prototype effects the seamless access of data by deep linking (i.e., geo-located) TerraFly and Giovanni, to enable a dynamic Web service, providing on-demand, near-real-time, satellite precipitation data. The Giovanni system is evolving towards a service-oriented architecture, thus making available to TerraFly users data not just from NASA but also potentially from many other sources, as well as making NASA satellite data potentially more widely accessible.",,
Rishe,"Xu B., Wolfson O., Rishe N.","In this paper we examine the dissemination of reports about resources in mobile peer-to-peer networks, where moving objects communicate with each other via short-range wireless transmission. Each disseminated report represents information about a spatial-temporal resource, such as the availability of a parking slot at a particular time and location. We introduce an architecture and a data model for dissemination of such reports. We develop an analytical model to quantify the benefit of report dissemination, where the benefit is measured in terms of time-saving. We further propose an incentive mechanism for participation in resource dissemination, and a method of pricing the resource information. 2006 IEEE.",,
Rishe,"Ilarri S., Wolfson O., Mena E., Illarramendi A., Rishe N.","Networks of sensors arise naturally in many different fields, from industrial applications (e.g., monitoring of environmental parameters in a chemical plant) to surveillance applications (e.g., sensors that detect the presence of intruders in a private property). The common feature of these applications is the necessity of a monitoring infrastructure that analyzes continuous supplies of data streams and outputs the values that satisfy certain constraints. In this paper we present an approach to process monitoring queries in a network of sensors with prediction functions. We consider sensors that communicate their values according to a threshold policy and our query processing leverages prediction functions to compare tuples efficiently and generate answers even in the absence of new incoming tuples. We deal with two types of constraints: window-join constraints and value constraints. 2006 IEEE.",,
Rishe,"Wolfson O., Xu B., Yin H., Rishe N.","In this paper we examine the benefit of reports about resources in mobile ad-hoc networks. Each disseminated report represents information about a spatio-temporal event, such as the availability of a parking slot or a cab request. Reports are disseminated by a peer-to-peer broadcast paradigm, in which an object periodically broadcasts the reports it carries to encountered objects. We evaluate the value of resource information in terms of how much time is saved when using the information to discover resources, compared to the case when the information is not used. Springer-Verlag Berlin Heidelberg 2005.",,
Rishe,"Hidalgo A., Monteagudo M., Rishe N., Graham S., Adjouadi M., Barreto A., Steinhoff R., Pierre K., Canas C.","In this paper, we consider the problem of creating a secure patient database system that complies with the patient privacy laws, particularly HIPAA. We analyze two models which seek to encrypt the database in order to comply with the patient privacy laws.",,
Rishe,"Torres J., Rishe N., Wolfson O., Teng W., Adjouadi M., Barreto A., Steinhoff R., Williams B., Gay J., Cary A.","With the proliferation of applications using remote sensing and high resolution satellite imagery, there has been a steady increase in the complexity of ways spatial data needs be manipulated and in the amount of raster data that has become available for use. With this increase, the management and storage of large sets of spatial raster objects via a DBMS needs to be addressed in order for applications to have efficient access to the data and high throughput. To provide this type of data management, there has been a shift to implement solutions that can represent and store raster data in databases. This paper presents a systematic approach to populate a database with the raster data. In particular, we present an algorithm that can be used to process and load large amounts of geo-spatial data, specifically satellite imagery, from a file system to a database.",,
Rishe,"Trigoso F., Rishe N., Adjouadi M., Barreto A., Steinhoff R., Necula S., Patel M., Gallon A., Brito A., Graham S., Cary A.","We propose a real-time system to effectively manage the patient flow problem. The system will provide automatic resource availability notifications, easy resource management and assignment, resource scheduling based on priority, proper activity scheduling, and proper staffing. A database schema of the proposed system is presented. The database schema is designed to model and present an automation solution to the patient flow problem. Since the patient flow problem is a general problem to most US hospitals, this schema can then be extended to be implemented to any specific hospital or medical practice with minimal effort. The generalized schema provides a simple solution to a complex problem. Definitions for the database objects are abstracted from the common consensus in the medical field. A sample implementation is described to show how the patient flow database model can be applied to provide a solution for a specific enterprise. Using technology to overcome the patient flow problem is not only feasible but also the obvious way to solve the patient flow problem. Technology will automate the solution, eliminate human error, provide accurate metrics to measure the successfulness of the implementation, and provide an easy, accessible and affordable medium to solve the problem.",,
Rishe,"Rishe N., Wolfson O., Adjouadi M., Barreto A., Steinhoff R., Bhadkamkar M., Varadarajan R., Cui Z., Sharifi M., Brito A., Slack C., Poitier P.","An Electronic Medical Record System (EMR) is responsible for efficiently gathering, storing, manipulating, and retrieving clinical information for providing timely patient care. A statewide EMR would serve a large number of the state's residents who opt into to the program, by pooling their medical records from all of their healthcare providers and then making the records available in real time to the patient and to all providers authorized by the patient. The system would guarantee to the patient the archival preservation of their records and easy interface thereto, as opposed to the need to collect fading paper copies from doctors A and B when going to see doctor C. Existing legal rights of the patient will allow pooling of data from doctors through easy low-tech interfaces and will entice doctors to opt-in higher-level interfaces which would allow for better classification of the patient's records and in return will provide better services to the doctor. Some patients would also opt to allow their records to be used for medical research. The system would save lives, significantly improve health care, and would make health care more cost effective. In this paper, we introduce the components of a clinical information system and propose our model for the implementation of such a system.",,
Rishe,"Wongsaroj B., Graham S., Wolfson O., Steinhoff R., Cary A., Chang L., Lee A., Rodriguez A., Singh P., Haynes R., Rush T., Barreto A., Adjouadi M., Rishe N.","The Extensible Markup Language (XML) is evolving into a leading means of Internet data exchange. With XML use increasing, a way to store and query the data natively in XML, rather than extract into SQL solely for data exchange, has become of importance. This paper proposes a method in which to handle and process XML natively by creating a Native XML Database and a proposes way to handle updating, inserting, and removing existing XML data.Zeng",,
Sadjadi,"Aazhang B., Abler R.T., Allebach J.P., Bost L.F., Cavallaro J.R., Chong E.K.P., Coyle E.J., Cullers J.B.S., Dennis S.M., Dong Y., Enjeti P.N., Filippas A.V., Froyd J.E., Garmire D., George J., Gilchrist B.E., Hohner G.S., Hughes W.L., Johnson A., Kim C., Kim H., Klenke R.H., Lagoudas M.Z., Llewellyn D.C., Lu Y.-H., Lybarger K.J., Marshall S., Muralidharan S., Ohta A.T., Ortega F.R., Riskin E.A., Rizzo D.M., Ryder C.R., Shiroma W.A., Siller T.J., Sonnenberg-Klein J., Sadjadi S.M., Strachan S.M., Taheri M., Woods G.L., Zoltowski C.B., Fabien B.C., Johnson P., Collins R., Murray P.","A survey of papers in the ASEE Multidisciplinary Engineering Division over the last three years shows three main areas of emphasis: individual courses; profiles of specific projects; and capstone design courses. However, propagating multidisciplinary education across the vast majority of disciplines offered at educational institutions with varying missions requires models that are independent of the disciplines, programs, and institutions in which they were originally conceived. Further, models that can propagate must be cost effective, scalable, and engage and benefit participating faculty. Since 2015, a consortium of twenty-four institutions has come together around one such model, the Vertically Integrated Projects (VIP) Program. VIP unites undergraduate education and faculty research in a team-based context, with students earning academic credits toward their degrees, and faculty and graduate students benefitting from the design/discovery efforts of their multidisciplinary teams. VIP integrates rich student learning experiences with faculty research, transforming both contexts for undergraduate learning and concepts of faculty research as isolated from undergraduate teaching. It provides a rich, costeffective, scalable, and sustainable model for multidisciplinary project-based learning. (1) It is rich because students participate multiple years as they progress through their curriculum; (2) It is cost-effective since students earn academic credit instead of stipends; (3) It is scalable because faculty can work with teams of students instead of individual undergraduate research fellows, and typical teams consist of fifteen or more students from different disciplines; (4) It is sustainable because faculty benefit from the research and design efforts of their teams, with teams becoming integral parts of their research. While VIP programs share key elements, approaches and implementations vary by institution. This paper shows how the VIP model works across sixteen different institutions with different missions, sizes, and student profiles. The sixteen institutions represent new and long-established VIP programs, varying levels of research activity, two Historically Black Colleges and Universities (HBCUs), a Hispanic-Serving Institution (HSI), and two international universities1. Theses sixteen profiles illustrate adaptability of the VIP model across different academic settings. American Society for Engineering Education, 2017.",,
Sadjadi,"Kargarmoakhar M., Taheri M., Sadjadi S.M.","This empirical study examines the adoption of agile software development, and the role of Scrum in computer science senior projects at Florida International University. This paper describes the senior projects and Scrum implementation. It highlights the advantages of incremental and iterative software development and discusses how Scrum can improve the productivity of software teams. In addition, it illustrates the other benefits such as engagement, transparency, frequent delivery and flexibility to change. Finally, it evaluates the outcomes by tracking the velocity estimations of each Scrum team. It demonstrates how Scrum and our tools can facilitate the software product development and the transition of our projects throughout semesters.",,
Sadjadi,"Taheri M., Sadjadi S.M.","With the advancement in technology, software development complexities are rising across the globe. This trend is forcing companies and organizations to adopt management methods and tools to accelerate time to market, more easily manage changing priorities, increase the customer satisfaction and reduce product expenses. Agile software development methods offer a solution to these issues, but problems remain over evaluation along with the offering of the correct agile software as well as a collection of agile tools. The purpose of this paper is to introduce best tools and features, criteria used for evaluating currently existing tools and propose a classification model to right agile tool selection. To prepare a list of the best tools and their features in the market, a practical research on existing tools and their features were performed. Finally, a classification model was prepared and the results show which tools best fit into different level of maturity in projects and companies. Copyright 2015 by KSI Research Inc. and Knowledge Systems Institute Graduate School.",,
Sadjadi,"Taheri M., Sadjadi S.M.","In the context of software development, companies, organizations and developer teams want to develop pure software products more efficiently and quickly. To deal with the new issues which accompany growing projects and software product complexities, agile tools boost simplicity and accelerate team's collaboration in a single framework. Adoption of agile tools can be a difficult process, due to agile tools must match company requirements to enhance the success of software development projects. The purpose of this paper is to introduce key criteria in details to consider in cloud services and cloud-based agile tool selection; also it presents top agile tools comparative classification based on a practical research to satisfy software development requirements under a cloud perspective.",,
Sadjadi,"Ezenwoye O., Sadjadi S.M.","Service-based workflow compositions like those used in Grid and Cloud computing require failure recovery mechanisms that support diverse failure handling strategies, separation of failure handling code from application code and user-defined exception handling. Apart from dealing with failures that result from such environments, these applications should be able to handle failures which are sensitive to the task context (task-specific failures). Traditional distributed system (transaction-based) recovery methods are not sufficient. In this paper, we show a selected group of projects that aim to provide exceptional behavior for service workflows. We classify these systems based on whether recover is performed at the task level, workflow-level or a combination of both. 2014 IEEE.",,
Sadjadi,"Rodero I., Villegas D., Bobroff N., Liu Y., Fong L., Sadjadi S.M.","The goal of Grid computing is to integrate the usage of computer resources from cooperating partners in the form of Virtual Organizations (VO). One of its key functions is to match jobs to execution resources efficiently. For interoperability between VOs, this matching operation occurs in resource brokering middleware, commonly referred to as the meta-scheduler or meta-broker. In this paper, we present an approach to a meta-scheduler architecture, combining hierarchical and peer-to-peer models for flexibility and extensibility. Interoperability is further promoted through the introduction of a set of protocols, allowing meta-schedulers to maintain sessions and exchange job and resource state using Web Services. Our architecture also incorporates a resource model that enables an efficient resource matching across multiple Virtual Organizations, especially where the compute resources and state are dynamic. Experiments demonstrate these new functional features across three distributed organizations (BSC, FIU, and IBM), that internally use different job scheduling technologies, computing infrastructure and security mechanisms. Performance evaluations through actual system measurements and simulations provide the insights on the architecture's effectiveness and scalability. 2013 Springer Science+Business Media Dordrecht.",,
Sadjadi,"Collazo-Mojica X.J., Sadjadi S.M.","We present our work on modeling distributed ensembles of virtual appliances (DEVAs) on Infrastructure as a Service (IaaS) clouds. Designing solutions on IaaS providers require a good under standing of the underlying details such as the software installation or the network configuration. We propose the use of DEVAs, a modeling approach built on top of the notion of virtual appliances, that allows easy-to-compose and ready-to-use cloud application architectures that are IaaS-agnostic, and that abstract away unnecessary details for web application developers. In this paper, we extend the definition of a DEVA from previous work by presenting an underlying metamodel and how that metamodel can be transformed to an actual deployment. We also present a case study where we model a web application architecture and we discuss how we can instantiate it in an IaaS cloud. We argue that the DEVA modeling approach is suitable for typical cloud use cases.",,
Sadjadi,"Villegas D., Sadjadi S.M.","Cloud computing represents a solution for applications with high scalability needs where usage patterns, and therefore resource requirements, may fluctuate based on external circumstances such as exposure or trending. However, in order to take advantage of the cloud's benefits, software engineers need to be able to express the application's needs in quantifiable terms. Additionally, cloud providers have to understand such requirements and offer methods to acquire the necessary infrastructure to fulfill the users' expectations. In this paper, we discuss the design and implementation of an Infrastructure as a Service cloud manager such that non-functional requirements determined during the requirements analysis phase can be mapped to properties for a group of Virtual Appliances running the application. The discussed management system ensures that expected Quality of Service is maintained during execution and can be considered during different development phases.",,
Sadjadi,"Villegas D., Sadjadi S.M.","Low upfront costs, rapid deployment of infrastructure and flexible management of resources has resulted in the quick adoption of cloud computing. Nowadays, different types of applications in areas such as enterprise web, virtual labs and high-performance computing are already being deployed in private and public clouds. However, one of the remaining challenges is how to allow users to specify Quality of Service (QoS) requirements for composite groups of virtual machines and enforce them effectively across the deployed resources. In this paper, we propose an Infrastructure as a Service resource manager capable of allocating Distributed Ensembles of Virtual Appliances (DEVAs) in the Cloud. DEVAs are groups of virtual machines and their network connectivities instantiated on heterogeneous shared resources with QoS specifications for individual entities as well as their connections. We discuss the different stages in their lifecycle: declaration, scheduling, provisioning and dynamic management, and show how this approach can be used to maintain QoS for complex deployments of virtual resources. 2011 Springer-Verlag.",,
Sadjadi,"Kalayci S., Dasgupta G., Fong L., Ezenwoye O., Sadjadi S.M.","This paper presents a decentralized execution approach to large-scale workflows on multiple resource domains. This approach includes a low overhead, decentralized runtime adaptation mechanism that improves the performance of the system. A prototype implementation based on standard Condor DAGMan workflow execution engine, does not require any modifications to Condor or its underlying system.",,
Sadjadi,"Ezenwoye O., Blake M.B., Dasgupta G., Sadjadi S.M., Kalayci S., Fong L.L.","Grid applications composed of multiple, distributed jobs are common areas for applying Web-scale workflows. Workflows over grid infrastructures are inherently complicated due to the need to both functionally assure the entire process and coordinate the underlying tasks. Often, these applications are long-running, and fault tolerance becomes a significant concern. Transparency is a vital aspect to understanding fault tolerance in these environments. 2010 IEEE.",,
Sadjadi,"Rodero I., Guim F., Corbalan J., Fong L., Sadjadi S.M.","The increasing demand for high performance computing resources has led to new forms of collaboration of distributed systems, such as grid computing systems. Moreover, the need for interoperability among different grid systems through the use of common protocols and standards has become increased in the last few years. In this paper we describe and evaluate scheduling techniques for multiple grid scenarios. In particular, they consist of the proposed ""bestBrokerRank"" broker selection policy and two different variants. The first one uses the resource information in aggregated forms as input, and the second one also uses the broker average bounded slowdown as a dynamic performance metric. From our evaluations performed with simulation tools, we state that, although the aggregation algorithms lose resource information accuracy, the broker selection policies using aggregated resource data do not penalize their performance significantly. Moreover, we show that the best performance results are obtained with the coordinated policy using dynamic performance information, in addition to aggregated resource information. Therefore, we conclude that delegating part of the scheduling responsibilities to the underlying scheduling layers promotes separation of concerns and is a good way to balance the performance among the different grid systems. 2009 Elsevier B.V. All rights reserved.",,
Sadjadi,"Rodero I., Guim F., Corbalan J., Fong L., Sadjadi S.M.","The increasing demand for resources of the high performance computing systems has led to new forms of collaboration of distributed systems such as interoperable grid systems that contain and manage their own resources. While with a single grid domain one of the most important tasks is the selection of the most appropriate set of resources to dispatch a job, in an interoperable grid environment this problem shifts to selecting the most appropriate domain containing the requiring resources for the job. In this paper, we present and evaluate broker selection strategies for interoperable grid systems. They use aggregated resource information as well as dynamic performance information of the underlying scheduling layers. From our evaluations performed with simulation tools, we conclude that aggregation techniques do not penalize performance significantly, and that delegating part of the scheduling responsibilities to the underlying scheduling layers is a good way to balance the load among the different grid systems. 2009 IEEE.",,
Sadjadi,"Ezenwoye O., Viswanathan B., Sadjadi S.M., Fong L., Dasgupta G., Kalayci S.","Scientific workflows are often composed by scientists that are not particularly familiar with performance and fault-tolerance issues of the underlying layer. The inherent nature of the infrastructure and environment for scientific workflow applications means that the movement of data comes with reliability challenges. Improving the reliablility scientific workflows in distributed environments, calls for the decoupling of data staging and computation activities, and each aspect needs to be addressed separately In this paper, we present an approach to managing scientific workflows that specifically provides constructs for reliable data staging. In our framework, data staging tasks are automatically separated from computation tasks in the definition of the workflow. High-level policies can be provided that allow for dynamic adaptation of the workflow to occur. Our approach permits the separate specification of the functional and non-functional requirements of the application and is dynamic enough to allow for the alteration of the workflow at runtime for optimization.",,
Sadjadi,"Liu Y., Bobroff N., Fong L., Seelam S., Villegas D., Sadjadi S.M., Rodero I.","Grid meta-broker is a key enabler in realizing the full potential of inter-operating grid computing systems. A challenge to properly evaluate the effectiveness of meta-brokers is the complexity of developing a realistic grid experimental environment. In this paper, this challenge is addressed by a unique combination of two approaches: using reduced workload traces to demonstrate the resource matching and scheduling functions of the meta-broker, and using emulation to provide a flexible and scalable modeling and management for local resources of a grid environment. Real workload traces are reduced while preserving their key workload characteristics to allow exploration of various dimensions of meta-broker functions in reasonable time. Evaluation of round-robin, queue-length, and utilization based meta-broker scheduling algorithms shows that they have different effects on various workloads. Copyright 2009 ACM.",,
Sadjadi,"Sadjadi S.M., McKinley P.K.","Increasingly, software systems are constructed by integrating and composing multiple existing applications. The resulting complexity increases the need for self-management of the system. However, adding autonomic behavior to composite systems is difficult, especially when the constituent components are heterogeneous and they were not originally designed to support such interactions. Moreover, entangling the code for self-management with the code for the business logic of the original applications may actually increase the complexity of the systems, counter to the desired goal. In this paper, we address autonomization of composite systems that use CORBA, one of the first widely used middleware platforms introduced more than 17 years ago that is still commonly used in numerous systems. We propose a model, called Adaptive CORBA Template (ACT), that enables autonomic behavior to be added to CORBA applications automatically and transparently, that is, without requiring any modifications to the code implementing the business logic of the original applications. To do so, ACT uses ""generic"" interceptors, which are added to CORBA applications at startup time and enable autonomic behavior to be introduced later at runtime. We have developed ACT/J, a prototype of ACT in Java. We describe a case study in which ACT/J is used to introduce three types of autonomic behavior (self-healing, self-optimization, and self-configuration) to a distributed surveillance application.",,
Sadjadi,"Sadjadi S.M., Trigoso F.","We define adaptability as the capacity of software in adjusting its behavior in response to changing conditions. To list just a few examples, adaptability is important in pervasive computing, where software in mobile devices need to adapt to dynamic changes in wireless networks; autonomic computing, where software in critical systems are required to be self-manageable; and grid computing, where software for long running scientific applications need to be resilient to hardware crashes and network outages. In this paper, we provide a realization of the transparent shaping programming model, called TRAP.NET, which enables transparent adaptation in existing .NET applications as a response to the changes in the application requirements and/or to the changes in their execution environment. Using TRAP.NET, we can adapt an application dynamically, at run time, or statically, at load time, without the need to manually modify the application original functionality-hence transparent. 2009 World Scientific Publishing Company.",,
Sadjadi,"Kalayci S., Ezenwoye O., Viswanathan B., Dasgupta G., Sadjadi S.M., Fong L.","Currently, many grid applications are developed as job flows that are composed of multiple jobs. The execution of job flows requires the support of a job flow manager and a job scheduler. Due to the long running nature of job flows, the support for fault tolerance and recovery policies is especially important. This support is inherently complicated due to the sequencing and dependency of jobs within a flow, and the required coordination between workflow engines and job schedulers. In this paper, we describe the design and implementation of a job flow manager that supports fault tolerance. First, we identify and label job flow patterns within a job flow during deployment time. Next, at runtime, we introduce a proxy that intercepts and resolves faults using job flow patterns and their corresponding fault-recovery policies. Our design has the advantages of separation of the job flow and fault handling logic, requiring no manipulation at the modeling time, and providing flexibility with respect to fault resolution at runtime. We validate our design with a prototypical implementation based on the ActiveBPEL workflow engine and GridWay Meta-scheduler, and Montage application as the case study. 2008 Springer Berlin Heidelberg.",,
Sadjadi,"Ezenwoye O., Sadjadi S.M.","With Web services, distributed applications can be encapsulated as self-contained, discoverable software components that can be integrated to create other applications. BPEL allows for the composition of existing Web services to create new higher-function Web services. We identified that the techniques currently applied at development time are not sufficient for ensuring the reliability of composite Web services In this paper, we present a language-based approach to transparently adapting BPEL processes to improve reliability. This approach addresses reliability at the Business process layer (i.e the language layer) using a code generator, which weaves fault-tolerant code to the original code and an external proxy. The generated code uses standard BPEL constructs, and therefore, does not require any changes to the BPEL engine.",,
Sadjadi,"Dasgupta G., Ezenwoye O., Fong L., Kalayci S., Sadjadi S.M., Viswanathan B.","The execution of job flow applications is a reality today in academic and industrial domains. Current approaches to execution of job flows often follow proprietary solutions on expressing the job flows and do not leverage recurrent job-flow patterns to address faults in Grid computing environments. In this paper, we provide a design solution to development of job-flow managers that uses standard technologies such as BPEL and JSDL to express job flows and employs a two-layer peer-to-peer architecture with interoperable protocols for cross-domain interactions among job-flow mangers. In addition, we identify a number of recurring job-flow patterns and introduce their corresponding fault-tolerant patterns to address runtime faults and exceptions. Finally, to keep the business logic of job flows separate from their fault-tolerant behavior, we use a transparent proxy that intercepts job-flow execution at runtime to handle potential faults using a growing knowledge base that contains the most recently identified job-flow patterns and their corresponding fault-tolerant patterns.",,
Sadjadi,"Dasgupta G., Ezenwoye O., Fong L., Kalayci S., Sadjadi S.M., Viswanathan B.","The execution of job flow applications is a reality today in academic and industrial domains. In this paper, we propose an approach to adding self-healing behavior to the execution of job flows without the need to modify the job flow engines or redevelop the job flows themselves. We show the feasibility of our non-intrusive approach to self-healing by inserting a generic proxy to an existing two-level job-flow management system, which employs job flow based service orchestration at the upper level, and service choreography at the lower level. The generic proxy is inserted transparently between these two layers so that it can intercept all their interactions. We developed a prototype of our approach in a real Grid environment to show how the proxy facilitates runtime handling for failure recovery. 2008 IEEE.",,
Sadjadi,"Liu Y., Sadjadi S.M., Fong L., Rodero I., Villegas D., Kalayci S., Bobroff N., Martinez J.C.","Grid computing supports workload execution on computing resources that are shared across a set of collaborative organizations. At the core of workload management for Grid computing is a software component, called meta-scheduler or Grid resource broker, that provides a virtual layer on top of heterogeneous Grid middleware, schedulers, and resources. Meta-schedulers typically enable end-users and applications to compete over distributed shared resources through the use of one or more instances of the same meta-scheduler, in a, centralized or distributed manner, respectively. We propose an approach to enabling autonomic meta-scheduling through the use of a new communication protocol that -if adopted by different meta-schedulers or by the applications using them-can improve the workload execution while avoiding potential chaos, which can be resulted from blind competition over resources. This can be made possible by allowing the metaschedulers and/or their applications to engage in a process to negotiate their roles (e.g., consumer, provider, or both), scheduling policies, service-level agreement, etc. To show the feasibility of our approach, we developed a prototype that enables some preliminary autonomic management among three different meta-schedulers, namely, GridWay, eNANOS, and TDWB. 2008 IEEE.",,
Sadjadi,"Koller R., Rangaswami R., Marrero J., Hernandez I., Smith G., Barsilai M., Necula S., Sadjadi S.M., Li T., Merrill K.","Host intrusion prevention systems for both servers and end-hosts must address the dual challenges of accuracy and performance. Researchers have mostly focused on addressing the former challenge, suggesting solutions based either on exploitbased penetration detection or anomaly-based misbehavior detection, but yet stopping short of comprehensive solutions that leverage merits of both approaches. The second challenge, however, is rarely addressed; doing so comprehensively is important since these systems can introduce substantial overhead and cause system slowdown, more so when the system load is high. We present Rootsense, a holistic and real-time intrusion prevention system that combines the merits of misbehaviorbased and anomaly-based detection. Four principles govern the design and implementation of Rootsense. First, Rootsense audits events within different subsystems of the host operating system and correlates them to comprehensively capture the global system state. Second, Rootsense restricts the detection domain to root compromises only; doing so reduces run-time overhead and increases detection accuracy (root behavior is more easily modeled than user behavior). Third, Rootsense adopts a dual approach to intrusion detection - a root penetration detector detects activities that exploit system vulnerabilities to penetrate the security perimeter, and a root misbehavior detector tracks misbehavior by root processes. Fourth, Rootsense is designed to be configurable for overhead management allowing the system administrator to tune the overhead characteristics of the intrusion prevention system that affect foreground task performance. A Linux implementation of Rootsense is analyzed for both accuracy and performance, using several real-world exploits and a range of end-host and server benchmarks.",,
Sadjadi,"Ezenwoye O., Sadjadi S.M.","Web services paradigm is allowing applications to electronically interact with one another over the Internet. The business process execution language (BPEL) takes this interaction to a higher level of abstraction by enabling the development of aggregate Web services. However, the autonomous and distributed nature of the partner services in an aggregate Web service present unique challenges to the reliability of the composite services. In this paper, we present an approach where existing BPEL processes are automatically instrumented, so that when one or more of their partner services do not provide satisfactory service (e.g., because of a service being overwhelmed, crashed, or because of a network outage), the request for service is redirected to a proxy Web service, where the failed or slow services are replaced by substitute services. 2008 Academy Publisher.",,
Sadjadi,"Badia R., Dasgupta G., Ezenwoye O., Fong L., Ho H., Khuri S., Liu Y., Luis S., Praino A., Prost J.-P., Radwan A., Sadjadi S.M., Shivaji S., Viswanathan B., Welsh P., Younis A.","The Latin American Grid (LA Grid) joint research program fosters collaborative research across eleven universities and IBM Research with the objective of developing innovative grid technologies and applying them to solve challenging problems in the application areas of bioinformatics and hurricane mitigation. This paper describes some of these innovative technologies, such as the support for transparent to the application expert grid enablement, meta-scheduling, job flows, data integration, and custom visualization, and shows how these technologies will be leveraged in the LA Grid infrastructure to provide solutions to pharmagenomics problems and hurricane prediction ensemble simulations. 2008 The authors and IOS Press.",,
Sadjadi,"Ezenwoye O., Sadjadi S.M.","TRAP/BPEL is a framework that adds autonomic behavior into existing BPEL processes automatically and transparently. We define an autonomic BPEL process as a composite Web service that is capable of responding to changes in its execution environment (e.g., a failure in a partner Web service). Unlike other approaches, TRAP/BPEL does not require any manual modifications to the original code of the BPEL processes and there is no need to extend the BPEL language nor its BPEL engine. In this paper, we describe the details of the TRAP/BPEL framework and use a case study to demonstrate the feasibility and effectiveness of our approach.",,
Sadjadi,"Sadjadi S.M., Martinez J., Soldo T., Atencio L., Badia R.M., Ejarque J.","High performance computing (HPC) is gaining popularity in solving scientific applications. Using the current programming standards, however, it takes an HPC expert to efficiently take advantage of HPC facilities; a skill that a scientist does not necessarily have. This lack of separation of concerns has resulted in scientific applications with rigid code, which entangles non-functional concerns (i.e., the parallel code) into functional concerns (i.e., the core business logic). Effectively, this tangled code hinders the maintenance and evolution of these applications. In this paper, we introduce Transparent Grid Enabler (TGE) that separates the task of developing the business logic of a scientific application from the task of improving its performance. TGE achieves this goal by integrating two existing software tools, namely, TRAP/J and GRID superscalar. A simple matrix multiplication program is used as a case study to demonstrate the current use and capabilities of TGE. Copyright (2007) by Knowledge Systems Institute (KSI).",,
Sadjadi,"Samimi F.A., McKinley P.K., Sadjadi S.M., Tang C., Shapiro J.K., Zhou Z.","This paper describes Service Clouds, a distributed infrastructure designed to facilitate rapid prototyping and deployment of adaptive communication services. The infrastructure combines adaptive middleware functionality with an overlay network substrate in order to support dynamic instantiation and reconfiguration of services. The Service Clouds architecture includes a collection of low-level facilities that can be invoked directly by applications or used to compose more complex services. After describing the Service Clouds architecture, we present results of experimental case studies conducted on the PlanetLab Internet testbed alone and a mobile computing testbed. 2007 IEEE.",,
Sadjadi,"Ezenwoye O., Sadjadi S.M.","Web services paradigm is allowing applications to interact with one another over the Internet. BPEL facilitates this interaction by providing a platform through which Web services can be integrated. However, the autonomous and distributed nature of the integrated services presents unique challenges to the reliability of composed services. The focus of our ongoing research is to transparently introduce autonomic behavior to BPEL processes in order to make them more resilient to the failure of partner services. In this work, we present an approach where BPEL processes are adapted by redirecting their interactions with partner services to a dynamic proxy. We describe the generative adaptation process and the architecture of the adaptive BPEL processes and their corresponding proxies. Finally, we use case studies to demonstrate how generated dynamic proxies are used to support self-healing and self-optimization in instrumented BPEL processes. 2007 IEEE.",,
Sadjadi,"Ezenwoye O., Sadjadi S.M.","Web services are increasingly being used to expose applications over the Internet. To promote efficiency and the reuse of software, these Web services are being integrated both within enterprises and across enterprises, creating higher function services. BPEL is a workflow language that can be used to facilitate this integration. Unfortunately, the autonomous nature of Web services leaves BPEL processes susceptible to the failures of their constituent services. In this paper, we present a systematic approach to making existing BPEL processes more fault tolerant by monitoring the involved Web services at runtime, and by replacing delinquent Web services. To show the feasibility of our approach, we developed a prototype implementation that generates more robust BPEL processes from existing ones automatically. The use of the prototype is demonstrated using an existing Loan Approval BPEL process.",,
Sadjadi,"Ezenwoye O., Sadjadi S.M.","Web services are increasingly being used to expose applications over the Internet. These Web services are being integrated within and across enterprises to create higher function services. BPEL is a workflow language that facilitates this integration. Although both academia and industry acknowledge the need for workflow languages, there are few technical papers focused on BPEL. In this paper, we provide an overview of BPEL and discuss its promises, limitations and challenges. Copyright 2006 ACM.",,
Sadjadi,"Sadjadi S.M., McKinley P.K., Kasten E.P., Zhou Z.","This paper describes the internal architecture and operation of an adaptable communication component called the MetaSocket. MetaSockets are created using Adaptive Java, a reflective extension to Java that enables a component's internal architecture and behavior to be adapted at runtime in response to external stimuli. This paper describes how adaptive behavior is implemented in MetaSockets, as well as how MetaSockets interact with other adaptive components, such as decision makers and event mediators. Results of experiments on a mobile computing testbed demonstrate how MetaSockets respond to dynamic wireless channel conditions in order to improve the quality of interactive audio streams delivered to iPAQ handheld computers. Copyright 2006 John Wiley & Sons, Ltd.",,
Sadjadi,"Samimi F.A., McKinley P.K., Sadjadi S.M.","We recently introduced Service Clouds, a distributed infrastructure designed to facilitate rapid prototyping and deployment of autonomic communication services. In this paper, we propose a model that extends Service Clouds to the wireless edge of the Internet. This model, called Mobile Service Clouds, enables dynamic instantiation, composition, configuration, and reconfiguration of services on an overlay network to support mobile computing. We have implemented a prototype of this model and applied it to the problem of dynamically instantiating and migrating proxy services for mobile hosts. We conducted a case study involving data streaming across a combination of PlanetLab nodes, local proxies, and wireless hosts. Results are presented demonstrating the effectiveness of the prototype in establishing new proxies and migrating their functionality in response to node failures. Springer-Verlag Berlin Heidelberg 2006.",,
Sadjadi,"Sadjadi S.M., McKinley P.K.","Increasingly, software systems are constructed by composing multiple existing applications. The resulting complexity increases the need for self-management of the system. However, adding autonomic behavior to composite systems is difficult, especially when the existing components were not originally designed to support such interactions. Moreover, entangling the code for integrated self-management with the code for the business logic of the original applications may actually increase the complexity of the system, counter to the desired goal. In this paper, we propose a technique to enable self-managing behavior to be added to composite systems transparently, that is, without requiring manual modifications to the existing code. The technique uses transparent shaping, developed previously to enable dynamic adaptation in existing programs, to weave self-managing behavior into existing applications, which interact through Web services. A case study demonstrates the use of this technique to construct a fault-tolerant surveillance application from two existing applications, one developed in .NET and the other in CORBA, without the need to modify the source code of the original applications.",,
Sadjadi,"Siddique S., Ege R.K., Sadjadi S.M.","Our paper describes a SIP and RTP based software component which is platform independent and is adaptable to various network infrastructures. The software component, named ""X-Communicator"" is capable of multimedia communications including audio, video, text (instant messaging), secure file transfer and desktop streaming with the last three features currently being implemented. The goal is to provide a complete remoteassistance solution and to make the system capable of adjusting to dynamic environment changes. X-Communicator features NAT handling capability and uses intelligent network detection components to identify network infrastructure. For the multimedia RTP data stream transmission it utilizes P2P streaming. The system uses SIP for transferring various control information and implements security. X-Communicator is very user-friendly and can interoperate with other SIP based standard user agents (soft or hard phones) and adjusts its communication capability accordingly. The paper proposes different possible network environments and the software's capability. X-Communicator is currently implemented as a functional prototype and is undergoing further development. 2005 IEEE.",,
Sadjadi,"Samimi F.A., McKinley P.K., Sadjadi S.M., Ge P.","In pervasive computing environments, conditions are highly variable and resources are limited. In order to meet the needs of applications, systems must adapt dynamically to changing situations. Since adaptation at one system layer may be insufficient, cross-layer, or vertical approaches to adaptation may be needed. Moreover, adaptation in distributed systems often requires horizontal co-operation among hosts. This cooperation is not restricted to the source and destination(s) of a data stream, but might also include intermediate hosts in an overlay network or mobile ad hoc network. We refer to this combined capability as universal adaptation. We contend that the model defining interaction between adaptive middleware and the operating system is critical to realizing universal adaptation. We explore this hypothesis by evaluating the Kernel-Middleware eXchange (KMX), a specific model for cross-layer, cross-system adaptation. We present the KMX architecture and discuss its potential role in supporting universal adaptation in pervasive computing environments. We then describe a prototype implementation of KMX and show results of an experimental case study in which KMX is used to improve the quality of video streaming to mobile nodes in a hybrid wired-wireless network. Copyright 2004 ACM.",,
Sadjadi,"Zhou Z., McKinley P.K., Sadjadi S.M.","This paper addresses the energy consumption of forward error correction (FEC) protocols as used to improve quality-of-service (QoS) for wireless computing devices. The paper also characterizes the effect on energy consumption and QoS of the power saving mode in 802.11 wireless local area networks (WLANs). Experiments are described in which FEC-encoded audio streams are multicast to mobile computers across a WLAN. Results of these experiments quantify the trade-offs between improved QoS, due to FEC, and additional energy consumption caused by receiving and decoding redundant packets. Two different approaches to FEC are compared relative to these metrics. The results of this study enable the development of adaptive software mechanisms that attempt to manage these tradeoffs in the presence of highly dynamic wireless environments.",,
Sadjadi,"Sadjadi S.M., McKinley P.K.","This paper addresses the design of adaptive middleware to support autonomic computing in pervasive computing environments. The particular problem we address here is how to support self-optimization to changing network connection capabilities as a mobile user interacts with heterogeneous elements in a wireless network infrastructure. The goal is to enable self-optimization to such changes transparently with respect to the core application code. We propose a solution based on the use of the generic proxy, which is a specific CORBA object that can intercept and process any CORBA request using rules and actions that can be introduced to the knowledge base of the proxy during execution. To explore its design and operation, we have incorporated the generic proxy into ACT, an adaptive middleware framework we designed previously to support adaptation in CORBA applications. Details of the generic proxy are presented. A case study is described in which a generic proxy is used to support self-optimization in an existing image retrieval application, when executed in a heterogeneous wireless environment.",,
Sadjadi,"Sadjadi S.M., McKinley P.K., Stirewalt R.E.K., Cheng B.H.C.","The application of transparent reflective aspect programming (TRAP) for Java programs to a multicast audio application was discussed. The audio-streaming application (ASA) was designed to deliver interactive audio from a microphone at one network node to multiple receiving nodes. The TRAP/J was made adaptable to wireless environment. Experiments on a mobile computing testbed demonstrated the utility of the TRAP/J to transparently and automatically enhance an existing application with new adaptive behavior, thus enabling the ASA to better deal with highly variable conditions in wireless networks.",,
Sadjadi,"McKinley P.K., Sadjadi S.M., Kasten E.P., Cheng B.H.C.","Compositional adaptation is one of the two general approaches to implementing software adaptation. This approach exchanges algorithmic or structural system components with others that improve a program's fit to its current environment. With compositional adaptation, an application can adopt new algorithms for addressing concerns that were unforeseen during development. This flexibility not only supports tuning of program variables or strategy selection but also allows dynamic recomposition of the software during execution.",,
Sadjadi,"Sadjadi S.M., McKinley P.K.","This paper proposes an Adaptive CORBA Template (ACT), which enables run-time improvements to CORBA applications in response to unanticipated changes in either their functional requirements or their execution environments. ACT enhances CORBA applications by transparently weaving adaptive code into their object request brokers (ORBs) at run time. The woven code intercepts and adapts the requests, replies, and exceptions that pass through the ORBs. Specifically, ACT can be used to develop an object-oriented framework in any language that supports dynamic loading of code and can be applied to any CORBA ORB that supports portable interceptors. Moreover, ACT can be used to support interoperation among otherwise incompatible adaptive CORBA frameworks. To evaluate the performance and functionality of ACT, we implemented a prototype in Java. Our experimental results show that the overhead introduced by the ACT infrastructure is negligible, while the adaptations offered are highly flexible.",,
Sadjadi,"McKinley P.K., Padmanabhan U.I., Ancha N., Sadjadi S.M.","This paper describes the design and operation of a composable proxy infrastructure that enables mobile Internet users to collaborate via heterogeneous devices and network connections. The approach is based on detachable Java I/O streams, which enable proxy filters and transcoders to be dynamically inserted, removed, and reordered on a given data stream. Unlike conventional Java I/O streams, detachable streams can be stopped, disconnected, reconnected, and restarted. As such, they provide a convenient method by which to support the dynamic composition of proxy services. Moreover, use of the I/O stream abstraction enables network distribution and stream adaptability to be implemented transparently with respect to application components. The operation and implementation of detachable streams are described. To evaluate the composable proxy infrastructure, it is used to enhance interactive audio communication among users of a Web-based collaborative computing framework. Two forward error correction (FEC) proxylets are developed, one using block erasure codes and the other using the GSM 06.10 encoding algorithm. Separately, each type of FEC improves the ability of the audio stream to tolerate errors in a wireless LAN environment. When composed in a single proxy, however, they cooperate to correct additional types of burst errors. Results are presented from a performance study conducted on a mobile computing testbed.",,
Sadjadi,"Sadjadi S.M., McKinley P.K., Kasten E.P.","This paper describes the internal architecture and operation of an adaptable communication component called the MetaSocket. MetaSockets are created using Adaptive Java, a reflective extension to Java that enables a component's internal architecture and behavior to be adapted at run time in response to external stimuli. This paper describes how adaptive behavior is implemented in MetaSockets, as well as how MetaSockets interact with other adaptive components, such as decision makers and event mediators. Results of experiments on a mobile computing testbed demonstrate how MetaSockets respond to dynamic wireless channel conditions in order to improve the quality of interactive audio streams delivered to iPAQ handheld computers. 2003 IEEE.",,
Sadjadi,"Yang Z., Cheng B.H.C., Stirewalt R.E.K., Sowell J., Sadjadi S.M., McKinley P.K.","This paper presents an aspect-oriented approach to dynamic adaptation. A systematic process for defining where, when, and how an adaptation is to be incorporated into an application is presented. Specifically, the paper presents a two-phase approach to dynamic adaptation, where the first phase prepares a non-adaptive program for adaptation, and the second phase implements the adaptation at run time. This approach is illustrated with a distributed conferencing application.",,
Sadjadi,"Kasten E.P., McKinley P.K., Sadjadi S.M., Stirewalt R.E.K.","Many middleware platforms use computational reflection to support adaptive functionality. Most approaches intertwine the activity of observing behavior (introspection) with the activity of changing behavior (intercession). This paper explores the use of language constructs to separate these parts of reflective functionality. This separation and ""packaging"" of reflective primitives is intended to facilitate the design of correct and consistent adaptive middleware. A prototype language, called Adaptive Java, is described in which this functionality is realized through extensions to the Java programming language. A case study is described in which ""metamorphic"" socket components are created from regular socket classes and used to realize adaptive behavior on wireless network connections. 2002 IEEE.",,
Sadjadi,"McKinley P.K., Sadjadi S.M., Kasten E.P., Kalaskar R.","Software on wearable computers must adapt to dynamic environmental conditions, such as changes in packet loss behavior on wireless communication channels. This paper investigates the use of programming language constructs to realize adaptive behavior. A prototype language, Adaptive Java, was used to construct MetaSocket components, whose composition and behavior can be adapted to changing conditions during execution. MetaSockets were then integrated into Pavilion, a web-based collaboration framework, and experiments were conducted on a mobile computing testbed containing Xybernaut MA-V wearable computer systems. Performance results demonstrate the utility of MetaSockets in reducing file transfer time and enhancing the quality of interactive audio streams. 2002 IEEE.",,
Sadjadi,"Rangaswami R., Masoud Sadjadi S., Prabakar N., Deng Y.","Multimedia communication services today are conceived, designed, and developed in isolation, following a stovepipe approach This has resulted in a fragmented and incompatible set of technologies and products. Building new communication services requires a lengthy and costly development cycle, which severely limits the pace of innovation. In this paper, we address the fundamental problem of automating the development of multimedia communication services, We propose a new paradigm for creating such services through declarative specification and generation, rather than through traditional design and development. Further, the proposed paradigm pays special attention to how the end-user specifies his/her communication needs, an important requirement largely ignored in existing approaches. 2007 IEEE.",,
Sadjadi,"Alvarez H.L., Chatfield D., Cox D.A., Crumpler E., D'Cunha C., Gutierrez R., Ibarra J., Johnson E., Kumar K., Milledge T., Narasimhan G., Masoud Sadjadi S., Chi Z.","The ""CyberBridges"" pilot project is an innovative model for creating a new generation of scientists and engineers who are capable of fully integrating cyberinfrastructure into the whole educational, professional, and creative process of their respective disciplines. CyberBridges augments graduate student education to include a foundation of understanding in Advanced Networking and Grid Infrastructure for High Performance Computing, and bridges the divide between the information technology community and diverse science and engineering disciplines. We demonstrate the effectiveness of CyberBridges by providing four case studies. Groundwork has begun to extend the outreach of CyberBridges for international research and education collaborations. 2007 IEEE.",,
Sadjadi,"Masoud Sadjadi S., McKinley P.K., Cheng B.H.C.","The need for adaptability in software is growing, driven in part by the emergence of pervasive and autonomic computing. In many cases, it is desirable to enhance existing programs with adaptive behavior, enabling them to execute effectively in dynamic environments. In this paper, we propose a general programming model called transparent shaping to enable dynamic adaptation in existing programs. We describe an approach to implementing transparent shaping that combines four key software development techniques: aspect-oriented programming to realize separation of concerns at development time, behavioral reflection to support software reconfiguration at run time, component-based design to facilitate independent development and deployment of adaptive code, and adaptive middleware to encapsulate the adaptive functionality. After presenting the general model, we discuss two specific realizations of transparent shaping that we have developed and used to create adaptable applications from existing programs. Copyright 2005 ACM.",,
Sadjadi,"Masoud Sadjadi S., McKinley P.K., Cheng B.H.C., Kurt Stirewalt R.E.","This paper describes TRAP/J, a software tool that enables new adaptable behavior to be added to existing Java applications transparently (that is, without modifying the application source code and without extending the JVM). The generation process combines behavioral reflection and aspect-oriented programming to achieve this goal. Specifically, TRAP/J enables the developer to select, at compile time, a subset of classes in the existing program that are to be adaptable at run time. TRAP/J then generates specific aspects and reflective classes associated with the selected classes, producing an adapt-ready program. As the program executes, new behavior can be introduced via interfaces to the adaptable classes. A case study is presented in which TRAP/J is used to introduce adaptive behavior to an existing audio-streaming application, enabling it to operate effectively in a lossy wireless network by detecting and responding to changing network conditions. Springer-Verlag 2004.",,
Saeed,"Awan M.G., Saeed F.","Mass Spectrometry (MS)-based proteomics has become an essential tool in the study of proteins. With the advent of modern MS machines huge amounts of data is being generated, which can only be processed by novel algorithmic tools. However, in the absence of data benchmarks and ground truth datasets algorithmic integrity testing and reproducibility is a challenging problem. To this end, MaSS-Simulator has been presented, which is an easy to use simulator and can be configured to simulate MS/MS datasets for a wide variety of conditions with known ground truths. MaSS-Simulator offers many configuration options to allow the user a great degree of control over the test datasets, which can enable rigorous and large- scale testing of any proteomics algorithm. MaSS-Simulator is assessed by comparing its performance against experimentally generated spectra and spectra obtained from NIST collections of spectral library. The results show that MaSS-Simulator generated spectra match closely with real-spectra and have a relative-error distribution centered around 25%. In contrast, the theoretical spectra for same peptides have relative-error distribution centered around 150%. MaSS-Simulator will enable developers to specifically highlight the capabilities of their algorithms and provide a strong proof of any pitfalls they might face. Source code, executables, and a user manual for MaSS-Simulator can be downloaded from https://github.com/pcdslab/MaSS-Simulator. 2018 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim",,
Saeed,"Awan M.G., Eslami T., Saeed F.","In the age of ever increasing data, faster and more efficient data processing algorithms are needed. Graphics Processing Units (GPU) are emerging as a cost-effective alternative architecture for high-end computing. The optimal design of GPU algorithms is a challenging task which requires thorough understanding of the high performance computing architecture as well as the algorithmic design. The steep learning curve needed for effective GPU-centric algorithm design and implementation requires considerable expertise, time, and resources. In this paper, we present GPU-DAEMON, a GPU Data Management, Algorithm Design and Optimization technique suitable for processing array based big omics data. Our proposed GPU algorithm design template outlines and provides generic methods to tackle critical bottlenecks which can be followed to implement high performance, scalable GPU algorithms for given big data problem. We study the capability of GPU-DAEMON by reviewing the implementation of GPU-DAEMON based algorithms for three different big data problems. Speed up of as large as 386x (over the sequential version) and 50x (over naive GPU design methods) are observed using the proposed GPU-DAEMON. GPU-DAEMON template is available at https://github.com/pcdslab/GPU-DAEMON and the source codes for GPU-ArraySort, G-MSR and GPU-PCC are available at https://github.com/pcdslab. 2018 Elsevier Ltd",,
Saeed,"Eslami T., Saeed F.","Functional magnetic resonance imaging (fMRI) is a non-invasive brain imaging technique, which has been regularly used for studying brain’s functional activities in the past few years. A very well-used measure for capturing functional associations in brain is Pearson’s correlation coefficient. Pearson’s correlation is widely used for constructing functional network and studying dynamic functional connectivity of the brain. These are useful measures for understanding the effects of brain disorders on connectivities among brain regions. The fMRI scanners produce huge number of voxels and using traditional central processing unit (CPU)-based techniques for computing pairwise correlations is very time consuming especially when large number of subjects are being studied. In this paper, we propose a graphics processing unit (GPU)-based algorithm called Fast-GPU-PCC for computing pairwise Pearson’s correlation coefficient. Based on the symmetric property of Pearson’s correlation, this approach returns N(N - 1)/2 correlation coefficients located at strictly upper triangle part of the correlation matrix. Storing correlations in a one-dimensional array with the order as proposed in this paper is useful for further usage. Our experiments on real and synthetic fMRI data for different number of voxels and varying length of time series show that the proposed approach outperformed state of the art GPU-based techniques as well as the sequential CPU-based versions. We show that Fast-GPU-PCC runs 62 times faster than CPU-based version and about 2 to 3 times faster than two other state of the art GPU-based methods. 2018 by the authors. Licensee MDPI, Basel, Switzerland.",,
Saeed,"Eslami T., Saeed F.","Attention deficit hyperactivity disorder (ADHD) is one of the most common brain disorders among children. This disorder is considered as a big threat for public health and causes attention, focus and organizing difficulties for children and even adults. Since the cause of ADHD is not known yet, data mining algorithms are being used to help discover patterns which discriminate healthy from ADHD subjects. Numerous efforts are underway with the goal of developing classification tools for ADHD diagnosis based on functional and structural magnetic resonance imaging data of the brain. In this paper, we used Eros, which is a technique for computing similarity between two multivariate time series along with k-Nearest-Neighbor classifier, to classify healthy vs ADHD children. We designed a model selection scheme called J-Eros which is able to pick the optimum value of k for k-Nearest-Neighbor from the training data.We applied this technique to the public data provided by ADHD-200 Consortium competition and our results show that J-Eros is capable of discriminating healthy from ADHD children such that we outperformed the best results reported by ADHD-200 competition about 20 percent for two datasets. The implemented code is available as GPL license on GitHub portal of our lab at https://github.com/pcdslab/J-Eros. 2018 Association for Computing Machinery.",,
Saeed,"Tariq U., Cheema U.I., Saeed F.","Energy efficiency is a crucial problem in data centers where big data is generally represented by directed or undirected graphs. Analysis of this big data graph is challenging due to volume and velocity of the data as well as irregular memory access patterns. Graph sampling is one of the most effective ways to reduce the size of graph while maintaining crucial characteristics. In this paper we present design and implementation of an FPGA based graph sampling method which is both time- and energy-efficient. This is in contrast to existing parallel approaches which include memory-distributed clusters, multicore and GPUs. Our strategy utilizes a novel graph data structure, that we call COPRA which allows time- and memory-efficient representation of graphs suitable for reconfigurable hardware such as FPGAs. Our experiments show that our proposed techniques are 2x faster and 3x more energy efficient as compared to serial CPU version of the algorithm. We further show that our proposed techniques give comparable speedups to GPU and multi-threaded CPU architecture while energy consumption is 10x less than GPU and 2x less than CPU. 2017 IEEE.",,
Saeed,"Pérez S.V., Saeed F.","It is now possible to compress and decompress large-scale Next-Generation Sequencing files taking advantage of high-performance computing techniques. To this end, we have recently introduced a scalable hybrid parallel algorithm, called phyNGSC, which allows fast compression as well as decompression of big FASTQ datasets using distributed and shared memory programming models via MPI and OpenMP. In this paper we present the design and implementation of a novel parallel data structure which lessens the dependency on decompression and facilitates the handling of DNA sequences in their compressed state using fine-grained decompression in a technique that is identified as in compresso data processing. Using our data structure compression and decompression throughputs of up to 8.71 GB/s and 10.12 GB/s were observed. Our proposed structure and methodology brings us one step closer to compressive genomics and sublinear analysis of big NGS datasets. 2017 IEEE.",,
Saeed,"Vargas-Perez S., Saeed F.","DNA sequencing has moved into the realm of Big Data due to the rapid development of high-throughput, low cost Next-Generation Sequencing (NGS) technologies. Sequential data compression solutions that once were sufficient to efficiently store and distribute this information are now falling behind. In this paper we introduce phyNGSC, a hybrid MPI-OpenMP strategy to speedup the compression of big NGS data by combining the features of both distributed and shared memory architectures. Our algorithm balances work-load among processes and threads, alleviates memory latency by exploiting locality, and accelerates I/O by reducing excessive read/write operations and inter-node message exchange. To make the algorithm scalable, we introduce a novel timestamp-based file structure that allows us to write the compressed data in a distributed and non-deterministic fashion while retaining the capability of reconstructing the dataset with its original order. Our experimental results show that phyNGSC achieved compression times for big NGS datasets that were 45 to 98 percent faster than NGS-specific sequential compressors with throughputs of up to 3 GB/s. Our theoretical analysis and experimental results suggest strong scalability with some datasets yielding super-linear speedups and constant efficiency. We were able to compress 1 terabyte of data in under 8 minutes compared to more than 5 hours taken by NGS-specific compression algorithms running sequentially. Compared to other parallel solutions, phyNGSC achieved up to 6x speedups while maintaining a higher compression ratio. The code for this implementation is available at https://github.com/pcdslab/PHYNGSC. 1990-2012 IEEE.",,
Saeed,"Awan M.G., Saeed F.","Modern high resolution Mass Spectrometry instruments can generate millions of spectra in a single systems biology experiment. Each spectrum consists of thousands of peaks but only a small number of peaks actively contribute to deduction of peptides. Therefore, pre-processing of MS data to detect noisy and non-useful peaks are an active area of research. Most of the sequential noise reducing algorithms are impractical to use as a pre-processing step due to high time-complexity. In this paper, we present a GPU based dimensionality-reduction algorithm, called G-MSR, for MS2 spectra. Our proposed algorithm uses novel data structures which optimize the memory and computational operations inside GPU. These novel data structures include Binary Spectra and Quantized Indexed Spectra (QIS). The former helps in communicating essential information between CPU and GPU using minimum amount of data while latter enables us to store and process complex 3-D data structure into a 1-D array structure while maintaining the integrity of MS data. Our proposed algorithm also takes into account the limited memory of GPUS and switches between in-core and out-of-core modes based upon the size of input data. G-MSR achieves a peak speed-up of 386x over its sequential counterpart and is shown to process over a million spectra in just 32 seconds. The code for this algorithm is available as a GPL open-source at GitHub at the following link: https://github.com/pcdslab/G-MSR. 2017 Association for Computing Machinery.",,
Saeed,"Eslami T., Awan M.G., Saeed F.","Functional Magnetic Resonance Imaging (fMRI) is a non-invasive brain imaging technique for studying the brain's functional activities. Pearson's Correlation Coefficient is an important measure for capturing dynamic behaviors and functional connectivity between brain components. One bottleneck in computing Correlation Coefficients is the time it takes to process big fMRI data. In this paper, we propose GPU-PCC, a GPU based algorithm based on vector dot product, which is able to compute pairwise Pearson's Correlation Coefficients while performing computation once for each pair. Our method is able to compute Correlation Coefficients in an ordered fashion without the need to do post-processing reordering of coefficients. We evaluated GPU-PCC using synthetic and real fMRI data and compared it with sequential version of computing Correlation Coefficient on CPU and existing state-of-the-art GPU method. We show that our GPU-PCC runs 94.62_ faster as compared to the CPU version and 4.28_ faster than the existing GPU based technique on a real fMRI dataset of size 90k voxels. The implemented code is available as GPL license on GitHub portal of our lab at https://github.com/pcdslab/GPU-PCC. 2017 Association for Computing Machinery.",,
Saeed,"Aledhari M., Marhoon A., Hamad A., Saeed F.","The revolution of smart devices has a significant and positive impact on the lives of many people, especially in regard to elements of healthcare. In part, this revolution is attributed to technological advances that enable individuals to wear and use medical devices to monitor their health activities, but remotely. Also, these smart, wearable medical devices assist health care providers in monitoring their patients remotely, thereby enabling physicians to respond quickly in the event of emergencies. An ancillary advantage is that health care costs will be reduced, another benefit that, when paired with prompt medical treatment, indicates significant advances in the contemporary management of health care. However, the competition among manufacturers of these medical devices creates a complexity of small and smart wearable devices such as ECG and EMG. This complexity results in other issues such as patient security, privacy, confidentiality, and identity theft. In this paper, we discuss the design and implementation of a hybrid real-time cryptography algorithm to secure lightweight wearable medical devices. The proposed system is based on an emerging innovative technology between the genomic encryptions and the deterministic chaos method to provide a quick and secure cryptography algorithm for real-time health monitoring that permits for threats to patient confidentiality to be addressed. The proposed algorithm also considers the limitations of memory and size of the wearable health devices. The experimental results and the encryption analysis indicate that the proposed algorithm provides a high level of security for the remote health monitoring system. 2017 IEEE.",,
Saeed,"Sandoval P.C., Claxton J.S., Lee J.W., Saeed F., Hoffert J.D., Knepper M.A.","Vasopressin-mediated regulation of renal water excretion is defective in a variety of water balance disorders in humans. It occurs in part through long-term mechanisms that regulate the abundance of the aquaporin-2 water channel in renal collecting duct cells. Here, we use deep DNA sequencing in mouse collecting duct cells to ask whether vasopressin signaling selectively increases Aqp2 gene transcription or whether it triggers a broadly targeted transcriptional network. ChIP-Seq quantification of binding sites for RNA polymerase II was combined with RNA-Seq quantification of transcript abundances to identify genes whose transcription is regulated by vasopressin. (View curated dataset at https://helixweb.nih.gov/ESBL/Database/Vasopressin/). The analysis revealed only 35 vasopressin-regulated genes (of 3659) including Aqp2. Increases in RNA polymerase II binding and mRNA abundances for Aqp2 far outstripped corresponding measurements for all other genes, consistent with the conclusion that vasopressin-mediated transcriptional regulation is highly selective for Aqp2. Despite the overall selectivity of the net transcriptional response, vasopressin treatment was associated with increased RNA polymerase II binding to the promoter proximal region of a majority of expressed genes, suggesting a nearly global positive regulation of transcriptional initiation with transcriptional pausing. Thus, the overall net selectivity appears to be a result of selective control of transcriptional elongation. 2016 The Author(s).",,
Saeed,"Awan M.G., Saeed F.","Modern day analytics deals with big datasets from diverse fields. For many application the data is in the form of an array which consists of large number of smaller arrays. Existing techniques focus on sorting a single large array and cannot be used for sorting large number of smaller arrays in an efficient manner. Currently no such algorithm is available which can sort such large number of arrays utilizing the massively parallel architecture of GPU devices. In this paper we present a highly scalable parallel algorithm, called GPU-ArraySort, for sorting large number of arrays using a GPU. Our algorithm performs in-place operations and makes minimum use of any temporary run-time memory. Our results indicate that we can sort up to 2 million arrays having 1000 elements each, within few seconds. We compare our results with the unorthodox tagged array sorting technique based on NVIDIAs Thrust library. GPU-ArraySort out-performs the tagged array sorting technique by sorting three times more data in a much smaller time. The developed tool and strategy will be made available at https://github.com/pcdslab/. 2016 IEEE.",,
Saeed,"Maabreh M., Gupta A., Saeed F.","Target-Decoy database is a common dependable strategy used in Peptide-Spectrum-Matching (PSM) for quality assessment. In this, a set of decoy labeled peptides are injected to the database and indexed along with real peptides. Crux Tide is a fast search engine that supports indexing peptides' database and decoys generation. In Tide, indexing FASTA files and generating decoys is a computationally expensive process. In this paper we first analyze the serial Tide indexer and decoy generator algorithm and, then describe a parallel shared memory solution. Our proposed technique utilizes a clever hashing technique to localize the process, and breaks up the processing dependency among threads. The developed parallel versions are able to reduce the computational complexity by approximately 50% and 25% of the sequential time using 4 and 8 threads, respectively. Moreover, our proposed solution could provide high-performance techniques of peptides' databases partitioning where each parallel unit could search and rank locally and independently. 2016 IEEE.",,
Saeed,"Awan M.G., Saeed F.","Motivation: Modern proteomics studies utilize high-throughput mass spectrometers which can produce data at an astonishing rate. These big mass spectrometry (MS) datasets can easily reach peta-scale level creating storage and analytic problems for large-scale systems biology studies. Each spectrum consists of thousands of peaks which have to be processed to deduce the peptide. However, only a small percentage of peaks in a spectrum are useful for peptide deduction as most of the peaks are either noise or not useful for a given spectrum. This redundant processing of non-useful peaks is a bottleneck for streaming high-throughput processing of big MS data. One way to reduce the amount of computation required in a high-throughput environment is to eliminate non-useful peaks. Existing noise removing algorithms are limited in their data-reduction capability and are compute intensive making them unsuitable for big data and high-throughput environments. In this paper we introduce a novel low-complexity technique based on classification, quantization and sampling of MS peaks. Results: We present a novel data-reductive strategy for analysis of Big MS data. Our algorithm, called MS-REDUCE, is capable of eliminating noisy peaks as well as peaks that do not contribute to peptide deduction before any peptide deduction is attempted. Our experiments have shown up to 100_ speed up over existing state of the art noise elimination algorithms while maintaining comparable high quality matches. Using our approach we were able to process a million spectra in just under an hour on a moderate server. Availability and implementation: The developed tool and strategy has been made available to wider proteomics and parallel computing community and the code can be found at https://github.com/pcdslab/MSREDUCE. 2016 The Author 2016. Published by Oxford University Press. All rights reserved.",,
Saeed,Saeed F.,"Proteogenomics is an emerging field of systems biology research at the intersection of proteomics and genomics. Two high-throughput technologies, Mass Spectrometry (MS) for proteomics and Next Generation Sequencing (NGS) machines for genomics are required to conduct proteogenomics studies. Independently both MS and NGS technologies are inflicted with data deluge which creates problems of storage, transfer, analysis and visualization. Integrating these big data sets (NGS+MS) for proteogenomics studies compounds all of the associated computational problems. Existing sequential algorithms for these proteogenomics datasets analysis are inadequate for big data and high performance computing (HPC) solutions are almost non-existent. The purpose of this paper is to introduce the big data problem of proteogenomics and the associated challenges in analyzing, storing and transferring these data sets. Further, opportunities for high performance computing research community are identified and possible future directions are discussed. 2015 IEEE.",,
Saeed,"Hefeida M.S., Saeed F.",We propose a Data Aware Communication Technique (DACT) that reduces energy consumption in Energy HarvestingWireless Sensor Networks (EH-WSN). DACT takes advantage of the data correlation present in household EH-WSN applications to reduce communication overhead. It adapts its functionality according to correlations in data communicated over the EH-WSN and operates independently from spatial and temporal correlations without requiring location information. Our results show that DACT improves communication efficiency of sensor nodes and can help reduce idle energy consumption in an average-size home by up to 90% as compared to spatial/temporal correlation-based communication techniques. IFIP International Federation for Information Processing 2016.,,
Saeed,"Aledhari M., Hefeida M.S., Saeed F.","Modern genomic studies utilize high-throughput instruments which can produce data at an astonishing rate. These big genomic datasets produced using next generation sequencing (NGS) machines can easily reach peta-scale level creating storage, analytic and transmission problems for large-scale system biology studies. Traditional networking protocols are oblivious to the data that is being transmitted and are designed for general purpose data transfer. In this paper we present a novel data-aware network transfer protocol to efficiently transfer big genomic data. Our protocol exploits the limited alphabet of DNA nucleotide and is developed over the hypertext transfer protocol (HTTP) framework. Our results show that proposed technique improves transmission up to 84 times when compared to normal HTTP encoding schemes. We also show that the performance of the resultant protocol (called VTTP) using a single machine is comparable to BitTorrent protocol used on 10 machines. IFIP International Federation for Information Processing 2016.",,
Saeed,"Khositseth S., Uawithya P., Somparn P., Charngkaew K., Thippamom N., Hoffert J.D., Saeed F., Payne D.M., Chen S.-H., Fenton R.A., Pisitkun T.","Hypokalemia (low serum potassium level) is a common electrolyte imbalance that can cause a defect in urinary concentrating ability, i.e., nephrogenic diabetes insipidus (NDI), but the molecular mechanism is unknown. We employed proteomic analysis of inner medullary collecting ducts (IMCD) from rats fed with a potassium-free diet for 1 day. IMCD protein quantification was performed by mass spectrometry using a label-free methodology. A total of 131 proteins, including the water channel AQP2, exhibited significant changes in abundance, most of which were decreased. Bioinformatic analysis revealed that many of the down-regulated proteins were associated with the biological processes of generation of precursor metabolites and energy, actin cytoskeleton organization, and cell-cell adhesion. Targeted LC-MS/MS and immunoblotting studies further confirmed the down regulation of 18 selected proteins. Electron microscopy showed autophagosomes/autophagolysosomes in the IMCD cells of rats deprived of potassium for only 1 day. An increased number of autophagosomes was also confirmed by immunofluorescence, demonstrating co-localization of LC3 and Lamp1 with AQP2 and several other down-regulated proteins in IMCD cells. AQP2 was also detected in autophagosomes in IMCD cells of potassium-deprived rats by immunogold electron microscopy. Thus, enhanced autophagic degradation of proteins, most notably including AQP2, is an early event in hypokalemia-induced NDI.",,
Saeed,"Perez S.V., Saeed F.","The amount of big data from high-throughput Next-Generation Sequencing (NGS) techniques represents various challenges such as storage, analysis and transmission of massive datasets. One solution to storage and transmission of data is compression using specialized compression algorithms. The existing specialized algorithms suffer from poor scalability with increasing size of the datasets and best available solutions can take hours to compress gigabytes of data. Compression and decompression using these techniques for peta-scale data sets is prohibitively expensive in terms of time and energy. In this paper we introduce paraDSRC, a parallel implementation of the DNA Sequence Reads Compression (DSRC) application using a message passing model that presents reduction of the compression time complexity by a factor of O(1/p) (where p is the number of processing units). Our experimental results show that paraDSRC achieves compression times that are 43% to 99% faster than DSRC and compression throughputs of up to 8.4GB/s on a moderate size cluster. For many of the datasets used in our experiments super-linear speedups have been registered making the implementation strongly scalable. We also show that paraDSRC is more than 25.6x faster than comparable parallel compression algorithms. The code is available for free-academic use at https://github.com/PCDS/paraDSRC. 2015 IEEE.",,
Saeed,"Aledhari M., Saeed F.","Genomic data is growing exponentially due to next generation sequencing technologies (NGS) and their ability to produce massive amounts of data in a short time. NGS technologies generate big genomic data that needs to be exchanged between different locations efficiently and reliably. The current network transfer protocols rely on Transmission Control Protocol (TCP) or User Data gram Protocol (UDP) protocols, ignoring data size and type. Universal application layer protocols such as HTTP are designed for wide variety of data types and are not particularly efficient for genomic data. Therefore, we present a new data-Aware transfer protocol for genomic-data that increases network throughput and reduces latency, called Genomic Text Transfer Protocol (GTTP). In this paper, we design and implement a new network transfer protocol for big genomic DNA dataset that relies on the Hypertext Transfer Protocol (HTTP). Modification to content-encoding of HTTP has been done that would transfer big genomic DNA datasets using machine-To-machine (M2M) and client(s)-server topologies. Our results show that our modification to HTTP reduces the transmitted data by 75% of original data and still be able to regenerate the data at the client side for bioinformatics analysis. Consequently, the transfer of data using GTTP is shown to be much faster (about 8 times faster than HTTP) when compared with regular HTTP. 2015 IEEE.",,
Saeed,"Awan M.G., Saeed F.","Mass spectrometry (MS) based proteomics has useful biological applications. Modern mass spectrometers produce millions of spectra from complex biological samples in a short time. Existing solutions for analysis of MS data are based on sequential approaches to analyze these complex data sets. However, the enormous data deluge from mass spectrometers have severely limited the capabilities of these tools. If useful research has to proceed there is a need to develop big data techniques that will allow efficient computation of these heterogeneous noisy MS data sets. In this paper, we show how random sampling of MS data sets can lead to efficient and fast analysis while decreasing the amount of data that needs to be analyzed for each spectrum. For millions of spectra our sampling techniques decreases the number of peaks by a significant number that need to be analyzed for downstream analysis. Copyright ISCA, BICOB 2015.",,
Saeed,"Hoffert J.D., Pisitkun T., Saeed F., Wilson J.L., Knepper M.A.","Satavaptan (SR121463) is a vasopressin V2 receptor antagonist that has been shown to improve hyponatremia in patients with cirrhosis, congestive heart failure, and syndrome of inappropriate antidiuresis. While known to inhibit ad-enylyl cyclase-mediated accumulation of intracellular cyclic AMP and potentially recruit ?-arrestin in kidney cell lines, very little is known regarding the signaling pathways that are affected by this drug. To this end, we carried out a global quantitative phosphoproteomic analysis of native rat inner medullary collecting duct cells pretreated with satavaptan or vehicle control followed by the V2 receptor agonist desmopressin (dDAVP) for 0.5, 2, 5, or 15 min. A total of 2,449 unique phosphopeptides from 1,160 proteins were identified. Phos-phopeptides significantly changed by satavaptan included many of the same kinases [protein kinase A, phosphoinositide 3-kinase, mitogen-activated protein kinase kinase kinase 7 (TAK1), and calcium/cal-modulin-dependent kinase kinase 2] and channels (aquaporin-2 and urea transporter UT-A1) regulated by vasopressin. Time course clustering and kinase motif analysis suggest that satavaptan blocks dDAVP-mediated activation of basophilic kinases, while also blocking dDAVP-mediated inhibition of proline-directed kinases. Satavaptan affects a variety of dDAVP-mediated processes including regulation of cell-cell junctions, actin cytoskeleton dynamics, and signaling through Rho GTPases. These results demonstrate that, overall, satavaptan acts as a selective V2 receptor antagonist and affects many of the same signaling pathways regulated by vasopressin. This study represents the first ""systems-wide"" analysis of a ""vaptan""-class drug and provides a wealth of new data regarding the effects of satavaptan on vasopressin-mediated phosphorylation events.",,
Saeed,"Saeed F., Hoffert J.D., Knepper M.A.","High-throughput mass spectrometers can produce massive amounts of redundant data at an astonishing rate with many of them having poor signal-to-noise (S/N) ratio. These low S/N ratio spectra may not get interpreted using conventional spectra-to-database matching techniques. In this paper, we present an efficient algorithm, CAMS-RS (Clustering Algorithm for Mass Spectra using Restricted Space and Sampling) for clustering of raw mass spectrometry data. CAMS-RS utilizes a novel metric (called F-set) that exploits the temporal and spatial patterns to accurately assess similarity between two given spectra. The F-set similarity metric is independent of the retention time and allows clustering of mass spectrometry data from independent LC-MS/MS runs. A novel restricted search space strategy is devised to limit the comparisons of the number of spectra. An intelligent sampling method is executed on individual bins that allow merging of the results to make the final clusters. Our experiments, using experimentally generated data sets, show that the proposed algorithm is able to cluster spectra with high accuracy and is helpful in interpreting low S/N ratio spectra. The CAMS-RS algorithm is highly scalable with increasing number of spectra and our implementation allows clustering of up to a million spectra within minutes. 2004-2012 IEEE.",,
Saeed,"Sanghi A., Zaringhalam M., Corcoran C.C., Saeed F., Hoffert J.D., Sandoval P., Pisitkun T., Knepper M.A.","Biological information is growing at a rapid pace, making it difficult for individual investigators to be familiar with all information that is relevant to their own research. Computers are beginning to be used to extract and curate biological information; however, the complexity of human language used in research papers continues to be a critical barrier to full automation of knowledge extraction. Here, we report a manually curated knowledge base of vasopressin actions in renal epithelial cells that is designed to be readable either by humans or by computer programs using natural language processing algorithms. The knowledge base consists of three related databases accessible at https://helixweb.nih.gov/ESBL/TinyUrls/Vaso_portal.html. One of the component databases reports vasopressin actions on individual proteins expressed in renal epithelia, including effects on phosphorylation, protein abundances, protein translocation from one subcellular compartment to another, protein-protein binding interactions, etc. The second database reports vasopressin actions on physiological measures in renal epithelia, and the third reports specific mRNA species whose abundances change in response to vasopressin. We illustrate the application of the knowledge base by using it to generate a protein kinase network that connects vasopressin binding in collecting duct cells to physiological effects to regulate the water channel protein aquaporin-2. 2014 the American Physiological Society.",,
Saeed,"Sandoval P.C., Slentz D.H., Pisitkun T., Saeed F., Hoffert J.D., Knepper M.A.","Vasopressin regulates water excretion, in part, by controlling the abundances of the water channel aquaporin-2 (AQP2) protein and regulatory proteins in the renal collecting duct. To determine whether vasopressin-induced alterations in protein abundance result from modulation of protein production, protein degradation, or both, we used protein mass spectrometry with dynamic stable isotope labeling in cell culture to achieve a proteome-wide determination of protein half-lives and relative translation rates in mpkCCD cells. Measurements were made at steady state in the absence or presence of the vasopressin analog, desmopressin (dDAVP). Desmopressin altered the translation rate rather than the stability of most responding proteins, but it significantly increased both the translation rate and the half-life of AQP2. In addition, proteins associated with vasopressin action, including Mal2, Akap12, gelsolin, myosin light chain kinase, annexin-2, and Hsp70, manifested altered translation rates. Interestingly, desmopressin increased the translation of seven glutathione S-transferase proteins and enhanced protein S-glutathionylation, uncovering a previously unexplored vasopressin-induced post-translational modification. Additional bioinformatic analysis of the mpkCCD proteome indicated a correlation between protein function and protein half-life. In particular, processes that are rapidly regulated, such as transcription, endocytosis, cell cycle regulation, and ubiquitylation are associated with proteins with especially short half-lives. These data extend our understanding of the mechanisms underlying vasopressin signaling and provide a broad resource for additional investigation of collecting duct function (http://helixweb.nih.gov/ ESBL/Database/ProteinHalfLives/index.html). Copyright 2013 by the American Society of Nephrology.",,
Saeed,"Saeed F., Hoffert J.D., Knepper M.A.","High-throughput mass spectrometers can produce thousands of peptide spectra from a single complex protein sample in a short amount of time. These data sets contain a substantial amount of redundancy (i.e. the same peptide is selected and identified multiple times in a single experiment) from peptides that may get selected multiple times in the liquid chromatography mass spectrometry (LC-MS/MS) experiment. The data from these mass spectrometers contain a substantial number of spectra that have low signal to noise (S/N) ratio and may not get interpreted due to poor quality. Recently, we presented a graph theoretic algorithm, CAMS (Clustering Algorithm for Mass Spectra) for clustering mass spectrometry data. CAMS utilized a novel metric, called a F-set, that allows accurate identification of the spectra that are similar with much higher accuracy and sensitivity than if single peak comparisons were performed. In this paper we present a multithreaded algorithm, called P-CAMS, for clustering of mass spectral data on multicore machines. The algorithm relies on intelligent matrix completion for graph construction and a load-balancing scheme for substantial speedups. We study the scalability performance of the proposed parallel algorithm on a multicore machine using synthetically generated spectra with parameters carefully chosen to mimic real-world mass spectrometry datasets. Real experimental datasets were also generated for quality assessment of the clustering results from the proposed algorithm. The results show that the proposed algorithms have scalable runtime performances and gives clustering results similar to a serial algorithm. The study also provides insight into the design of high performance algorithms for irregular problems in proteomics on many-core architectures. Copyright 2013 ACM.",,
Saeed,"Saeed F., Pisitkun T., Hoffert J.D., Rashidian S., Wang G., Gucek M., Knepper M.A.","Phosphorylation site assignment of high throughput tandem mass spectrometry (LC-MS/MS) data is one of the most common and critical aspects of phosphoproteomics. Correctly assigning phosphorylated residues helps us understand their biological significance. The design of common search algorithms (such as Sequest, Mascot etc.) do not incorporate site assignment; therefore additional algorithms are essential to assign phosphorylation sites for mass spectrometry data. The main contribution of this study is the design and implementation of a linear time and space dynamic programming strategy for phosphorylation site assignment referred to as PhosSA. The proposed algorithm uses summation of peak intensities associated with theoretical spectra as an objective function. Quality control of the assigned sites is achieved using a post-processing redundancy criteria that indicates the signal-tonoise ratio properties of the fragmented spectra. The quality assessment of the algorithm was determined using experimentally generated data sets using synthetic peptides for which phosphorylation sites were known. We report that PhosSA was able to achieve a high degree of accuracy and sensitivity with all the experimentally generated mass spectrometry data sets. The implemented algorithm is shown to be extremely fast and scalable with increasing number of spectra (we report up to 0.5 million spectra/hour on a moderate workstation). The algorithm is designed to accept results from both Sequest and Mascot search engines. An executable is freely available at http://helixweb.nih.gov/ESBL/PhosSA/ for academic research purposes. 2013 Saeed et al.",,
Saeed,"Saeed F., Khokhar A.A.","The center-star alignment method is one of the few alignment algorithms that has theoretical guarantees of giving a 2-approx solution. The method requires finding a center sequence to generate the alignment. However, finding the center sequence is of the order of O(N2) which is not feasible for large number of sequences. With the advent of fast sequencing techniques (454, Solexa), the number of sequences that have to be aligned and processed is huge and can reach up to one billion sequences. The alignment of these sequences with the reference genome is one of the basic steps in mapping, alignments and sequence analysis. In this paper we present a highly scalable parallel algorithm to find the Center-star sequence and alignments on multiprocessor platforms. Our method relies on ranking and sampling of sequences using k-mer rank. The proposed algorithm is capable of finding the center sequence in a short time with near linear speedups with increasing number of processors. The proposed algorithm has been implemented on a cluster of workstations using MPI library and results to find the Center sequence for up to 6.4 million sequences are presented. Detailed computation and communication complexity analysis of the algorithm is given along with quality assessment for the alignments.",,
Saeed,"Saeed F., Pisitkun T., Knepper M.A., Hoffert J.D.","High-throughput spectrometers are capable of producing data sets containing thousands of spectra for a single biological sample. These data sets contain a substantial amount of redundancy from peptides that may get selected multiple times in a LC-MS/MS experiment. In this paper, we present an efficient algorithm, CAMS (Clustering Algorithm for Mass Spectra) for clustering mass spectrometry data which increases both the sensitivity and confidence of spectral assignment. CAMS utilizes a novel metric, called F-set, that allows accurate identification of the spectra that are similar. A graph theoretic framework is defined that allows the use of F-set metric efficiently for accurate cluster identifications. The accuracy of the algorithm is tested on real HCD and CID data sets with varying amounts of peptides. Our experiments show that the proposed algorithm is able to cluster spectra with very high accuracy in a reasonable amount of time for large spectral data sets. Thus, the algorithm is able to decrease the computational time by compressing the data sets while increasing the throughput of the data by interpreting low S/N spectra. 2012 IEEE.",,
Saeed,"Saeed F., Pisitkun T., Hoffert J.D., Wang G., Gucek M., Knepper M.A.","Phosphorylation site assignment of large-scale data from high throughput tandem mass spectrometry (LC-MS/MS) data is an important aspect of phosphoproteomics. Correct assignment of phosphorylated residue(s) is important for functional interpretation of the data within a biological context. Common search algorithms (Sequest etc.) for mass spectrometry data are not designed for accurate site assignment; thus, additional algorithms are needed. In this paper, we propose a linear-time and linear-space dynamic programming strategy for phosphorylation site assignment. The algorithm, referred to as PhosSA, optimizes the objective function defined as the summation of peak intensities that are associated with theoretical phosphopeptide fragmentation ions. Quality control is achieved through the use of a post-processing criteria whose value is indicative of the signal-to-noise (S/N) properties and redundancy of the fragmentation spectra. The algorithm is tested using experimentally generated data sets of peptides with known phosphorylation sites while varying the fragmentation strategy (CID or HCD) and molar amounts of the peptides. The algorithm is also compatible with various peptide labeling strategies including SILAC and iTRAQ. PhosSA is shown to achieve > 99% accuracy with a high degree of sensitivity. The algorithm is extremely fast and scalable (able to process up to 0.5 million peptides in an hour). The implemented algorithm is freely available at http://helixweb.nih.gov/ESBL/PhosSA/ for academic purposes. 2012 IEEE.",,
Saeed,"Saeed F., Pisitkun T., Hoffert J., Knepper M.","Phosphorylation site assignment of high throughput tandem mass spectrometry (LC-MS/MS) data is one of the most common and critical aspects of phosphoproteomics. Although a number of tools have been proposed for automated site assignments, most of them have been limited in scalability or quality. In this paper, we propose a parallelized version of the PhosSA algorithm (called P-PhosSA) that we recently introduced for site assignment. A domain decomposition strategy is introduced for site assignment and the decomposed data is executed on multiple cores. The algorithm has been parallelized using Java Threads executing on multicore systems. The parallelized algorithm is tested using experimentally generated data sets of peptides with known phosphorylation sites while varying the fragmentation strategy (CID, HCD) and molarities of the peptides. The algorithm is also compatible with various peptide labeling strategies including SILAC and iTRAQ. The proposed algorithm is shown to be extremely scalable and a dramatic decrease in time is reported with increasing number of threads.",,
Saeed,"Bolger S.J., Hurtado P.A.G., Hoffert J.D., Saeed F., Pisitkun T., Knepper M.A.","Vasopressin regulates transport across the collecting duct epithelium in part via effects on gene transcription. Transcriptional regulation occurs partially via changes in phosphorylation of transcription factors, transcriptional coactivators, and protein kinases in the nucleus. To test whether vasopressin alters the nuclear phosphoproteome of vasopressin- sensitive cultured mouse mpkCCD cells, we used stable isotope labeling and mass spectrometry to quantify thousands of phosphorylation sites in nuclear extracts and nuclear pellet fractions. Measurements were made in the presence and absence of the vasopressin analog dDAVP. Of the 1,251 sites quantified, 39 changed significantly in response to dDAVP. Network analysis of the regulated proteins revealed two major clusters (""cell-cell adhesion"" and ""transcriptional regulation"") that were connected to known elements of the vasopressin signaling pathway. The hub proteins for these two clusters were the transcriptional coactivator ?-catenin and the transcription factor c-Jun. Phosphorylation of ?-catenin at Ser552 was increased by dDAVP [log2(dDAVP/vehicle) = 1.79], and phosphorylation of c-Jun at Ser73 was decreased [log2(dDAVP/vehicle)=-0.53]. The ?-catenin site is known to be targeted by either protein kinase A or Akt, both of which are activated in response to vasopressin. The c-Jun site is a canonical target for the MAP kinase Jnk2, which is downregulated in response to vasopressin in the collecting duct. The data support the idea that vasopressin-mediated control of transcription in collecting duct cells involves selective changes in the nuclear phosphoproteome. All data are available to users at http://helixweb.nih.gov/ESBL/ Database/mNPPD/. 2012 the American Physiological Society.",,
Saeed,"Zhao B., Pisitkun T., Hoffert J.D., Knepper M.A., Saeed F.","Profiling using high-throughput MS has discovered an overwhelming number of novel protein phosphorylation sites (""phosphosites""). However, the functional relevance of these sites is not always clear. In light of recent studies on the evolutionary mechanism of phosphorylation, we have developed CPhos, a Java program that can assess the conservation of phosphosites among species using an information theory-based approach. The degree of conservation established using CPhos can be used to assess the functional significance of phosphosites. CPhos has a user friendly graphical user interface and is available both as a web service and as a standalone Java application to assist phosphoproteomic researchers in analyzing and prioritizing lists of phosphosites for further experimental validation. CPhos can be accessed or downloaded at http://helixweb.nih.gov/CPhos/. 2012 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.",,
Saeed,"Douglass J., Gunaratne R., Bradford D., Saeed F., Hoffert J.D., Steinbach P.J., Knepper M.A., Pisitkun T.","A general question in molecular physiology is how to identify candidate protein kinases corresponding to a known or hypothetical phosphorylation site in a protein of interest. It is generally recognized that the amino acid sequence surrounding the phosphorylation site provides information that is relevant to identification of the cognate protein kinase. Here, we present a mass spectrometry-based method for profiling the target specificity of a given protein kinase as well as a computational tool for the calculation and visualization of the target preferences. The mass spectrometry-based method identifies sites phosphorylated in response to in vitro incubation of protein mixtures with active recombinant protein kinases followed by standard phosphoproteomic methodologies. The computational tool, called ""PhosphoLogo,"" uses an information-theoretic algorithm to calculate position-specific amino acid preferences and anti-preferences from the mass-spectrometry data http://helixweb.nih.gov/PhosphoLogo/). The method was tested using protein kinase A (catalytic subunit ?), revealing the well-known preference for basic amino acids in positions -2 and -3 relative to the phosphorylated amino acid. It also provides evidence for a preference for amino acids with a branched aliphatic side chain in position +1, a finding compatible with known crystal structures of protein kinase A. The method was also employed to profile target preferences and anti-preferences for 15 additional protein kinases with potential roles in regulation of epithelial transport: CK2, p38, AKT1, SGK1, PKC?, CaMK2?, DAPK1, MAPKAPK2, PKD3, PIM1, OSR1, STK39/SPAK, GSK3?, Wnk1, and Wnk4.",,
Saeed,"Hoffert J.D., Pisitkun T., Saeed F., Song J.H., Chou C.-L., Knepper M.A.","G protein-coupled receptors (GPCRs) regulate diverse physiological processes, and many human diseases are due to defects in GPCR signaling. To identify the dynamic response of a signaling network downstream from a prototypical Gs-coupled GPCR, the vasopressin V2 receptor, we have carried out multireplicate, quantitative phosphoproteomics with iTRAQ labeling at four time points following vasopressin exposure at a physiological concentration in cells isolated from rat kidney. A total of 12,167 phosphopeptides were identified from 2,783 proteins, with 273 changing significantly in abundance with vasopressin. Two-dimensional clustering of phosphopeptide time courses and Gene Ontology terms revealed that ligand binding to the V2 receptor affects more than simply the canonical cyclic adenosine monophosphateprotein kinase A and arrestin pathways under physiological conditions. The regulated proteins included key components of actin cytoskeleton remodeling, cell-cell adhesion, mitogen-activated protein kinase signaling, Wnt/?-catenin signaling, and apoptosis pathways. These data suggest that vasopressin can regulate an array of cellular functions well beyond its classical role in regulating water and solute transport. These results greatly expand the current view of GPCR signaling in a physiological context and shed new light on potential roles for this signaling network in disorders such as polycystic kidney disease. Finally, we provide an online resource of physiologically regulated phosphorylation sites with dynamic quantitative data (http://helixweb.nih.gov/ESBL/Database/TiPD/ index.html). 2012 by The American Society for Biochemistry and Molecular Biology, Inc.",,
Saeed,"Pisitkun T., Hoffert J.D., Saeed F., Knepper M.A.","Investigation of physiological mechanisms at a cellular level often requires production of high-quality antibodies, frequently using synthetic peptides as immunogens. Here we describe a new, web-based software tool called NHLBI-AbDesigner that allows the user to visualize the information needed to choose optimal peptide sequences for peptide-directed antibody production (http://helixweb.nih.gov/AbDesigner/). The choice of an immunizing peptide is generally based on a need to optimize immunogenicity, antibody specificity, multispecies conservation, and robustness in the face of posttranslational modifications (PTMs). AbDesigner displays information relevant to these criteria as follows: 1) ""Immunogenicity Score,"" based on hydropathy and secondary structure prediction; 2) ""Uniqueness Score,"" a predictor of specificity of an antibody against all proteins expressed in the same species; 3) ""Conservation Score,"" a predictor of ability of the antibody to recognize orthologs in other animal species; and 4) ""Protein Features"" that show structural domains, variable regions, and annotated PTMs that may affect antibody performance. AbDesigner displays the information online in an interactive graphical user interface, which allows the user to recognize the trade-offs that exist for alternative synthetic peptide choices and to choose the one that is best for a proposed application. Several examples of the use of AbDesigner for the display of such trade-offs are presented, including production of a new antibody to Slc9a3. We also used the program in large-scale mode to create a database listing the 15-amino acid peptides with the highest Immunogenicity Scores for all known proteins in five animal species, one plant species (Arabidopsis thaliana), and Saccharomyces cerevisiae. 2012 by the American Physiological Society.",,
Saeed,"Saeed F., Perez-Rathke A., Gwarnicki J., Berger-Wolf T., Khokhar A.","Genome resequencing with short reads generated from pyrosequencing generally relies on mapping the short reads against a single reference genome. However, mapping of reads from multiple reference genomes is not possible using a pairwise mapping algorithm. In order to align the reads w.r.t each other and the reference genomes, existing multiple sequence alignment(MSA) methods cannot be used because they do not take into account the position of these short reads with respect to the genome, and are highly inefficient for a large number of sequences. In this paper, we develop a highly scalable parallel algorithm based on domain decomposition, referred to as P-Pyro-Align, to align such a large number of reads from single or multiple reference genomes. The proposed alignment algorithm accurately aligns the erroneous reads, and has been implemented on a cluster of workstations using MPI library. Experimental results for different problem sizes are analyzed in terms of execution time, quality of the alignments, and the ability of the algorithm to handle reads from multiple haplotypes. We report high quality multiple alignment of up to 0.5 million reads. The algorithm is shown to be highly scalable and exhibits super-linear speedups with increasing number of processors. 2011 Elsevier Inc. All rights reserved.",,
Saeed,"Saeed F., Pisitkun T., Knepper M.A., Hoffert J.D.","Large-scale proteomic analysis is emerging as a powerful technique in biology and relies heavily on data acquired by state-of-the-art mass spectrometers. As with any other field in Systems Biology, computational tools are required to deal with this ocean of data. iTRAQ (isobaric Tags for Relative and Absolute quantification) is a technique that allows simultaneous quantification of proteins from multiple samples. Although iTRAQ data gives useful insights to the biologist, it is more complex to perform analysis and draw biological conclusions because of its multi-plexed design. One such problem is to find proteins that behave in a similar way (i.e. change in abundance) among various time points since the temporal variations in the proteomics data reveal important biological information. Distance based methods such as Euclidian distance or Pearson coefficient, and clustering techniques such as k-mean etc, are not able to take into account the temporal information of the series. In this paper, we present an linear-time algorithm for clustering similar patterns among various iTRAQ time course data irrespective of their absolute values. The algorithm, referred to as Temporal Pattern Mining(TPM), maps the data from a Cartesian plane to a discrete binary plane. After the mapping a dynamic programming technique allows mining of similar data elements that are temporally closer to each other. The proposed algorithm accurately clusters iTRAQ data that are temporally closer to each other with more than 99% accuracy. Experimental results for different problem sizes are analyzed in terms of quality of clusters, execution time and scalability for large data sets. An example from our proteomics data is provided at the end to demonstrate the performance of the algorithm and its ability to cluster temporal series irrespective of their distance from each other.",,
Saeed,"Saeed F., Khokhar A., Zagordi O., Beerenwinkel N.","Pyrosequencing is among the emerging sequencing techniques, capable of generating upto 100,000 overlapping reads in a single run. This technique is much faster and cheaper than the existing state of the art sequencing technique such as Sanger. However, the reads generated by pyrosequencing are short in size and contain numerous errors. In order to use these reads for any subsequent analysis, the reads must be aligned . Existing multiple sequence alignment methods cannot be used as they do not take into account the specific positions of the sequences with respect to the genome, and are highly inefficient for large number of sequences. Therefore, the common practice has been to use either simple pairwise alignment despite its poor accuracy for error prone pyroreads, or use computationally expensive techniques based on sequential gap propagation. In this paper, we develop a computationally efficient method based on domain decomposition, referred to as pyro-align, to align such large number of reads. The proposed alignment algorithm accurately aligns the erroneous reads in a short period of time, which is orders of magnitude faster than any existing method. The accuracy of the alignment is confirmed from the consensus obtained from the multiple alignments.",,
Saeed,"Saeed F., Khokhar A.","Multiple Sequences Alignment (MSA) of biological sequences is a fundamental problem in computational biology due to its critical significance in wide ranging applications including haplotype reconstruction, sequence homology, phylogenetic analysis, and prediction of evolutionary origins. The MSA problem is considered NP-hard and known heuristics for the problem do not scale well with increasing numbers of sequences. On the other hand, with the advent of a new breed of fast sequencing techniques it is now possible to generate thousands of sequences very quickly. For rapid sequence analysis, it is therefore desirable to develop fast MSA algorithms that scale well with an increase in the dataset size. In this paper, we present a novel domain decomposition based technique to solve the MSA problem on multiprocessing platforms. The domain decomposition based technique, in addition to yielding better quality, gives enormous advantages in terms of execution time and memory requirements. The proposed strategy allows one to decrease the time complexity of any known heuristic of O (N)x complexity by a factor of O (1 / p)x, where N is the number of sequences, x depends on the underlying heuristic approach, and p is the number of processing nodes. In particular, we propose a highly scalable algorithm, Sample-Align-D, for aligning biological sequences using Muscle system as the underlying heuristic. The proposed algorithm has been implemented on a cluster of workstations using the MPI library. Experimental results for different problem sizes are analyzed in terms of quality of alignment, execution time and speed-up. 2009 Elsevier Inc. All rights reserved.",,
Saeed,"Saeed F., Khokhar A.","Multiple Sequence Alignment (MSA) is one of the most computationally intensive tasks in Computational Biology. Existing best known solutions for multiple sequence alignment take several hours (in some cases days) of computation time to align, for example, 2000 homologous sequences of average length 300. Inspired by the Sample Sort approach in parallel processing, in this paper we propose a highly scalable multiprocessor solution for the MSA problem in phylogenetically diverse sequences. Our method employs an intelligent scheme to partition the set of sequences into smaller subsets using k-mer count based similarity index, referred to as k-mer rank. Each subset is then independently aligned in parallel using any sequential approach. Further fine tuning of the local alignments is achieved using constraints derived from a global ancestor of the entire set. The proposed Sample-Align-D Algorithm has been implemented on a cluster of workstations using MPI message passing library. The accuracy of the proposed solution has been tested on standard benchmarks such as PREFAB. The accuracy of the alignment produced by our methods is comparable to that of well known sequential MSA techniques. We were able to align 2000 randomly selected sequences from the Methanosarcina acetivorans genome in less than 10 minutes using Sample-Align-D on a 16 node cluster, compared to over 23 hours on sequential MUSCLE system running on a single cluster node. ©2008 IEEE.",,
Smith,"Smith D.M., Smith G.","We investigate a problem in quantitative information flow, namely to find the maximum information leakage caused by n repeated independent runs of a channel C with b columns. While this scenario is of general interest, it is particularly motivated by the study of timing attacks on cryptography implemented using the countermeasures known as blinding and bucketing. We measure leakage in terms of multiplicative Bayes capacity (also known as min-capacity) and we prove tight bounds that greatly improve the previously-known ones. To enable efficient computation of our new bounds, we investigate them using techniques of analytic combinatorics, proving that they satisfy a useful recurrence and (when b = 2) a close connection to Ramanujan's Q-function. 2017 IEEE.",,
Smith,"Bordenabe N.E., Smith G.","A fundamental challenge in controlling the leakage of sensitive information by computer systems is the possibility of correlations between different secrets, with the result that leaking information about one secret may also leak information about a different secret. We explore such leakage, here called Dalenius leakage, within the context of the g-leakage family of leakage measures. We prove a fundamental equivalence between Dalenius min-entropy leakage under arbitrary correlations and g-leakage under arbitrary gain functions, and show how this equivalence increases the significance of the composition refinement relation. We also consider Dalenius leakage in the case when the marginal distributions induced by the correlation are known, giving techniques to compute stronger upper bounds in this case. 2016 IEEE.",,
Smith,"Alvim M.S., Chatzikokolakis K., McIver A., Morgan C., Palamidessi C., Smith G.","Quantitative information flow aims to assess and control the leakage of sensitive information by computer systems. A key insight in this area is that no single leakage measure is appropriate in all operational scenarios, as a result, many leakage measures have been proposed, with many different properties. To clarify this complex situation, this paper studies information leakage axiomatically, showing important dependencies among different axioms. It also establishes a completeness result about the g-leakage family, showing that any leakage measure satisfying certain intuitively-reasonable properties can be expressed as a g-leakage. 2016 IEEE.",,
Smith,Smith G.,"In computer security, it is frequently necessary in practice to accept some leakage of confidential information. This motivates the development of theories of Quantitative Information Flow aimed at showing that some leaks are 'small' and therefore tolerable. We describe the fundamental view of channels as mappings from prior distributions on secrets to hyper-distributions, which are distributions on posterior distributions, and we show how g-leakage provides a rich family of operationally-significant measures of leakage. We also discuss two approaches to achieving robust judgments about leakage: notions of capacity and a robust leakage ordering called composition refinement. 2015 IEEE.",,
Smith,"McIver A., Morgan C., Smith G., Espinoza B., Meinicke L.","The observable output of a probabilistic system that processes a secret input might reveal some information about that input. The system can be modelled as an information-theoretic channel that specifies the probability of each output, given each input. Given a prior distribution on those inputs, entropy-like measures can then quantify the amount of information leakage caused by the channel. But it turns out that the conventional channel representation, as a matrix, contains structure that is redundant with respect to that leakage, such as the labeling of columns, and columns that are scalar multiples of each other. We therefore introduce abstract channels by quotienting over those redundancies. A fundamental question for channels is whether one is worse than another, from a leakage point of view. But it is difficult to answer this question robustly, given the multitude of possible prior distributions and leakage measures. Indeed, there is growing recognition that different leakage measures are appropriate in different circumstances, leading to the recently proposed g-leakage measures, which use gain functions g to model the operational scenario in which a channel operates: the strong g-leakage pre-order requires that channel A never leak more than channel B, for any prior and any gain function. Here we show that, on abstract channels, the strong g-leakage pre-order is antisymmetric, and therefore a partial order. It was previously shown [1] that the strong g-leakage ordering is implied by a structural ordering called composition refinement, which requires that A = BR, for some channel R; but the converse was not established in full generality, left open as the so-called Coriaceous Conjecture. Using ideas from [2], we here confirm the Coriaceous Conjecture. Hence the strong g-leakage ordering and composition refinement coincide, giving our partial order both structural- and leakage-testing significance. 2014 Springer-Verlag.",,
Smith,"Alvim M.S., Chatzikokolakis K., McIver A., Morgan C., Palamidessi C., Smith G.","Protecting sensitive information from improper disclosure is a fundamental security goal. It is complicated, and difficult to achieve, often because of unavoidable or even unpredictable operating conditions that can lead to breaches in planned security defences. An attractive approach is to frame the goal as a quantitative problem, and then to design methods that measure system vulnerabilities in terms of the amount of information they leak. A consequence is that the precise operating conditions, and assumptions about prior knowledge, can play a crucial role in assessing the severity of any measured vunerability. We develop this theme by concentrating on vulnerability measures that are robust in the sense of allowing general leakage bounds to be placed on a program, bounds that apply whatever its operating conditions and whatever the prior knowledge might be. In particular we propose a theory of channel capacity, generalising the Shannon capacity of information theory, that can apply both to additive- and to multiplicative forms of a recently-proposed measure known as g-leakage. Further, we explore the computational aspects of calculating these (new) capacities: one of these scenarios can be solved efficiently by expressing it as a Kantorovich distance, but another turns out to be NP-complete. We also find capacity bounds for arbitrary correlations with data not directly accessed by the channel, as in the scenario of Dalenius's Desideratum. 2014 IEEE.",,
Smith,"Espinoza B., Smith G.","Secrecy is fundamental to computer security, but real systems often cannot avoid leaking some secret information. For this reason, it is useful to model secrecy quantitatively, thinking of it as a “resource” that may be gradually “consumed” by a system. In this paper, we explore this intuition through several dynamic and static models of secrecy consumption, ultimately focusing on (average) vulnerability and min-entropy leakage as especially useful models of secrecy consumption. We also consider several composition operators that allow smaller systems to be combined into a larger system, and explore the extent to which the secrecy consumption of a combined system is constrained by the secrecy consumption of its constituents. 2013 Elsevier Inc.",,
Smith,"Alvim M.S., Chatzikokolakis K., Palamidessi C., Smith G.","This paper introduces g-leakage, a rich generalization of the min-entropy model of quantitative information flow. In g-leakage, the benefit that an adversary derives from a certain guess about a secret is specified using a gain function g. Gain functions allow a wide variety of operational scenarios to be modeled, including those where the adversary benefits from guessing a value close to the secret, guessing a part of the secret, guessing a property of the secret, or guessing the secret within some number of tries. We prove important properties of g-leakage, including bounds between min-capacity, g-capacity, and Shannon capacity. We also show a deep connection between a strong leakage ordering on two channels, C 1 and C 2, and the possibility of factoring C 1 into C 2C 3, for some C 3. Based on this connection, we propose a generalization of the Lattice of Information from deterministic to probabilistic channels. 2012, Daniel Hedin.",,
Smith,"Espinoza B., Smith G.","Theories of quantitative information flow offer an attractive framework for analyzing confidentiality in practical systems, which often cannot avoid ""small"" leaks of confidential information. Recently there has been growing interest in the theory of min-entropy leakage, which measures uncertainty based on a random variable's vulnerability to being guessed in one try by an adversary. Here we contribute to this theory by studying the min-entropy leakage of systems formed by cascading two channels together, using the output of the first channel as the input to the second channel. After considering the semantics of cascading carefully and exposing some technical subtleties, we prove that the min-entropy leakage of a cascade of two channels cannot exceed the leakage of the first channel; this result is a min-entropy analogue of the classic data-processing inequality. We show however that a comparable bound does not hold for the second channel. We then consider the min-capacity, or maximum leakage over all a priori distributions, showing that the min-capacity of a cascade of two channels cannot exceed the min-capacity of either channel. 2012 Springer-Verlag.",,
Smith,"Meng Z., Smith G.","We investigate a problem in quantitative information flow, namely to find the maximum information leakage caused by n repeated independent runs of a channel C with b columns. While this scenario is of general interest, it is particularly motivated by the study of timing attacks on cryptography implemented using the countermeasures known as blinding and bucketing. We measure leakage in terms of multiplicative Bayes capacity (also known as min-capacity) and we prove tight bounds that greatly improve the previously-known ones. To enable efficient computation of our new bounds, we investigate them using techniques of analytic combinatorics, proving that they satisfy a useful recurrence and (when b = 2) a close connection to Ramanujan's Q-function. 2017 IEEE.",,
Smith,"Smith G., Alpízar R.","A fundamental challenge in controlling the leakage of sensitive information by computer systems is the possibility of correlations between different secrets, with the result that leaking information about one secret may also leak information about a different secret. We explore such leakage, here called Dalenius leakage, within the context of the g-leakage family of leakage measures. We prove a fundamental equivalence between Dalenius min-entropy leakage under arbitrary correlations and g-leakage under arbitrary gain functions, and show how this equivalence increases the significance of the composition refinement relation. We also consider Dalenius leakage in the case when the marginal distributions induced by the correlation are known, giving techniques to compute stronger upper bounds in this case. 2016 IEEE.",,
Smith,Smith G.,"Quantitative information flow aims to assess and control the leakage of sensitive information by computer systems. A key insight in this area is that no single leakage measure is appropriate in all operational scenarios, as a result, many leakage measures have been proposed, with many different properties. To clarify this complex situation, this paper studies information leakage axiomatically, showing important dependencies among different axioms. It also establishes a completeness result about the g-leakage family, showing that any leakage measure satisfying certain intuitively-reasonable properties can be expressed as a g-leakage. 2016 IEEE.",,
Smith,"Köpf B., Smith G.","In computer security, it is frequently necessary in practice to accept some leakage of confidential information. This motivates the development of theories of Quantitative Information Flow aimed at showing that some leaks are 'small' and therefore tolerable. We describe the fundamental view of channels as mappings from prior distributions on secrets to hyper-distributions, which are distributions on posterior distributions, and we show how g-leakage provides a rich family of operationally-significant measures of leakage. We also discuss two approaches to achieving robust judgments about leakage: notions of capacity and a robust leakage ordering called composition refinement. 2015 IEEE.",,
Smith,"Alpízar R., Smith G.","The observable output of a probabilistic system that processes a secret input might reveal some information about that input. The system can be modelled as an information-theoretic channel that specifies the probability of each output, given each input. Given a prior distribution on those inputs, entropy-like measures can then quantify the amount of information leakage caused by the channel. But it turns out that the conventional channel representation, as a matrix, contains structure that is redundant with respect to that leakage, such as the labeling of columns, and columns that are scalar multiples of each other. We therefore introduce abstract channels by quotienting over those redundancies. A fundamental question for channels is whether one is worse than another, from a leakage point of view. But it is difficult to answer this question robustly, given the multitude of possible prior distributions and leakage measures. Indeed, there is growing recognition that different leakage measures are appropriate in different circumstances, leading to the recently proposed g-leakage measures, which use gain functions g to model the operational scenario in which a channel operates: the strong g-leakage pre-order requires that channel A never leak more than channel B, for any prior and any gain function. Here we show that, on abstract channels, the strong g-leakage pre-order is antisymmetric, and therefore a partial order. It was previously shown [1] that the strong g-leakage ordering is implied by a structural ordering called composition refinement, which requires that A = BR, for some channel R; but the converse was not established in full generality, left open as the so-called Coriaceous Conjecture. Using ideas from [2], we here confirm the Coriaceous Conjecture. Hence the strong g-leakage ordering and composition refinement coincide, giving our partial order both structural- and leakage-testing significance. 2014 Springer-Verlag.",,
Smith,"Andrés M.E., Palamidessi C., Van Rossum P., Smith G.","Protecting sensitive information from improper disclosure is a fundamental security goal. It is complicated, and difficult to achieve, often because of unavoidable or even unpredictable operating conditions that can lead to breaches in planned security defences. An attractive approach is to frame the goal as a quantitative problem, and then to design methods that measure system vulnerabilities in terms of the amount of information they leak. A consequence is that the precise operating conditions, and assumptions about prior knowledge, can play a crucial role in assessing the severity of any measured vunerability. We develop this theme by concentrating on vulnerability measures that are robust in the sense of allowing general leakage bounds to be placed on a program, bounds that apply whatever its operating conditions and whatever the prior knowledge might be. In particular we propose a theory of channel capacity, generalising the Shannon capacity of information theory, that can apply both to additive- and to multiplicative forms of a recently-proposed measure known as g-leakage. Further, we explore the computational aspects of calculating these (new) capacities: one of these scenarios can be solved efficiently by expressing it as a Kantorovich distance, but another turns out to be NP-complete. We also find capacity bounds for arbitrary correlations with data not directly accessed by the channel, as in the scenario of Dalenius's Desideratum. 2014 IEEE.",,
Smith,Smith G.,"Secrecy is fundamental to computer security, but real systems often cannot avoid leaking some secret information. For this reason, it is useful to model secrecy quantitatively, thinking of it as a “resource” that may be gradually “consumed” by a system. In this paper, we explore this intuition through several dynamic and static models of secrecy consumption, ultimately focusing on (average) vulnerability and min-entropy leakage as especially useful models of secrecy consumption. We also consider several composition operators that allow smaller systems to be combined into a larger system, and explore the extent to which the secrecy consumption of a combined system is constrained by the secrecy consumption of its constituents. 2013 Elsevier Inc.",,
Smith,"Koller R., Rangaswami R., Marrero J., Hernandez I., Smith G., Barsilai M., Necula S., Sadjadi S.M., Li T., Merrill K.","This paper introduces g-leakage, a rich generalization of the min-entropy model of quantitative information flow. In g-leakage, the benefit that an adversary derives from a certain guess about a secret is specified using a gain function g. Gain functions allow a wide variety of operational scenarios to be modeled, including those where the adversary benefits from guessing a value close to the secret, guessing a part of the secret, guessing a property of the secret, or guessing the secret within some number of tries. We prove important properties of g-leakage, including bounds between min-capacity, g-capacity, and Shannon capacity. We also show a deep connection between a strong leakage ordering on two channels, C 1 and C 2, and the possibility of factoring C 1 into C 2C 3, for some C 3. Based on this connection, we propose a generalization of the Lattice of Information from deterministic to probabilistic channels. 2012, Daniel Hedin.",,
Smith,Smith G.,"Theories of quantitative information flow offer an attractive framework for analyzing confidentiality in practical systems, which often cannot avoid ""small"" leaks of confidential information. Recently there has been growing interest in the theory of min-entropy leakage, which measures uncertainty based on a random variable's vulnerability to being guessed in one try by an adversary. Here we contribute to this theory by studying the min-entropy leakage of systems formed by cascading two channels together, using the output of the first channel as the input to the second channel. After considering the semantics of cascading carefully and exposing some technical subtleties, we prove that the min-entropy leakage of a cascade of two channels cannot exceed the leakage of the first channel; this result is a min-entropy analogue of the classic data-processing inequality. We show however that a comparable bound does not hold for the second channel. We then consider the min-capacity, or maximum leakage over all a priori distributions, showing that the min-capacity of a cascade of two channels cannot exceed the min-capacity of either channel. 2012 Springer-Verlag.",,
Smith,Smith G.,"We investigate a problem in quantitative information flow, namely to find the maximum information leakage caused by n repeated independent runs of a channel C with b columns. While this scenario is of general interest, it is particularly motivated by the study of timing attacks on cryptography implemented using the countermeasures known as blinding and bucketing. We measure leakage in terms of multiplicative Bayes capacity (also known as min-capacity) and we prove tight bounds that greatly improve the previously-known ones. To enable efficient computation of our new bounds, we investigate them using techniques of analytic combinatorics, proving that they satisfy a useful recurrence and (when b = 2) a close connection to Ramanujan's Q-function. 2017 IEEE.",,
Smith,"Smith G., Alpízar R.","A fundamental challenge in controlling the leakage of sensitive information by computer systems is the possibility of correlations between different secrets, with the result that leaking information about one secret may also leak information about a different secret. We explore such leakage, here called Dalenius leakage, within the context of the g-leakage family of leakage measures. We prove a fundamental equivalence between Dalenius min-entropy leakage under arbitrary correlations and g-leakage under arbitrary gain functions, and show how this equivalence increases the significance of the composition refinement relation. We also consider Dalenius leakage in the case when the marginal distributions induced by the correlation are known, giving techniques to compute stronger upper bounds in this case. 2016 IEEE.",,
Smith,Smith G.,"Quantitative information flow aims to assess and control the leakage of sensitive information by computer systems. A key insight in this area is that no single leakage measure is appropriate in all operational scenarios, as a result, many leakage measures have been proposed, with many different properties. To clarify this complex situation, this paper studies information leakage axiomatically, showing important dependencies among different axioms. It also establishes a completeness result about the g-leakage family, showing that any leakage measure satisfying certain intuitively-reasonable properties can be expressed as a g-leakage. 2016 IEEE.",,
Smith,"Deng Z., Smith G.","In computer security, it is frequently necessary in practice to accept some leakage of confidential information. This motivates the development of theories of Quantitative Information Flow aimed at showing that some leaks are 'small' and therefore tolerable. We describe the fundamental view of channels as mappings from prior distributions on secrets to hyper-distributions, which are distributions on posterior distributions, and we show how g-leakage provides a rich family of operationally-significant measures of leakage. We also discuss two approaches to achieving robust judgments about leakage: notions of capacity and a robust leakage ordering called composition refinement. 2015 IEEE.",,
Smith,"Smith G., Alpízar R.","The observable output of a probabilistic system that processes a secret input might reveal some information about that input. The system can be modelled as an information-theoretic channel that specifies the probability of each output, given each input. Given a prior distribution on those inputs, entropy-like measures can then quantify the amount of information leakage caused by the channel. But it turns out that the conventional channel representation, as a matrix, contains structure that is redundant with respect to that leakage, such as the labeling of columns, and columns that are scalar multiples of each other. We therefore introduce abstract channels by quotienting over those redundancies. A fundamental question for channels is whether one is worse than another, from a leakage point of view. But it is difficult to answer this question robustly, given the multitude of possible prior distributions and leakage measures. Indeed, there is growing recognition that different leakage measures are appropriate in different circumstances, leading to the recently proposed g-leakage measures, which use gain functions g to model the operational scenario in which a channel operates: the strong g-leakage pre-order requires that channel A never leak more than channel B, for any prior and any gain function. Here we show that, on abstract channels, the strong g-leakage pre-order is antisymmetric, and therefore a partial order. It was previously shown [1] that the strong g-leakage ordering is implied by a structural ordering called composition refinement, which requires that A = BR, for some channel R; but the converse was not established in full generality, left open as the so-called Coriaceous Conjecture. Using ideas from [2], we here confirm the Coriaceous Conjecture. Hence the strong g-leakage ordering and composition refinement coincide, giving our partial order both structural- and leakage-testing significance. 2014 Springer-Verlag.",,
Smith,Smith G.,"Protecting sensitive information from improper disclosure is a fundamental security goal. It is complicated, and difficult to achieve, often because of unavoidable or even unpredictable operating conditions that can lead to breaches in planned security defences. An attractive approach is to frame the goal as a quantitative problem, and then to design methods that measure system vulnerabilities in terms of the amount of information they leak. A consequence is that the precise operating conditions, and assumptions about prior knowledge, can play a crucial role in assessing the severity of any measured vunerability. We develop this theme by concentrating on vulnerability measures that are robust in the sense of allowing general leakage bounds to be placed on a program, bounds that apply whatever its operating conditions and whatever the prior knowledge might be. In particular we propose a theory of channel capacity, generalising the Shannon capacity of information theory, that can apply both to additive- and to multiplicative forms of a recently-proposed measure known as g-leakage. Further, we explore the computational aspects of calculating these (new) capacities: one of these scenarios can be solved efficiently by expressing it as a Kantorovich distance, but another turns out to be NP-complete. We also find capacity bounds for arbitrary correlations with data not directly accessed by the channel, as in the scenario of Dalenius's Desideratum. 2014 IEEE.",,
Smith,"Deng Z., Smith G.","Secrecy is fundamental to computer security, but real systems often cannot avoid leaking some secret information. For this reason, it is useful to model secrecy quantitatively, thinking of it as a “resource” that may be gradually “consumed” by a system. In this paper, we explore this intuition through several dynamic and static models of secrecy consumption, ultimately focusing on (average) vulnerability and min-entropy leakage as especially useful models of secrecy consumption. We also consider several composition operators that allow smaller systems to be combined into a larger system, and explore the extent to which the secrecy consumption of a combined system is constrained by the secrecy consumption of its constituents. 2013 Elsevier Inc.",,
Smith,Smith G.,"This paper introduces g-leakage, a rich generalization of the min-entropy model of quantitative information flow. In g-leakage, the benefit that an adversary derives from a certain guess about a secret is specified using a gain function g. Gain functions allow a wide variety of operational scenarios to be modeled, including those where the adversary benefits from guessing a value close to the secret, guessing a part of the secret, guessing a property of the secret, or guessing the secret within some number of tries. We prove important properties of g-leakage, including bounds between min-capacity, g-capacity, and Shannon capacity. We also show a deep connection between a strong leakage ordering on two channels, C 1 and C 2, and the possibility of factoring C 1 into C 2C 3, for some C 3. Based on this connection, we propose a generalization of the Lattice of Information from deterministic to probabilistic channels. 2012, Daniel Hedin.",,
Smith,"Vuong N.N., Smith G.S., Deng Y.","Theories of quantitative information flow offer an attractive framework for analyzing confidentiality in practical systems, which often cannot avoid ""small"" leaks of confidential information. Recently there has been growing interest in the theory of min-entropy leakage, which measures uncertainty based on a random variable's vulnerability to being guessed in one try by an adversary. Here we contribute to this theory by studying the min-entropy leakage of systems formed by cascading two channels together, using the output of the first channel as the input to the second channel. After considering the semantics of cascading carefully and exposing some technical subtleties, we prove that the min-entropy leakage of a cascade of two channels cannot exceed the leakage of the first channel; this result is a min-entropy analogue of the classic data-processing inequality. We show however that a comparable bound does not hold for the second channel. We then consider the min-capacity, or maximum leakage over all a priori distributions, showing that the min-capacity of a cascade of two channels cannot exceed the min-capacity of either channel. 2012 Springer-Verlag.",,
Smith,Smith G.,"We investigate a problem in quantitative information flow, namely to find the maximum information leakage caused by n repeated independent runs of a channel C with b columns. While this scenario is of general interest, it is particularly motivated by the study of timing attacks on cryptography implemented using the countermeasures known as blinding and bucketing. We measure leakage in terms of multiplicative Bayes capacity (also known as min-capacity) and we prove tight bounds that greatly improve the previously-known ones. To enable efficient computation of our new bounds, we investigate them using techniques of analytic combinatorics, proving that they satisfy a useful recurrence and (when b = 2) a close connection to Ramanujan's Q-function. 2017 IEEE.",,
Smith,"Volpano D., Smith G.","A fundamental challenge in controlling the leakage of sensitive information by computer systems is the possibility of correlations between different secrets, with the result that leaking information about one secret may also leak information about a different secret. We explore such leakage, here called Dalenius leakage, within the context of the g-leakage family of leakage measures. We prove a fundamental equivalence between Dalenius min-entropy leakage under arbitrary correlations and g-leakage under arbitrary gain functions, and show how this equivalence increases the significance of the composition refinement relation. We also consider Dalenius leakage in the case when the marginal distributions induced by the correlation are known, giving techniques to compute stronger upper bounds in this case. 2016 IEEE.",,
Smith,"Smith G., Volpano D.","Quantitative information flow aims to assess and control the leakage of sensitive information by computer systems. A key insight in this area is that no single leakage measure is appropriate in all operational scenarios, as a result, many leakage measures have been proposed, with many different properties. To clarify this complex situation, this paper studies information leakage axiomatically, showing important dependencies among different axioms. It also establishes a completeness result about the g-leakage family, showing that any leakage measure satisfying certain intuitively-reasonable properties can be expressed as a g-leakage. 2016 IEEE.",,
Smith,"Volpano Dennis, Smith Geoffrey","In computer security, it is frequently necessary in practice to accept some leakage of confidential information. This motivates the development of theories of Quantitative Information Flow aimed at showing that some leaks are 'small' and therefore tolerable. We describe the fundamental view of channels as mappings from prior distributions on secrets to hyper-distributions, which are distributions on posterior distributions, and we show how g-leakage provides a rich family of operationally-significant measures of leakage. We also discuss two approaches to achieving robust judgments about leakage: notions of capacity and a robust leakage ordering called composition refinement. 2015 IEEE.",,
Smith,"Smith G., Volpano D.","The observable output of a probabilistic system that processes a secret input might reveal some information about that input. The system can be modelled as an information-theoretic channel that specifies the probability of each output, given each input. Given a prior distribution on those inputs, entropy-like measures can then quantify the amount of information leakage caused by the channel. But it turns out that the conventional channel representation, as a matrix, contains structure that is redundant with respect to that leakage, such as the labeling of columns, and columns that are scalar multiples of each other. We therefore introduce abstract channels by quotienting over those redundancies. A fundamental question for channels is whether one is worse than another, from a leakage point of view. But it is difficult to answer this question robustly, given the multitude of possible prior distributions and leakage measures. Indeed, there is growing recognition that different leakage measures are appropriate in different circumstances, leading to the recently proposed g-leakage measures, which use gain functions g to model the operational scenario in which a channel operates: the strong g-leakage pre-order requires that channel A never leak more than channel B, for any prior and any gain function. Here we show that, on abstract channels, the strong g-leakage pre-order is antisymmetric, and therefore a partial order. It was previously shown [1] that the strong g-leakage ordering is implied by a structural ordering called composition refinement, which requires that A = BR, for some channel R; but the converse was not established in full generality, left open as the so-called Coriaceous Conjecture. Using ideas from [2], we here confirm the Coriaceous Conjecture. Hence the strong g-leakage ordering and composition refinement coincide, giving our partial order both structural- and leakage-testing significance. 2014 Springer-Verlag.",,
Smith,"Smith Geoffrey, Volpano Dennis","Protecting sensitive information from improper disclosure is a fundamental security goal. It is complicated, and difficult to achieve, often because of unavoidable or even unpredictable operating conditions that can lead to breaches in planned security defences. An attractive approach is to frame the goal as a quantitative problem, and then to design methods that measure system vulnerabilities in terms of the amount of information they leak. A consequence is that the precise operating conditions, and assumptions about prior knowledge, can play a crucial role in assessing the severity of any measured vunerability. We develop this theme by concentrating on vulnerability measures that are robust in the sense of allowing general leakage bounds to be placed on a program, bounds that apply whatever its operating conditions and whatever the prior knowledge might be. In particular we propose a theory of channel capacity, generalising the Shannon capacity of information theory, that can apply both to additive- and to multiplicative forms of a recently-proposed measure known as g-leakage. Further, we explore the computational aspects of calculating these (new) capacities: one of these scenarios can be solved efficiently by expressing it as a Kantorovich distance, but another turns out to be NP-complete. We also find capacity bounds for arbitrary correlations with data not directly accessed by the channel, as in the scenario of Dalenius's Desideratum. 2014 IEEE.",,
Smith,"Volpano Dennis, Smith Geoffrey","Secrecy is fundamental to computer security, but real systems often cannot avoid leaking some secret information. For this reason, it is useful to model secrecy quantitatively, thinking of it as a “resource” that may be gradually “consumed” by a system. In this paper, we explore this intuition through several dynamic and static models of secrecy consumption, ultimately focusing on (average) vulnerability and min-entropy leakage as especially useful models of secrecy consumption. We also consider several composition operators that allow smaller systems to be combined into a larger system, and explore the extent to which the secrecy consumption of a combined system is constrained by the secrecy consumption of its constituents. 2013 Elsevier Inc.",,
Smith,"Volpano Dennis, Smith Geoffrey","This paper introduces g-leakage, a rich generalization of the min-entropy model of quantitative information flow. In g-leakage, the benefit that an adversary derives from a certain guess about a secret is specified using a gain function g. Gain functions allow a wide variety of operational scenarios to be modeled, including those where the adversary benefits from guessing a value close to the secret, guessing a part of the secret, guessing a property of the secret, or guessing the secret within some number of tries. We prove important properties of g-leakage, including bounds between min-capacity, g-capacity, and Shannon capacity. We also show a deep connection between a strong leakage ordering on two channels, C 1 and C 2, and the possibility of factoring C 1 into C 2C 3, for some C 3. Based on this connection, we propose a generalization of the Lattice of Information from deterministic to probabilistic channels. 2012, Daniel Hedin.",,
Smith,"Volpano D., Smith G.","Theories of quantitative information flow offer an attractive framework for analyzing confidentiality in practical systems, which often cannot avoid ""small"" leaks of confidential information. Recently there has been growing interest in the theory of min-entropy leakage, which measures uncertainty based on a random variable's vulnerability to being guessed in one try by an adversary. Here we contribute to this theory by studying the min-entropy leakage of systems formed by cascading two channels together, using the output of the first channel as the input to the second channel. After considering the semantics of cascading carefully and exposing some technical subtleties, we prove that the min-entropy leakage of a cascade of two channels cannot exceed the leakage of the first channel; this result is a min-entropy analogue of the classic data-processing inequality. We show however that a comparable bound does not hold for the second channel. We then consider the min-capacity, or maximum leakage over all a priori distributions, showing that the min-capacity of a cascade of two channels cannot exceed the min-capacity of either channel. 2012 Springer-Verlag.",,
Smith,"Smith G., Volpano D.","We investigate a problem in quantitative information flow, namely to find the maximum information leakage caused by n repeated independent runs of a channel C with b columns. While this scenario is of general interest, it is particularly motivated by the study of timing attacks on cryptography implemented using the countermeasures known as blinding and bucketing. We measure leakage in terms of multiplicative Bayes capacity (also known as min-capacity) and we prove tight bounds that greatly improve the previously-known ones. To enable efficient computation of our new bounds, we investigate them using techniques of analytic combinatorics, proving that they satisfy a useful recurrence and (when b = 2) a close connection to Ramanujan's Q-function. 2017 IEEE.",,
Smith,"Volpano D., Smith G.","A fundamental challenge in controlling the leakage of sensitive information by computer systems is the possibility of correlations between different secrets, with the result that leaking information about one secret may also leak information about a different secret. We explore such leakage, here called Dalenius leakage, within the context of the g-leakage family of leakage measures. We prove a fundamental equivalence between Dalenius min-entropy leakage under arbitrary correlations and g-leakage under arbitrary gain functions, and show how this equivalence increases the significance of the composition refinement relation. We also consider Dalenius leakage in the case when the marginal distributions induced by the correlation are known, giving techniques to compute stronger upper bounds in this case. 2016 IEEE.",,
Smith,"Smith G., Volpano D.","Quantitative information flow aims to assess and control the leakage of sensitive information by computer systems. A key insight in this area is that no single leakage measure is appropriate in all operational scenarios, as a result, many leakage measures have been proposed, with many different properties. To clarify this complex situation, this paper studies information leakage axiomatically, showing important dependencies among different axioms. It also establishes a completeness result about the g-leakage family, showing that any leakage measure satisfying certain intuitively-reasonable properties can be expressed as a g-leakage. 2016 IEEE.",,
Smith,"Volpano Dennis, Irvine Cynthia, Smith Geoffrey","In computer security, it is frequently necessary in practice to accept some leakage of confidential information. This motivates the development of theories of Quantitative Information Flow aimed at showing that some leaks are 'small' and therefore tolerable. We describe the fundamental view of channels as mappings from prior distributions on secrets to hyper-distributions, which are distributions on posterior distributions, and we show how g-leakage provides a rich family of operationally-significant measures of leakage. We also discuss two approaches to achieving robust judgments about leakage: notions of capacity and a robust leakage ordering called composition refinement. 2015 IEEE.",,
Smith,"Volpano D., Smith G.","The observable output of a probabilistic system that processes a secret input might reveal some information about that input. The system can be modelled as an information-theoretic channel that specifies the probability of each output, given each input. Given a prior distribution on those inputs, entropy-like measures can then quantify the amount of information leakage caused by the channel. But it turns out that the conventional channel representation, as a matrix, contains structure that is redundant with respect to that leakage, such as the labeling of columns, and columns that are scalar multiples of each other. We therefore introduce abstract channels by quotienting over those redundancies. A fundamental question for channels is whether one is worse than another, from a leakage point of view. But it is difficult to answer this question robustly, given the multitude of possible prior distributions and leakage measures. Indeed, there is growing recognition that different leakage measures are appropriate in different circumstances, leading to the recently proposed g-leakage measures, which use gain functions g to model the operational scenario in which a channel operates: the strong g-leakage pre-order requires that channel A never leak more than channel B, for any prior and any gain function. Here we show that, on abstract channels, the strong g-leakage pre-order is antisymmetric, and therefore a partial order. It was previously shown [1] that the strong g-leakage ordering is implied by a structural ordering called composition refinement, which requires that A = BR, for some channel R; but the converse was not established in full generality, left open as the so-called Coriaceous Conjecture. Using ideas from [2], we here confirm the Coriaceous Conjecture. Hence the strong g-leakage ordering and composition refinement coincide, giving our partial order both structural- and leakage-testing significance. 2014 Springer-Verlag.",,
Smith,Smith G.S.,"Protecting sensitive information from improper disclosure is a fundamental security goal. It is complicated, and difficult to achieve, often because of unavoidable or even unpredictable operating conditions that can lead to breaches in planned security defences. An attractive approach is to frame the goal as a quantitative problem, and then to design methods that measure system vulnerabilities in terms of the amount of information they leak. A consequence is that the precise operating conditions, and assumptions about prior knowledge, can play a crucial role in assessing the severity of any measured vunerability. We develop this theme by concentrating on vulnerability measures that are robust in the sense of allowing general leakage bounds to be placed on a program, bounds that apply whatever its operating conditions and whatever the prior knowledge might be. In particular we propose a theory of channel capacity, generalising the Shannon capacity of information theory, that can apply both to additive- and to multiplicative forms of a recently-proposed measure known as g-leakage. Further, we explore the computational aspects of calculating these (new) capacities: one of these scenarios can be solved efficiently by expressing it as a Kantorovich distance, but another turns out to be NP-complete. We also find capacity bounds for arbitrary correlations with data not directly accessed by the channel, as in the scenario of Dalenius's Desideratum. 2014 IEEE.",,
Smith,Smith G.S.,"Secrecy is fundamental to computer security, but real systems often cannot avoid leaking some secret information. For this reason, it is useful to model secrecy quantitatively, thinking of it as a “resource” that may be gradually “consumed” by a system. In this paper, we explore this intuition through several dynamic and static models of secrecy consumption, ultimately focusing on (average) vulnerability and min-entropy leakage as especially useful models of secrecy consumption. We also consider several composition operators that allow smaller systems to be combined into a larger system, and explore the extent to which the secrecy consumption of a combined system is constrained by the secrecy consumption of its constituents. 2013 Elsevier Inc.",,
Smith,Smith G.S.,"This paper introduces g-leakage, a rich generalization of the min-entropy model of quantitative information flow. In g-leakage, the benefit that an adversary derives from a certain guess about a secret is specified using a gain function g. Gain functions allow a wide variety of operational scenarios to be modeled, including those where the adversary benefits from guessing a value close to the secret, guessing a part of the secret, guessing a property of the secret, or guessing the secret within some number of tries. We prove important properties of g-leakage, including bounds between min-capacity, g-capacity, and Shannon capacity. We also show a deep connection between a strong leakage ordering on two channels, C 1 and C 2, and the possibility of factoring C 1 into C 2C 3, for some C 3. Based on this connection, we propose a generalization of the Lattice of Information from deterministic to probabilistic channels. 2012, Daniel Hedin.",,
Smith,Smith G.S.,"Theories of quantitative information flow offer an attractive framework for analyzing confidentiality in practical systems, which often cannot avoid ""small"" leaks of confidential information. Recently there has been growing interest in the theory of min-entropy leakage, which measures uncertainty based on a random variable's vulnerability to being guessed in one try by an adversary. Here we contribute to this theory by studying the min-entropy leakage of systems formed by cascading two channels together, using the output of the first channel as the input to the second channel. After considering the semantics of cascading carefully and exposing some technical subtleties, we prove that the min-entropy leakage of a cascade of two channels cannot exceed the leakage of the first channel; this result is a min-entropy analogue of the classic data-processing inequality. We show however that a comparable bound does not hold for the second channel. We then consider the min-capacity, or maximum leakage over all a priori distributions, showing that the min-capacity of a cascade of two channels cannot exceed the min-capacity of either channel. 2012 Springer-Verlag.",,
Smith,"Volpano D.M., Smith G.S.","We investigate a problem in quantitative information flow, namely to find the maximum information leakage caused by n repeated independent runs of a channel C with b columns. While this scenario is of general interest, it is particularly motivated by the study of timing attacks on cryptography implemented using the countermeasures known as blinding and bucketing. We measure leakage in terms of multiplicative Bayes capacity (also known as min-capacity) and we prove tight bounds that greatly improve the previously-known ones. To enable efficient computation of our new bounds, we investigate them using techniques of analytic combinatorics, proving that they satisfy a useful recurrence and (when b = 2) a close connection to Ramanujan's Q-function. 2017 IEEE.",,
Smith,Smith G.S.,"A fundamental challenge in controlling the leakage of sensitive information by computer systems is the possibility of correlations between different secrets, with the result that leaking information about one secret may also leak information about a different secret. We explore such leakage, here called Dalenius leakage, within the context of the g-leakage family of leakage measures. We prove a fundamental equivalence between Dalenius min-entropy leakage under arbitrary correlations and g-leakage under arbitrary gain functions, and show how this equivalence increases the significance of the composition refinement relation. We also consider Dalenius leakage in the case when the marginal distributions induced by the correlation are known, giving techniques to compute stronger upper bounds in this case. 2016 IEEE.",,
Smith,Smith G.S.,"Quantitative information flow aims to assess and control the leakage of sensitive information by computer systems. A key insight in this area is that no single leakage measure is appropriate in all operational scenarios, as a result, many leakage measures have been proposed, with many different properties. To clarify this complex situation, this paper studies information leakage axiomatically, showing important dependencies among different axioms. It also establishes a completeness result about the g-leakage family, showing that any leakage measure satisfying certain intuitively-reasonable properties can be expressed as a g-leakage. 2016 IEEE.",,
Weiss,"Balcazar R., Barreto A., Ortega F.R., Weiss M., Tarre K., Rishe N.D.","CircGR is a multi-touch non-symbolic gesture recognition algorithm, which utilizes circular statistic measures to implement linearithmic (On log n) template-based matching. CircGR provides a solution to gesture designers, which allows for building complex multi-touch gestures with high-confidence accuracy. We demonstrated the algorithm and described a user study with 60 subjects and over 12,000 gestures collected for an original gesture set of 36. The accuracy is over 99% with the Matthews correlation coefficient of 0.95. In addition, early gesture detection was successful in CircGR as well. 2017 Association for Computing Machinery.",,
Weiss,"Lowery R.K., Uribe G., Jimenez E.B., Weiss M.A., Herrera K.J., Regueiro M., Herrera R.J.","Analyses of the genetic relationships among modern humans, Neanderthals and Denisovans have suggested that 1-4% of the non-Sub-Saharan African gene pool may be Neanderthal derived, while 6-8% of the Melanesian gene pool may be the product of admixture between the Denisovans and the direct ancestors of Melanesians. In the present study, we analyzed single nucleotide polymorphism (SNP) diversity among a worldwide collection of contemporary human populations with respect to the genetic constitution of these two archaic hominins and Pan troglodytes (chimpanzee). We partitioned SNPs into subsets, including those that are derived in both archaic lineages, those that are ancestral in both archaic lineages and those that are only derived in one archaic lineage. By doing this, we have conducted separate examinations of subsets of mutations with higher probabilities of divergent phylogenetic origins. While previous investigations have excluded SNPs from common ancestors in principal component analyses, we included common ancestral SNPs in our analyses to visualize the relative placement of the Neanderthal and Denisova among human populations. To assess the genetic similarities among the various hominin lineages, we performed genetic structure analyses to provide a comparison of genetic patterns found within contemporary human genomes that may have archaic or common ancestral roots. Our results indicate that 3.6% of the Neanderthal genome is shared with roughly 65.4% of the average European gene pool, which clinally diminishes with distance from Europe. Our results suggest that Neanderthal genetic associations with contemporary non-Sub-Saharan African populations, as well as the genetic affinities observed between Denisovans and Melanesians most likely result from the retention of ancient mutations in these populations. 2013 Elsevier B.V.",,
Weiss,"Chen S.-C., Wang X., Rishe N., Weiss M.A.","With the increasing use of geographical data in real-world applications, Geographic Information Systems (GISs) have recently emerged as a fruitful area for research. In order to provide information to a multitude of users, the World Wide Web (WWW) techniques have been integrated into GISs. A high-performance web-based GIS, called TerraFly, has been developed in order to provide web-based GIS accesses to the general public. The design of TerraFly considers two major aspects: (1) the system architecture including client, database server, proxy server and information server; and (2) the semantic R-tree data structure and semantic queries. The system architecture utilizes the existing resources to achieve maximum performance by using the ""Internally Distributed Multithreading (IDMT)"" technique. The spatial access method, semantic R-trees, is used to search for an object, based on both spatial and semantic information. System performance results are presented and analyzed. Reducing network traffic to achieve faster response to users is also discussed. 2003 Elsevier Inc. All rights reserved.",,
Weiss,"Drysdale S., Hromcik J., Weiss M.A., Hahne R.","With the Java language replacing C++ on the 2004 AP CS Exam, teachers need to be informed about the changes that must be implemented to support an OO approach to programming. This special session will include a retrospective look at the motivation behind the change to an object-oriented language, the process undertaken to select a testable language subset, the need to continue the development and classroom implementation of a Case Study, and a look at how the shift from an object-based approach to programming in C++ to an OO approach in Java leads to curriculum modification. The AP CS Development Committee's charge is to not only provide a comprehensive testing mechanism, but also advise, through various publications, a direction that high school teachers should take in preparing a foundation for more advanced student studies during college. This special session will bring together two college and two high school members of the AP CS Development Committee to share some of their insights into how the experts do it. AP teachers will learn valuable information to support their shift to Java in the classroom, about the exam for which they are preparing their students, and pertinent information about the Marine Biology Simulation. Time will be provided to discuss participant's questions.",,
Weiss,"Chen S.-C., Wang X., Rishe N., Weiss M.A.","With the increasing use of geographical data in real-world applications, Geographic Information Systems (GISs) have recently emerged as a fruitful area for research. Nowadays, a GIS can be combined with World Wide Web (WWW) techniques to provide information to a multitude of users. A high-performance web-based GIS, called TerraFly, has been developed in order to provide web-based GIS accesses to the general public. The design of TerraFly considers three major aspects including system architecture, data structures, and networking. The system architecture utilizes the existing resources to achieve maximum performance by using the ""Internally Distributed Multithreading (IDMT)"" technique. A spatial access method, semantic R-trees, is used to search an object based on both spatial and semantic information. System performance results are presented and analyzed. Reducing network traffic to achieve faster response to users is also discussed.",,
Weiss,Weiss M.A.,"This paper describes our experiences incorporating Java in a Data Structures course. We describe the features of Java that made for a more interesting course, the difficulties that we encountered, and compare Java to the prior languages used in this course, Ada and C++. All in all, we found Java to be a reasonable, but not overwhelming better, alternative. Our students were particularly happy with the experiment.",,
Weiss,"Guo S., Sun W., Weiss M.A.","Satisfiability, equivalence, and implication problems involving conjunctive queries are important and widely encountered problems in database management systems. These problems need to be efficiently and effectively solved. In this paper, we consider queries which are conjunctions of the inequalities of the form (X op C), (X op Y), and/or (X op Y + C), where X and Y are two attributes, C is a constant, and op ? {<, ?, =, ?, >, ?}. These types of inequalities are widely used in database systems, since the first type is a selection, the second type is a ?-join, and the third type is a very popular clause in a deductive database system. The satisfiability, equivalence, and implication problems in the integer domain (for attributes and constants) have been shown to be NP-hard [20], [24]. However, we show that these problems can be solved efficiently in the real domain. The incorporation of the real domain is significant, because the real domain is practically and widely used in a database. Necessary and sufficient conditions and algorithms are presented. A novel concept of the ""modulo closure"" and a set of sound and complete axioms with respect to the ""modulo closure"" are also proposed to infer all correct and necessary inequalities from a given query. The proposed axioms generalize Ullman's axioms [27] where queries only consist of ?-joins. ©1996 IEEE.",,
Weiss,Weiss M.A.,"We consider the worst-case running time of Shellsort when only a constant number, c, of increments are allowed. For c = 3, we show that Shellsort can be implemented to run in O (N5/3) time, which is optimal. For c = 4, we further improve the running time to O(N11/17), and, for c = 5, we obtain a bound of O(N23/25). We also show an O(N1+1/k) bound for general c, where k = ?(1+ ?8c + 1)/4?. For c = 6, this is O(N3/2).",,
Weiss,"Guo S., Sun W., Weiss M.A.","Satisfiability, implication, and equivalence problems involving conjunctive inequalities are important and widely encountered database problems that need to be efficiently and effectively processed. In this article we consider two popular types of arithmetic inequalities, (X op Y) and (X op C), where X and Y are attributes, C is a constant of the domain or X, and op ? {<, ?, =, ?, >, ?}. These inequalities are most frequently used in a database system, inasmuch as the former type of inequality represents a ? - join, and the latter is a selection. We study the satisfiability and implication problems under the integer domain and the real domain, as well as under two different operator sets ({<, ?, =, ?, >} and {<, ?, =, ?, ?, >}). Our results show that solutions under different domains and/or different operator sets are quite different. Out of these eight cases, excluding two cases that had been shown to be NP-hard, we either report the first necessary and sufficient conditions for these problems as well as their efficient algorithms with complexity analysis (for four cases), or provide an improved algorithm (for two cases). These iff conditions and algorithms are essential to database designers, practitioners, and researchers. These algorithms have been implemented and an experimental study comparing the proposed algorithms and those previously known is conducted. Our experiments show that the proposed algorithms are more efficient than previously known algorithms even for small input.",,
Weiss,Weiss M.A.,We show that a treap of N items can be constructed in O(N) time if the items (but not necessarily their priorities) are already sorted. The same algorithm gives a linear-time construction of a Cartesian tree whose items are presorted along the x-coordinates. 1994.,,
Weiss,"Ding Y., Weiss M.A.","We provide tight bounds of 2d log n + o(d log n), 3d2 log n + o(d2 log n), and 3 2d2n + o (d2n) on the number of comparisons used for insertion, deletion, and construction of the n item d-dimensional interval heap. 1994.",,
Weiss,"Ding Y., Weiss M.A.","A data structure that implements a mergeable double-ended priority queue, namely the relaxed min-max heap, is presented. A relaxed min-max heap of n items can be constructed in O(n) time. In the worst case, operations find_min() and find_max() can be performed in constant time, while each of the operations merge(), insert(), delete_min(), delete_max(), decrease_key(), and delete_key() can be performed in O(log n) time. Moreover, insert() has O(1) amortized running time. If lazy merging is used, merge() will also have O(1) worst-case and amortized time. The relaxed min-max heap is the first data structure that achieves these bounds using only two pointers (puls one bit) per item. 1993 Springer-Verlag.",,
Weiss,"Ding Y., Weiss M.A.","This paper presents the k-d heap, an efficient data structure that implements a multi-dimensional priority queue. The basic form of the k-d heap uses no extra space, takes linear time to construct, and supports instant access to the items carrying the minimum key of any dimension, as well as logarithmic time insertion, deletion, and modification of any item in the queue. Moreover, it can be extended to a multi-dimensional double-ended mergeable priority queue, capable of efficiently supporting all the operations linked to priority queues. The k-d heap is very easily implemented, and has direct applications. Springer-Verlag Berlin Heidelberg 1993.",,
Weiss,"Orji C.U., Weiss M.A., Solworth J.A.","We propose a scheme for improving write performance in traditional mirrored disks using write caching. The technique uses write-only disk caches to improve write performance; a write through to disk for cached data and a write twice scheme to provide a highly fault tolerant system. The scheme is simple and can be readily integrated into existing systems with only low-level software redesign. Unlike some mirroring techniques, this technique maintains the semantics of traditional mirroring, allowing a disk to disk copy during recovery while improving write efficiency by almost a factor of 2 over traditional mirrors. 1993, Springer Verlag. All rights reserved.",,
Weiss,"Ding Y., Weiss M.A.","The performance of Heapsort algorithms on arbitrary input is examined. It is proved that an n log n-O(n) lower bound on the number of comparisons holds for a set of Heapsort algorithms, including Williams-Floyd's algorithm, Carlsson's bottom-up linear or binary insertion algorithm, and all up-down algorithms, on any input. 1992 Springer-Verlag.",,
Weiss,"Sun Wei, Prabhakaran Nagarajan, Weiss Mark Allen, Zhou Xiao-dong","The authors present an approach to automatically acquire semantic integrity constraints (SICs) based on queries previously processed in an object-oriented database environment. The automatic scheme acquires dynamic SICs from a history of queries and their previously processed answers. The proposed acquisition approach serves as an interactive tool for specifying SICs. Furthermore, even though the acquired constraints do not carry the application semantics and only reflect the snapshot state of the database, they can still be used in applications such as semantic query optimization so that a larger class of queries may be semantically optimized. The proposed approach exploits many distinct object-oriented features.",,
Weiss,Weiss M.A.,"We present the results of a large empirical study of the running time of Shellsort. A previous study reported by Knuth for several increment sequences gave running times between ?(N1.25) and ?(N1.28) or ?(Nlog2 N), but these forms are not very accurate especially for large permutations. Our results give a running time of ?(N5/4) for all of the increment sequences suggested by Knuth and ?(N7/6) for an increment sequence suggested by Sedgewick. Our fits are accurate to within 1% for 250 ? N &lt; 100 000 and about 0.1% for larger N. Much of the error in the fits appears to be related to the error in the measured data. These results are significant because they suggest that O(log N) increment sequences with ?(Nk) worst-case running times have ?(N(k+1)/2) average-case running times and that there is no increment sequence for which Shellsort is O(N log N), even on average.",,
Weiss,"Weiss M.A., Sedgewick R.",It is proved that the running time of Shellsort using an increment sequence given by Sedgewick is ?(N4 3) which matches the known upper bound. Extending this proof technique to various increment sequences leads to lower bounds that in general always match the known upper bounds. This suggests that Shellsort runs in ? (N1+?{lunate}/ log N for increment sequences of practical interest and that no increment sequence exists that would make Shellsort optimal. 1990.,,
Weiss,"Weiss M.A., Navlakha J.K.","We show that the kth smallest element in a large heap is at expected depth ?log k. Simulation results indicate that this bound is tight, and that the variance of the depth is no more than 0.8, independent of k. This leads to a simple algorithm for actually finding the kth smallest element that appears to run in O(k) expected time, which would improve the previous best-known bound of O(klog k). We prove an ?(klog k) lower bound for worst case running time of any algorithm to solve this problem. Springer-Verlag Berlin Heidelberg 1989.",,
Weiss,"Weiss M.A., Sedgewick R.","Shellsort is a simple classic algorithm that runs competitively on both mid-sized and nearly sorted files. It uses an increment sequence, the choice of which can drastically affect the algorithm's running time. Due to the results of Pratt, the running time of Shellsort was long thought to be ?(N3/2) for increment sequences that are “almost geometric”. However, recent results have lowered the upper bound substantially, although the new bounds were not known to be tight. In this paper, we show that an increment sequence given by Sedgewick is ?(N4/3) by analyzing the time required to sort a particularly bad permutation. Extending this proof technique to various increment sequences seems to lead to lower bounds that in general match the known upper bounds and suggests that Shellsort runs in ?(N1 + ?/?log N) for increment sequences of practical interest, and that no increment sequence exists that would make Shellsort optimal. 1988, Springer-Verlag.",,
Xie,"Cheraghchi M., Grigorescu E., Juba B., Wimmer K., Xie N.","AC0?MOD2 circuits are AC0 circuits augmented with a layer of parity gates just above the input layer. We study AC0?MOD2 circuit lower bounds for computing the Boolean Inner Product functions. Recent works by Servedio and Viola (ECCC TR12-144) and Akavia et al. (ITCS 2014) have highlighted this problem as a frontier problem in circuit complexity that arose both as a first step towards solving natural special cases of the matrix rigidity problem and as a candidate for constructing pseudorandom generators of minimal complexity. We give the first superlinear lower bound for the Boolean Inner Product function against AC0?MOD2 of depth four or greater. Specifically, we prove a superlinear lower bound for circuits of arbitrary constant depth, and an ?˜(n2) lower bound for the special case of depth-4 AC0?MOD2. 2018 Elsevier Inc.",,
Xie,"Mokhtari S., Li T., Xie N.","Enabling a computer to precisely understand a document so that it can predict the sentiment is crucial, yet the unsolved goal of NLP! Although, the majority of existing methods classify text documents only based on semantic of text data and ignore relevant contextual information. There are a few studies which take this data into account, but they suffer from model complexity. To address this issue, we propose an attention-based hierarchical neural network model to incorporate user preferences and product characteristics into sentiment classification task. Our model benefits from both character and word level embedding to build a hierarchical representation of the documents content. Afterward, contextual information via attentions mechanism applied over word encoding layer to capture the user- specific meaning of each word. We evaluate our system on two publicly available dataset Yelp and Amazon for fine-grained sentiment classification. Our model shows significant performances over several robust baseline methods. 2018 IEEE.",,
Xie,"Mokhtari S., Li T., Xie N.","Our daily digital life is surrounded by algorith-mically selected contents such as recommendations, reviews, and news feeds. The process of extracting useful information out of large volumes data is imposing great impacts on many aspects of people's life. However, as the data volume becomes huge, users tend to get lost in details and miss the general picture. Indeed, the fast growing online data has required the development of systems that not only accurately distill useful and representative knowledge out of large data corpus but also display the extracted information in an easy-to-understand manner. In this paper, we propose a unified framework which generates visual structured summaries of customer reviews, thus providing a holistic view of a large group of reviews. Our model employs a deep neural network for opinion mining which relies on the character-level inputs. To the best of our knowledge, previously there is no uniform framework that performs visual summarization of consumer opinions as proposed in this paper. 2018 IEEE.",,
Xie,"Xie N., Xu S., Xu Y.","Given n vectors x0, x1, …, xn-1 in {0,1}m, how to find two vectors whose pairwise Hamming distance is minimum? This problem is known as the Closest Pair Problem. If these vectors are generated uniformly at random except two of them are correlated with Pearson-correlation coefficient ?, then the problem is called the Light Bulb Problem. In this work, we propose a novel coding-based scheme for the Close Pair Problem. We design both randomized and deterministic algorithms, which achieve the best known running time when the minimum distance is very small compared to the length of input vectors. When applied to the Light Bulb Problem, our algorithms yields state-of-the-art deterministic running time when the Pearson-correlation coefficient ? is very large. 2018, Springer International Publishing AG, part of Springer Nature.",,
Xie,"Haviv I., Xie N.","A function f:F2n?{0,1} is triangle-free if there are no x1,x2,x3?F2n satisfying x1+ x2+ x3= 0 and f(x1) = f(x2) = f(x3) = 1. In testing triangle-freeness, the goal is to distinguish with high probability triangle-free functions from those that are ?-far from being triangle-free. It was shown by Green that the query complexity of the canonical tester for the problem is upper bounded by a function that depends only on ? (Green 2005); however, the best-known upper bound is a tower-type function of 1 / ?. The best known lower bound on the query complexity of the canonical tester is 1 / ?13.239 (Fu &amp; Kleinberg 2014). In this work we introduce a new approach to proving lower bounds on the query complexity of triangle-freeness. We relate the problem to combinatorial questions on collections of vectors in ZDn and to sunflower conjectures studied by Alon, Shpilka &amp; Umans (2013). The relations yield that a refutation of the Weak Sunflower Conjecture over Z4 implies a super-polynomial lower bound on the query complexity of the canonical tester for triangle-freeness. Our results are extended to testing k-cycle-freeness of functions with domain Fpn for every k? 3 and a prime p. In addition, we generalize the lower bound of Fu and Kleinberg to k-cycle-freeness for k? 4 by generalizing the construction of uniquely solvable puzzles due to Coppersmith &amp; Winograd (1990). 2016, Springer International Publishing.",,
Xie,"Li T., Xie N., Zeng C., Zhou W., Zheng L., Jiang Y., Yang Y., Ha H.-Y., Xue W., Huang Y., Chen S.-C., Navlakha J., Iyengar S.S.","Improving disaster management and recovery techniques is one of national priorities given the huge toll caused by man-made and nature calamities. Data-driven disaster management aims at applying advanced data collection and analysis technologies to achieve more effective and responsive disaster management, and has undergone considerable progress in the last decade. However, to the best of our knowledge, there is currently no work that both summarizes recent progress and suggests future directions for this emerging research area. To remedy this situation, we provide a systematic treatment of the recent developments in data-driven disaster management. Specifically, we first present a general overview of the requirements and system architectures of disaster management systems and then summarize state-of-the-art data-driven techniques that have been applied on improving situation awareness as well as in addressing users' information needs in disaster management. We also discuss and categorize general data-mining and machine-learning techniques in disaster management. Finally, we recommend several research directions for further investigations. 2017 ACM.",,
Xie,"Cheraghchi M., Grigorescu E., Juba B., Wimmer K., Xie N.","AC0 o MOD2 circuits are AC0 circuits augmented with a layer of parity gates just above the input layer. We study AC0 o MOD2 circuit lower bounds for computing the Boolean Inner Product functions. Recent works by Servedio and Viola (ECCC TR12-144) and Akavia et al. (ITCS 2014) have highlighted this problem as a frontier problem in circuit complexity that arose both as a first step towards solving natural special cases of the matrix rigidity problem and as a candidate for constructing pseudorandom generators of minimal complexity. We give the first superlinear lower bound for the Boolean Inner Product function against AC0 o MOD2 of depth four or greater. Specifically, we prove a superlinear lower bound for circuits of arbitrary constant depth, and an ?(n2) lower bound for the special case of depth-4 AC0 o MOD2. Our proof of the depth-4 lower bound employs a new ""moment-matching"" inequality for bounded, nonnegative integer-valued random variables that may be of independent interest: we prove an optimal bound on the maximum difference between two discrete distributions' values at 0, given that their first d moments match.",,
Xie,"Tsang H.Y., Xie N., Zhang S.","We study a conjecture called ""linear rank conjecture"" recently raised in (Tsang et al. [16]), which asserts that if many linear constraints are required to lower the degree of a GF(2) polynomial, then the Fourier sparsity (i.e. number of non-zero Fourier coefficients) of the polynomial must be large. We notice that the conjecture implies a surprising phenomenon that, if the highest degree monomials of a GF(2) polynomial satisfy a certain condition (Specifically, the highest degree monomials do not vanish under a small number of linear restrictions.), then the Fourier sparsity of the polynomial is large regardless of the monomials of lower degrees-whose number is generally much larger than that of the highest degree monomials. We develop a new technique for proving lower bound on the Fourier sparsity of GF(2) polynomials, and apply it to certain special classes of polynomials to showcase the above phenomenon (A full version of this paper is available at http://arxiv. org/abs/1508.02158). Springer International Publishing Switzerland 2016.",,
Xie,"Haviv I., Xie N.","A function f : F2n ? (0; 1) is triangle-free if there are no X1, x2, x3 F2n satisfying x1 + x2 + x3 = 0 and f(x1) = f(x2) = f(x3) = 1. In testing triangle-freeness, the goal is to distinguish with high probability triangle-free functions from those that are ?-far from being triangle-free. It was shown by Green that the query complexity of the canonical tester for the problem is upper bounded by a function that depends only on ? (GAFA, 2005), however the best known upper bound is a tower type function of 1/?. The best known lower bound on the query complexity of the canonical tester is 1/?13:239 (Fu and Kleinberg, RANDOM, 2014). In this work we introduce a new approach to proving lower bounds on the query complexity of triangle-freeness. We relate the problem to combinatorial questions on collections of vectors in ZDn and to sun flower conjectures studied by Alon, Shpilka, and Umans (Comput. Complex., 2013). The relations yield that a refutation of the Weak Sunower Conjecture over Z4 implies a super-polynomial lower bound on the query complexity of the canonical tester for trianglefreeness. Our results are extended to testing k-cycle-freeness of functions with domain Fpn for every k ? 3 and a prime p. In addition, we generalize the lower bound of Fu and Kleinberg to k-cycle-freeness for k ? 4 by generalizing the construction of uniquely solvable puzzles due to Coppersmith and Winograd (J. Symbolic Comput., 1990). Copyright 2015 ACM.",,
Xie,"Bhattacharyya A., Xie N.","Given a Boolean function (Formula presented.), we say a triple (x, y, x + y) is a triangle in f if (Formula presented.). A triangle-free function contains no triangle. If f differs from every triangle-free function on at least (Formula presented.) points, then f is said to be (Formula presented.)-far from triangle-free. In this work, we analyze the query complexity of testers that, with constant probability, distinguish triangle-free functions from those (Formula presented.)-far from triangle-free. Let the canonical tester for triangle-freeness denotes the algorithm that repeatedly picks x and y uniformly and independently at random from (Formula presented.), queries f(x), f(y) and f(x + y), and checks whether f(x) = f(y) = f(x + y) = 1. Green showed that the canonical tester rejects functions (Formula presented.)-far from triangle-free with constant probability if its query complexity is a tower of 2’s whose height is polynomial in (Formula presented.). Fox later improved the height of the tower in Green’s upper bound to (Formula presented.). A trivial lower bound of (Formula presented.) on the query complexity is immediate. In this paper, we give the first non-trivial lower bound for the number of queries needed. We show that, for every small enough (Formula presented.), there exists an integer (Formula presented.) such that for all (Formula presented.) there exists a function (Formula presented.) depending on all n variables which is (Formula presented.)-far from being triangle-free and requires (Formula presented.) queries for the canonical tester. We also show that the query complexity of any general (possibly adaptive) one-sided tester for triangle-freeness is at least square root of the query complexity of the corresponding canonical tester. Consequently, this means that any one-sided tester for triangle-freeness must make at least (Formula presented.) queries. 2014, Springer Basel.",,
Xie,"Zhou W., Shen C., Li T., Chen S.-C., Xie N.","Hurricane Sandy affected the east coast of U.S. in 2012 and posed immense threats to businesses, human lives and properties. In order to minimize the consequent loss of a catastrophe like this, a critical task in disaster management is to understand situation updates about the disaster from a large number of disaster-related documents, and obtain a big picture of the disaster's trends and how it affects different areas. In this paper, we present a two-layer storyline generation framework which generates an overall or a global storyline of the disaster events in the first layer, and provides condensed information about specific regions affected by the disaster (i.e., a location-specific storyline) in the second layer. To generate the overall storyline of a disaster, we consider both temporal and spatial factors, which are encoded using integer linear programming. While for location-specific storylines, we employ a Steiner tree based method. Compared with the previous work of storyline generation, which generates flat storylines without considering spatial information, our framework is more suitable for large-scale disaster events. We further demonstrate the efficacy of our proposed framework through the evaluation on the datasets of three major hurricane disasters. 2014 IEEE.",,
Xie,"Zhou W., Shen C., Li T., Chen S.-C., Xie N., Wei J.","People are attracted to large cities because of more employment opportunities, convenient facilities, and rich cultural activities. However, large cities are also more vulnerable to natural disasters, which have caused widespread physical destructions, great loss of life and property, and immense havoc. 'Which city is less susceptible to natural disasters?' is thus one of the most critical questions one faces when making decisions on travelling or job and business relocation. In this work, we propose a bipartite-graph based framework to compare the impacts of disasters on two cities by answering different queries using textual documents collected online. Besides intuitive simple comparison using statistics, our system also generates textual comparative summaries to better describe the differences between the two cities in terms of safety. Although a number of online services provide disaster events statistic information for cities, our framework compares the impacts of disasters on cities in a more straightforward and comprehensive way. 2014 IEEE.",,
Xie,"Tsang H.Y., Wong C.H., Xie N., Zhang S.","We study Boolean functions with sparse Fourier spectrum or small spectral norm, and show their applications to the Log-rank Conjecture for XOR functions f(x y) - A fairly large class of functions including well studied ones such as Equality and Hamming Distance. The rank of the communication matrix Mf for such functions is exactly the Fourier sparsity of f. Let d = deg2(f) be the 2-degree of f and DCC(f.) stand for the deterministic communication complexity for f(x y). We show that 1) DCC(f.) = O(2d2/2 logd-2 ? f1). In particular, the Log-rank conjecture holds for XOR functions with constant 2-degree. 2) DCC(f.) = O(d?? f?1) = ?O(?rank(Mf )). This improves the (trivial) linear bound by nearly a quadratic factor. We obtain our results through a degree-reduction protocol based on a variant of polynomial rank, and actually conjecture that the communication cost of our protocol is at most log O(1) rank(Mf ). The above bounds are obtained from different analysis for the number of parity queries required to reduce f's 2-degree. Our bounds also hold for the parity decision tree complexity of f, a measure that is no less than the communication complexity. Along the way we also prove several structural results about Boolean functions with small Fourier sparsity ? f??0 or spectral norm ? f? ? 1, which could be of independent interest. For functions f with constant F2-degree, we show that: 1) f can be written as the summation of quasi-polynomially many indicator functions of subspaces with ±-signs, improving the previous doubly exponential upper bound by Green and Sanders; 2) being sparse in Fourier domain is polynomially equivalent to having a small parity decision tree complexity; and 3) f depends only on polylogf?1 linear functions of input variables. For functions f with small spectral norm, we show that: 1) there is an affine subspace of co-dimension O(? f??1) on which f(x) is a constant, and 2) there is a parity decision tree of depth O(? f?1 log f?? 0) for computing f. Copyright 2013 by The Institute of Electrical and Electronics Engineers, Inc.",,
Xie,"Grigorescu E., Wimmer K., Xie N.","We study lower bounds for testing membership in families of linear/affine-invariant Boolean functions over the hypercube. Motivated by the recent resurgence of attention to the permutation isomorphism problem, we first focus on families that are linearly/affinely isomorphic to some fixed function. Our main result is a tight adaptive, two-sided ?(n2) lower bound for testing linear isomorphism to the inner-product function. This is the first lower bound for testing linear isomorphism to a specific function that matches the trivial upper bound. Our proof exploits the elegant connection between testing and communication complexity discovered by Blais et al. (Computational Complexity, 2012.) Our second result shows an ?(2 n/4) query lower bound for any adaptive, two-sided tester for membership in the Maiorana-McFarland class of bent functions. This class of Boolean functions is also affine-invariant and its rich structure and pseudorandom properties have been well-studied in mathematics, coding theory and cryptography. 2013 Springer-Verlag.",,
Xie,"Rubinfeld R., Xie N.","A discrete distribution D over ?1 _··· _?n is called (non-uniform) k -wise independent if for any subset of k indices {i1,...,ik} and for any z1??i1,...,zk??ik, PrX~D[Xi1···Xik = z1···zk] = PrX~D[Xi1 = z1]···PrX~D[Xik = zk]. We study the problem of testing (non-uniform) k -wise independent distributions over product spaces. For the uniform case we show an upper bound on the distance between a distribution D from k -wise independent distributions in terms of the sum of Fourier coefficients of D at vectors of weight at most k. Such a bound was previously known only when the underlying domain is {0,1}n. For the non-uniform case, we give a new characterization of distributions being k -wise independent and further show that such a characterization is robust based on our results for the uniform case. These results greatly generalize those of Alon et al. (STOC'07, pp. 496-505) on uniform k -wise independence over the Boolean cubes to non-uniform k -wise independence over product spaces. Our results yield natural testing algorithms for k -wise independence with time and sample complexity sublinear in terms of the support size of the distribution when k is a constant. The main technical tools employed include discrete Fourier transform and the theory of linear systems of congruences.2012 Wiley Periodicals, Inc. Random Struct. Alg., 2013 Copyright 2012 Wiley Periodicals, Inc..",,
Xie,"Mansour Y., Rubinstein A., Vardi S., Xie N.","We propose a general method for converting online algorithms to local computation algorithms, by selecting a random permutation of the input, and simulating running the online algorithm. We bound the number of steps of the algorithm using a query tree, which models the dependencies between queries. We improve previous analyses of query trees on graphs of bounded degree, and extend this improved analysis to the cases where the degrees are distributed binomially, and to a special case of bipartite graphs. Using this method, we give a local computation algorithm for maximal matching in graphs of bounded degree, which runs in time and space O(log3 n). We also show how to convert a large family of load balancing algorithms (related to balls and bins problems) to local computation algorithms. This gives several local load balancing algorithms which achieve the same approximation ratios as the online algorithms, but run in O(logn) time and space. Finally, we modify existing local computation algorithms for hypergraph 2-coloring and k-CNF and use our improved analysis to obtain better time and space bounds, of O(log4 n), removing the dependency on the maximal degree of the graph from the exponent. 2012 Springer-Verlag.",,
Xie,"Alon N., Rubinfeld R., Vardi S., Xie N.","Recently Rubinfeld et al. (ICS 2011, pp. 223-238) proposed a new model of sublinear algorithms called local computation algorithms. In this model, a computation problem F may have more than one legal solution and each of them consists of many bits. The local computation algorithm for F should answer in an online fashion, for any index i, the i th bit of some legal solution of F. Further, all the answers given by the algorithm should be consistent with at least one solution of F. In this work, we continue the study of local computation algorithms. In particular, we develop a technique which under certain conditions can be applied to construct local computation algorithms that run not only in polylogarithmic time but also in polylogarithmic space. Moreover, these local computation algorithms are easily parallelizable and can answer all parallel queries consistently. Our main technical tools are pseudorandom numbers with bounded independence and the theory of branching processes. Copyright SIAM.",,
Xie,"Bhattacharyya A., Chen V., Sudan M., Xie N.","The rich collection of successes in property testing raises a natural question: Why are so many different properties turning out to be locally testable? Are there some broad ""features"" of properties that make them testable? Kaufman and Sudan (STOC 2008) proposed the study of the relationship between the invariances satisfied by a property and its testability. Particularly, they studied properties that were invariant under linear transformations of the domain and gave a characterization of testability in certain settings. However, the properties that they examined were also linear. This led us to investigate linear-invariant properties that are not necessarily linear. Here we describe some of the resulting works which consider natural linear-invariant properties, specifically properties that are described by forbidden patterns of values that a function can take, and show testability under various settings. 2010 Springer-Verlag.",,
Xie,"Rubinfeld R., Xie N.","A distribution D over ?1_?_? n is called (non-uniform) k-wise independent if for any set of k indices {i 1, ..., i k } and for any , . We study the problem of testing (non-uniform) k-wise independent distributions over product spaces. For the uniform case we show an upper bound on the distance between a distribution D from the set of k-wise independent distributions in terms of the sum of Fourier coefficients of D at vectors of weight at most k. Such a bound was previously known only for the binary field. For the non-uniform case, we give a new characterization of distributions being k-wise independent and further show that such a characterization is robust. These greatly generalize the results of Alon et al. [1] on uniform k-wise independence over the binary field to non-uniform k-wise independence over product spaces. Our results yield natural testing algorithms for k-wise independence with time and sample complexity sublinear in terms of the support size when k is a constant. The main technical tools employed include discrete Fourier transforms and the theory of linear systems of congruences. 2010 Springer-Verlag Berlin Heidelberg.",,
Xie,"Bhattacharyya A., Xie N.","Let f1, f2, f3 : F2n ? {0,1} be three Boolean functions. We say a triple (x, y, x + y) is a triangle in the function-triple (f1, f2, f3) if f1(x) = f2(y) = f3 (x + y) = 1. (f 1, f2, f3) is said to be triangle-free if there is no triangle in the triple. The distance between a function-triple and triangle-freeness is the minimum fraction of function values one needs to modify in order to make the function-triple triangle-free. A canonical tester for triangle-freeness repeatedly picks x and y uniformly and independently at random and checks if f1(x) = f2 (y) = f3(x + y) = 1. Based on an algebraic regularity lemma, Green showed that the number of queries for the canonical testing algorithm is upper-bounded by a tower of 2's whose height is polynomial in 1/?. A trivial query complexity lower bound of ?(1/?) is straightforward to show. In this paper, we give the first non-trivial query complexity lower bound for testing triangle-freeness in Boolean functions. We show that, for every small enough ? there exists an integer no(?) such that for all n ? no there exists a function-triple f1, f2, f3 : F 2n ? {0, 1} depending on all the n variables which is ?-far from being triangle-free and requires (1/?) 4.47? queries for the canonical tester. For the single function case that f1 = f2 = f3, we obtain a weaker lower bound of (1/?)3.409?. We also show that the query complexity of any general (possibly adaptive) one-sided tester for triangle-freeness is at least square-root of the query complexity of the corresponding canonical tester. Consequently, this yields (1/?) 2.423? and (1/?)1.704? query complexity lower bounds for multi-function and single-function triangle-freeness respectively, with respect to general one-sided testers. Copyright by SIAM.",,
Xie,"Bhattacharyya A., Chen V., Sudan M., Xie N.","We consider the task of testing properties of Boolean functions that are invariant under linear transformations of the Boolean cube. Previous work in property testing, including the linearity test and the test for Reed-Muller codes, has mostly focused on such tasks for linear properties. The one exception is a test due to Green for ""triangle freeness"": A function f: F 2n ? F2 satisfies this property if f(x),f(y),f(x + y) do not all equal 1, for any pair x,y ? F 2n. Here we extend this test to a more systematic study of testing for linear-invariant nonlinear properties. We consider properties that are described by a single forbidden pattern (and its linear transformations), i.e., a property is given by k points v1,?,vk ? F2k and f: F2n ? F 2 satisfies the property that if for all linear maps L: F 2k ? F2n it is the case that f(L(v1)),?,f(L(vk)) do not all equal 1. We show that this property is testable if the underlying matroid specified by v 1,?,vk is a graphic matroid. This extends Green's result to an infinite class of new properties. Our techniques extend those of Green and in particular we establish a link between the notion of ""1-complexity linear systems"" of Green and Tao, and graphic matroids, to derive the results. A. Bhattacharyya, V. Chen, M. Sudan, and N. Xie.",,
Xie,"Kaufman T., Litsyn S., Xie N.","For Boolean functions that are ?-away from the set of linear functions, we study the lower bound on the rejection probability (denoted by rej(?)) of the linearity test suggested by Blum, Luby and Rubinfeld. This problem is one of the most extensively studied problems in property testing of Boolean functions. The previously best bounds for rej(?) were obtained by Bellare, Coppersmith, Håstad, Kiwi and Sudan. They used Fourier analysis to show that REJ(?) ? ? for every 0 ? ? ? 1/2. They also conjectured that this bound might not be tight for ?'s that are close to 1/2. In this paper we show that this indeed is the case. Specifically, we improve the lower bound of REJ(?) ? ? by an additive term that depends only on ?: REJ(?)? ?+min {1376?3(1 - 2?)12, 1/4 ? (1 - 2 ?)4}, for every 0 ? ? ? 1/2. Our analysis is based on a relationship between REJ(?) and the weight distribution of a coset of the Hadamard code. We use both Fourier analysis and coding theory tools to estimate this weight distribution. 2008 Springer-Verlag Berlin Heidelberg.",,
Xie,"Alon N., Andoni A., Kaufman T., Matulef K., Rubinfeld R., Xie N.","In this work, we consider the problems of testing whether adistribution over (0,1 n) is k-wise (resp. (,k)-wise) independentusing samples drawn from that distribution. For the problem of distinguishing k-wise independent distributions from those that are -far from k-wise independence in statistical distance, we upper bound the number ofrequired samples by (n k/ 2) and lower bound it by (n k-1/2/) (these bounds hold for constantk, and essentially the same bounds hold for general k). Toachieve these bounds, we use Fourier analysis to relate adistribution's distance from k-wise independence to its biases, a measure of the parity imbalance it induces on a setof variables. The relationships we derive are tighter than previouslyknown, and may be of independent interest. To distinguish (,k)-wise independent distributions from thosethat are -far from (,k)-wise independence in statistical distance, we upper bound thenumber of required samples by O(k log n / 22) and lower bound it by ( k log n / 2 k(+) log 1/2 k(+)). Although these bounds are anexponential improvement (in terms of n and k) over thecorresponding bounds for testing k-wise independence, we give evidence thatthe time complexity of testing (,k)-wise independence isunlikely to be poly(n,1/,1/) for k=(log n),since this would disprove a plausible conjecture concerning the hardness offinding hidden cliques in random graphs. Under the conjecture, ourresult implies that for, say, k = log n and = 1 / n 0.99,there is a set of (,k)-wise independent distributions, and a set of distributions at distance =1/n 0.51 from (,k)-wiseindependence, which are indistinguishable by polynomial time algorithms. Copyright 2007 ACM.",,
Zeng,"Nadeem S., Su Z., Zeng W., Kaufman A., Gu X.","This work presents a novel framework for spherical mesh parameterization. An efficient angle-preserving spherical parameterization algorithm is introduced, which is based on dynamic Yamabe flow and the conformal welding method with solid theoretic foundation. An area-preserving spherical parameterization is also discussed, which is based on discrete optimal mass transport theory. Furthermore, a spherical parameterization algorithm, which is based on the polar decomposition method, balancing angle distortion and area distortion is presented. The algorithms are tested on 3D geometric data and the experiments demonstrate the efficiency and efficacy of the proposed methods. 1995-2012 IEEE.",,
Zeng,"Shi R., Zeng W., Su Z., Jiang J., Damasio H., Lu Z., Wang Y., Yau S.-T., Gu X.","Automatic computation of surface correspondence via harmonic map is an active research field in computer vision, computer graphics and computational geometry. It may help document and understand physical and biological phenomena and also has broad applications in biometrics, medical imaging and motion capture industries. Although numerous studies have been devoted to harmonic map research, limited progress has been made to compute a diffeomorphic harmonic map on general topology surfaces with landmark constraints. This work conquers this problem by changing the Riemannian metric on the target surface to a hyperbolic metric so that the harmonic mapping is guaranteed to be a diffeomorphism under landmark constraints. The computational algorithms are based on Ricci flow and nonlinear heat diffusion methods. The approach is general and robust. We employ our algorithm to study the constrained surface registration problem which applies to both computer vision and medical imaging applications. Experimental results demonstrate that, by changing the Riemannian metric, the registrations are always diffeomorphic and achieve relatively high performance when evaluated with some popular surface registration evaluation standards. 2016 IEEE.",,
Zeng,"Shahid A.R., Jeukeng L., Zeng W., Pissinou N., Iyengar S.S., Sahni S., Varela-Conover M.","The proliferation of GPS-enabled mobile devices has made location-based services (LBSs) very popular in mobile networks and protecting user's location information has become a critical issue of it. In a typical location-based service framework, the LBS requires the user to provide precise location information to assure better Quality of Services(QoS), while the user wants to hide this information as much as possible. This dilemma of privacy and quality of service trade-off has been studied in literature extensively. In this paper, we propose a spatial obfuscation framework, Privacy Preserving Voronoi Cell (PPVC) based on Voronoi diagram. In this framework user's region of interest (ROI) is divided into n sectors and each sector is concealed with an irregular quadrilateral, generated from a n-gon Voronoi cell and user's location is confounded within a convex region called Anonymity Zone. We evaluate PPVC with simulated environment, showing the efficiency and efficacy of the proposed framework. Finally, we implement it on Android phone with a user centric privacy scale. 2017 IEEE.",,
Zeng,"Zeng W., Yang Y.-J., Razib M.","This work presents an efficient method to compute the registration between surfaces with consistent graph constraints based on Tutte graph embedding. Most natural objects have consistent anatomical structures, extracted as i-somorphic feature graphs. For genus zero surfaces with ? 1 boundaries, the graphs are planar and usually 3-connected. By using Tutte embedding, each feature graph is embedded as a convex subdivision of a planar convex domain. Using the convex subdivision as constraint, surfaces are mapped onto convex subdivision domains and the registration is then computed over them. The computation is based on constrained harmonic maps to minimize the stretching energy, where curvy graph constraints become linear ones. This method is theoretically rigorous. The algorithm solves sparse linear systems and is computationally efficient and robust. The resulting mappings are proved to be unique and diffeomorphic. Experiments on various facial surface data demonstrate its efficiency and practicality. 2016 IEEE.",,
Zeng,"Yang Y.-J., Zeng W., Meng X.-X.","The conformality of freeform surfaces highly affects their rendering and tessellation results. To improve the conformality of freeform surfaces, a novel freeform surface representation named hierarchical freeform surfaces is presented in this paper. The conformality energy of hierarchical freeform surfaces is first formulated and its numerical approximation is then constructed using the composite Simpson's rule. By constructing the parameterization of the initial freeform transformation using the Ricci flow method, the optimal freeform transformation is obtained by the Levenberg_Marquardt method, which is further interleaved with the freeform refinement procedure to generate a hierarchical freeform surface with bounded conformality deviations. Examples are given to show the performance of our algorithm for rendering and tessellation applications. 2016 Elsevier Ltd",,
Zeng,"Li S., Zeng W., Zhou D., Gu X., Gao J.","Motivated by mobile sensor networks as in participatory sensing applications, we are interested in developing a practical, lightweight solution for routing in a mobile network. While greedy routing is robust to mobility, it may get stuck in a local minimum, which then requires non-Trivial recovery methods. We find an embedding of the network such that greedy routing using the virtual coordinates guarantees delivery, thus eliminating the necessity of any recovery methods. Our contribution is to replace the in-network computation of the embedding by a preprocessing of the domain before network deployment and encode the map of network domain to virtual coordinate space by using a small number of parameters which can be preloaded to all sensor nodes. As a result, the map is only dependent on the network domain and is independent of the network connectivity. Each node can directly compute or update its virtual coordinates by applying the locally stored map on its geographical coordinates. This represents the first practical solution for using virtual coordinates for greedy routing in a sensor network and could be easily extended to the case of a mobile network. The paper describes algorithmic innovations as well as implementations on a real testbed. 2002-2012 IEEE.",,
Zeng,"Su Z., Wang Y., Shi R., Zeng W., Sun J., Luo F., Gu X.","Surface based 3D shape analysis plays a fundamental role in computer vision and medical imaging. This work proposes to use optimal mass transport map for shape matching and comparison, focusing on two important applications including surface registration and shape space. The computation of the optimal mass transport map is based on Monge-Brenier theory, in comparison to the conventional method based on Monge-Kantorovich theory, this method significantly improves the efficiency by reducing computational complexity from O(n2) to O(n). For surface registration problem, one commonly used approach is to use conformal map to convert the shapes into some canonical space. Although conformal mappings have small angle distortions, they may introduce large area distortions which are likely to cause numerical instability thus resulting failures of shape analysis. This work proposes to compose the conformal map with the optimal mass transport map to get the unique area-preserving map, which is intrinsic to the Riemannian metric, unique, and diffeomorphic. For shape space study, this work introduces a novel Riemannian framework, Conformal Wasserstein Shape Space, by combing conformal geometry and optimal mass transport theory. In our work, all metric surfaces with the disk topology are mapped to the unit planar disk by a conformal mapping, which pushes the area element on the surface to a probability measure on the disk. The optimal mass transport provides a map from the shape space of all topological disks with metrics to the Wasserstein space of the disk and the pullback Wasserstein metric equips the shape space with a Riemannian metric. We validate our work by numerous experiments and comparisons with prior approaches and the experimental results demonstrate the efficiency and efficacy of our proposed approach. 2015 IEEE.",,
Zeng,"Zeng W., Razib M., Shahid A.B.","Conventional splines offer powerful means for modeling surfaces and volumes in three-dimensional Euclidean space. A one-dimensional quaternion spline has been applied for animation purpose, where the splines are defined to model a one-dimensional submanifold in the three-dimensional Lie group. Given two surfaces, all of the diffeomorphisms between them form an infinite dimensional manifold, the so-called diffeomorphism space. In this work, we propose a novel scheme to model finite dimensional submanifolds in the diffeomorphism space by generalizing conventional splines. According to quasiconformal geometry theorem, each diffeomorphism determines a Beltrami differential on the source surface. Inversely, the diffeomorphism is determined by its Beltrami differential with normalization conditions. Therefore, the diffeomorphism space has one-to-one correspondence to the space of a special differential form. The convex combination of Beltrami differentials is still a Beltrami differential. Therefore, the conventional spline scheme can be generalized to the Beltrami differential space and, consequently, to the diffeomorphism space. Our experiments demonstrate the efficiency and efficacy of diffeomorphism splines. The diffeomorphism spline has many potential applications, such as surface registration, tracking and animation. 2015 by the author.",,
Zeng,"Yang Y.-J., Zeng W.","The equiareality of NURBS surfaces greatly affects the results of visualization and tessellation applications, especially when dealing with extruding and intruding shapes. To obtain more equiareal parameterizations of NURBS surfaces than the M_bius based method, an optimization algorithm is presented in this paper based on the more flexible composite reparameterization functions. For fixed knots, the optimal composite reparameterization can be obtained using the Levenberg-Marquardt method. For a given tolerance, a uniform subdivision scheme is interleaved with the optimization procedure and this process finishes until the change of the equiareal deviation energy is less than the given tolerance. Examples are given to show the performance of our algorithm for visualization and tessellation applications. 2014 Elsevier B.V.",,
Zeng,"Zhang M., Zeng W., Guo R., Luo F., Gu X.D.","Ricci flow deforms the Riemannian metric proportionally to the curvature, such that the curvature evolves according to a nonlinear heat diffusion process, and becomes constant eventually. Ricci flow is a powerful computational tool to design Riemannian metrics by prescribed curvatures. Surface Ricci flow has been generalized to the discrete setting. This work surveys the theory of discrete surface Ricci flow, its computational algorithms, and the applications for surface registration and shape analysis. 2015, Springer Science+Business Media New York.",,
Zeng,"Yang Y.-J., Zeng W., Song T.-Q.","The conformality of NURBS surfaces greatly affects the results of rendering and tessellation applications. To improve the conformality of NURBS surfaces, an optimization algorithm using general bilinear transformations is presented in this paper. The conformality energy is first formulated and its numerical approximation is then constructed using the composite simpson's rule. The initial general bilinear transformation is obtained by approximating the conformal mapping of its 3D discretized mesh using a least square method, which is further optimized by the Levenberg-Marquardt method. Examples are given to show the performance of our algorithm for rendering and tessellation applications. 2015 Elsevier Ltd.",,
Zeng,"Razib M., Lu Z.-L., Zeng W.","Brain mapping plays an important role in neuroscience and medical imaging fields, which flattens the convoluted brain cortical surface and exposes the hidden geometry details onto a canonical domain. Existing methods such as conformal mappings didnêt consider the anatomical atlas network structure, and the anatomical landmarks, e.g., gyri curves, appear highly curvy on the canonical domains. Using such maps, it is difficult to recognize the connecting pattern and compare the atlases. In this work, we present a novel brain mapping method to efficiently visualize the convoluted and partially invisible cortical surface through a well-structured view, called the structural brain mapping. In computation, the brain atlas network (–node” - the junction of anatomical cortical regions, –edge” - the connecting curve between cortical regions) is first mapped to a planar straight line graph based on Tutte graph embedding, where all the edges are crossing-free and all the faces are convex polygons; the brain surface is then mapped to the convex shape domain based on harmonic map with linear constraints. Experiments on two brain MRI databases, including 250 scans with automatic atlases processed by FreeSurfer and 40 scans with manual atlases from LPBA40, demonstrate the efficiency and efficacy of the algorithm and the practicability for visualizing and comparing brain cortical anatomical structures. Springer International Publishing Switzerland 2015.",,
Zeng,"Su Z., Zeng W., Wang Y., Lu Z.-L., Gu X.","Brain morphometry study plays a fundamental role in medical imaging analysis and diagnosis. This work proposes a novel framework for brain cortical surface classification using Wasserstein distance, based on uniformization theory and Riemannian optimal mass transport theory. By Poincare uniformization theorem, all shapes can be conformally deformed to one of the three canonical spaces: the unit sphere, the Euclidean plane or the hyperbolic plane. The uniformization map will distort the surface area elements. The area-distortion factor gives a probability measure on the canonical uniformization space. All the probability measures on a Riemannian manifold form the Wasserstein space. Given any 2 probability measures, there is a unique optimal mass transport map between them, the transportation cost defines the Wasserstein distance between them. Wasserstein distance gives a Riemannian metric for the Wasserstein space. It intrinsically measures the dissimilarities between shapes and thus has the potential for shape classification. To the best of our knowledge, this is the first work to introduce the optimal mass transport map to general Riemannian manifolds. The method is based on geodesic power Voronoi diagram. Comparing to the conventional methods, our approach solely depends on Riemannian metrics and is invariant under rigid motions and scalings, thus it intrinsically measures shape distance. Experimental results on classifying brain cortical surfaces with different intelligence quotients demonstrated the efficiency and efficacy of our method. Springer International Publishing Switzerland 2015.",,
Zeng,"Su Z., Zeng W., Wang Y., Lu Z.-L., Gu X.","Brain morphometry study plays a fundamental role in medical imaging analysis and diagnosis. This work proposes a novel framework for brain cortical surface classification using Wasserstein distance, based on uniformization theory and Riemannian optimal mass transport theory. By Poincare uniformization theorem, all shapes can be conformally deformed to one of the three canonical spaces: the unit sphere, the Euclidean plane or the hyperbolic plane. The uniformization map will distort the surface area elements. The area-distortion factor gives a probability measure on the canonical uniformization space. All the probability measures on a Riemannian manifold form the Wasserstein space. Given any 2 probability measures, there is a unique optimal mass transport map between them, the transportation cost defines the Wasserstein distance between them. Wasserstein distance gives a Riemannian metric for the Wasserstein space. It intrinsically measures the dissimilarities between shapes and thus has the potential for shape classification. To the best of our knowledge, this is the first. work to introduce the optimal mass transport map to general Riemannian manifolds. The method is based on geodesic power Voronoi diagram. Comparing to the conventional methods, our approach solely depends on Riemannian metrics and is invariant under rigid motions and scalings, thus it intrinsically measures shape distance. Experimental results on classifying brain cortical surfaces with different intelligence quotients demonstrated the efficiency and efficacy of our method.",,
Zeng,"Zeng W., Yang Y.-J.","In virtual colonoscopy, colon conformal flattening plays an important role, which unfolds the colon wall surface to a rectangle planar image and preserves local shapes by conformal mapping, so that the cancerous polyps and other abnormalities can be easily and thoroughly recognized and visualized without missing hidden areas. In such maps, the anatomical landmarks (taeniae coli, flexures, and haustral folds) are naturally mapped to convoluted curves on 2D domain, which poses difficulty for comparing shapes from geometric feature details. Understanding the nature of landmark curves to the whole surface structure is meaningful but it remains challenging and open. In this work, we present a novel and effective colon flattening method based on quasiconformal mapping, which straightens the main anatomical landmark curves with least conformality (angle) distortion. It provides a canonical and straightforward view of the long, convoluted and folded tubular colon surface. The computation is based on the holomorphic 1-form method with landmark straightening constraints and quasiconformal optimization, and has linear time complexity due to the linearity of 1-forms in each iteration. Experiments on various colon data demonstrate the efficiency and efficacy of our algorithm and its practicability for polyp detection and findings visualization; furthermore, the result reveals the geometric characteristics of anatomical landmarks on colon surfaces.",,
Zeng,"Zeng W., Yang Y.-J.","In virtual colonoscopy, colon conformal flattening plays an important role, which unfolds the colon wall surface to a rectangle planar image and preserves local shapes by conformal mapping, so that the cancerous polyps and other abnormalities can be easily and thoroughly recognized and visualized without missing hidden areas. In such maps, the anatomical landmarks (taeniae coli, flexures, and haustral folds) are naturally mapped to convoluted curves on 2D domain, which poses difficulty for comparing shapes from geometric feature details. Understanding the nature of landmark curves to the whole surface structure is meaningful but it remains challenging and open. In this work, we present a novel and effective colon flattening method based on quasiconformal mapping, which straightens the main anatomical landmark curves with least conformality (angle) distortion. It provides a canonical and straightforward view of the long, convoluted and folded tubular colon surface. The computation is based on the holomorphic 1-form method with landmark straightening constraints and quasiconformal optimization, and has linear time complexity due to the linearity of 1-forms in each iteration. Experiments on various colon data demonstrate the efficiency and efficacy of our algorithm and its practicability for polyp detection and findings visualization; furthermore, the result reveals the geometric characteristics of anatomical landmarks on colon surfaces. 2014 Springer International Publishing.",,
Zeng,"Zeng W., Yang Y.-J.","This work presents a novel surface matching and registration method based on the landmark curve-driven canonical surface quasiconformal mapping, where an open genus zero surface decorated with landmark curves is mapped to a canonical domain with horizontal or vertical straight segments and the local shapes are preserved as much as possible. The key idea of the canonical mapping is to minimize the harmonic energy with the landmark curve straightening constraints and generate a quasi-holomorphic 1-form which is zero in one parameter along landmark and results in a quasiconformal mapping. The mapping exists and is unique and intrinsic to surface and landmark geometry. The novel shape representation provides a conformal invariant shape signature. We use it as Teichmôller coordinates to construct a subspace of the conventional Teichmôller space which considers geometry feature details and therefore increases the discriminative ability for matching. Furthermore, we present a novel and efficient registration method for surfaces with landmark curve constraints by computing an optimal mapping over the canonical domains with straight segments, where the curve constraints become linear forms. Due to the linearity of 1-form and harmonic map, the algorithms are easy to compute, efficient and practical. Experiments on human face and brain surfaces demonstrate the efficiency and efficacy and the potential for broader shape analysis applications. 2014 Springer International Publishing.",,
Zeng,"Lui L.M., Zeng W., Yau S.-T., Gu X.","Shape analysis is a central problem in the field of computer vision. In 2D shape analysis, classification and recognition of objects from their observed silhouettes are extremely crucial but difficult. It usually involves an efficient representation of 2D shape space with a metric, so that its mathematical structure can be used for further analysis. Although the study of 2D simply-connected shapes has been subject to a corpus of literatures, the analysis of multiply-connected shapes is comparatively less studied. In this work, we propose a representation for general 2D multiply-connected domains with arbitrary topologies using conformal welding. A metric can be defined on the proposed representation space, which gives a metric to measure dissimilarities between objects. The main idea is to map the exterior and interior of the domain conformally to unit disks and circle domains (unit disk with several inner disks removed), using holomorphic 1-forms. A set of diffeomorphisms of the unit circle \(\mathbb {S}^{1}\) can be obtained, which together with the conformal modules are used to define the shape signature. A shape distance between shape signatures can be defined to measure dissimilarities between shapes. We prove theoretically that the proposed shape signature uniquely determines the multiply-connected objects under suitable normalization. We also introduce a reconstruction algorithm to obtain shapes from their signatures. This completes our framework and allows us to move back and forth between shapes and signatures. With that, a morphing algorithm between shapes can be developed through the interpolation of the Beltrami coefficients associated with the signatures. Experiments have been carried out on shapes extracted from real images. Results demonstrate the efficacy of our proposed algorithm as a stable shape representation scheme. 2014 IEEE.",,
Zeng,"Yang Y.-J., Zeng W., Chen J.-F.","The equiareality of NURBS surfaces greatly affects the results of visualization and tessellation applications, especially when dealing with extruding and intruding shapes. To improve the equiareality of given NURBS surfaces, an optimization algorithm using the M_bius transformations is presented in this paper. The optimal M_bius transformation is obtained by computing the intersection of two planar algebraic curves, whose coefficients are computed explicitly for B_zier and B-spline surfaces, while numerically for NURBS surfaces. Examples are given to show the performance of our algorithm for visualization and tessellation applications. 2013 Elsevier Inc. All rights reserved.",,
Zeng,"Zeng W., Lui L.M., Gu X.","This work proposes a novel framework for optimization in the constrained diffeomorphism space for deformable surface registration. First the diffeomorphism space is modeled as a special complex functional space on the source surface, the Beltrami coefficient space. The physically plausible constraints, in terms of feature landmarks and deformation types, define subspaces in the Beltrami coefficient space. Then the harmonic energy of the registration is minimized in the constrained subspaces. The minimization is achieved by alternating two steps: 1) optimization - diffuse the Beltrami coefficient, and 2) projection - first deform the conformal structure by the current Beltrami coefficient and then compose with a harmonic map from the deformed conformal structure to the target. The registration result is diffeomorphic, satisfies the physical landmark and deformation constraints, and minimizes the conformality distortion. Experiments on human facial surfaces demonstrate the efficiency and efficacy of the proposed registration framework. 2014 IEEE.",,
Zeng,"Li H., Zeng W., Morvan J.M., Chen L., Gu X.D.","Surface meshing plays a fundamental role in graphics and visualization. Many geometric processing tasks involve solving geometric PDEs on meshes. The numerical stability, convergence rates and approximation errors are largely determined by the mesh qualities. In practice, Delaunay refinement algorithms offer satisfactory solutions to high quality mesh generations. The theoretical proofs for volume based and surface based Delaunay refinement algorithms have been established, but those for conformal parameterization based ones remain wide open. This work focuses on the curvature measure convergence for the conformal parameterization based Delaunay refinement algorithms. Given a metric surface, the proposed approach triangulates its conformal uniformization domain by the planar Delaunay refinement algorithms, and produces a high quality mesh. We give explicit estimates for the Hausdorff distance, the normal deviation, and the differences in curvature measures between the surface and the mesh. In contrast to the conventional results based on volumetric Delaunay refinement, our stronger estimates are independent of the mesh structure and directly guarantee the convergence of curvature measures. Meanwhile, our result on Gaussian curvature measure is intrinsic to the Riemannian metric and independent of the embedding. In practice, our meshing algorithm is much easier to implement and much more efficient. The experimental results verified our theoretical results and demonstrated the efficiency of the meshing algorithm. 2014 IEEE.",,
Zeng,"Zhang M., Guo R., Zeng W., Luo F., Yau S.-T., Gu X.","Ricci flow deforms the Riemannian metric proportionally to the curvature, such that the curvature evolves according to a heat diffusion process and eventually becomes constant everywhere. Ricci flow has demonstrated its great potential by solving various problems in many fields, which can be hardly handled by alternative methods so far. This work introduces the unified theoretic framework for discrete surface Ricci flow, including all the common schemes: tangential circle packing, Thurston's circle packing, inversive distance circle packing and discrete Yamabe flow. Furthermore, this work also introduces a novel schemes, virtual radius circle packing and the mixed type schemes, under the unified framework. This work gives explicit geometric interpretation to the discrete Ricci energies for all the schemes with all back ground geometries, and the corresponding Hessian matrices. The unified frame work deepens our understanding to the discrete surface Ricci flow theory, and has inspired us to discover the new schemes, improved the flexibility and robustness of the algorithms, greatly simplified the implementation and improved the efficiency. Experimental results show the unified surface Ricci flow algorithms can handle general surfaces with different topologies, and is robust to meshes with different qualities, and is effective for solving real problems. 2014 Elsevier Inc. All rights reserved.",,
Zeng,"Yang C.-L., Wang W.-Z., Yang Y.-J., Lu L., Zhu Z.-J., Zhu B., Zeng W.","Visibility computation plays an important role in applications such as architectural design, art gallery patrolling and virtual worlds. In this paper, we present an algorithm to compute the weak visibility polygons (WVP) of Non Uniform Rational B-spline (NURBS) curves inside simple polygons. The NURBS curve is first subdivided into triangular curves. We then compute the WVP of each triangular curve by shearing that of its triangle hull. Finally, all triangular curves' WVPs are merged together to obtain the WVP of the NURBS curve. Analysis and examples are given to show the performance of our algorithm. 2013 Elsevier B.V. All rights reserved.",,
Zeng,"Luo W., Su Z., Zhang M., Zeng W., Dai J., Gu X.","A shape signature based on surface Ricci flow and optimal mass transportation is introduced for the purpose of surface comparison. First, the surface is conformally mapped onto plane by Ricci flow, which induces a measure on the planar domain. Second, the unique optimal mass transport map is computed that transports the new measure to the canonical measure on the plane. The map is obtained by a convex optimization process. This optimal transport map encodes all the information of the Riemannian metric on the surface. The shape signature consists of the optimal transport map, together with the mean curvature, which can fully recover the original surface. The discrete theories of surface Ricci flow and optimal mass transportation are explained thoroughly. The algorithms are given in detail. The signature is tested on human facial surfaces with different expressions accquired by structured light 3-D scanner based on phase-shifting method. The experimental results demonstrate the efficiency and efficacy of the method. 2014 Society of Photo-Optical Instrumentation Engineers.",,
Zeng,"Su Z., Zeng W., Shi R., Wang Y., Sun J., Gu X.","Brain mapping transforms the brain cortical surface to canonical planar domains, which plays a fundamental role in morphological study. Most existing brain mapping methods are based on angle preserving maps, which may introduce large area distortions. This work proposes an area preserving brain mapping method based on Monge-Brenier theory. The brain mapping is intrinsic to the Riemannian metric, unique, and diffeomorphic. The computation is equivalent to convex energy minimization and power Voronoi diagram construction. Comparing to the existing approaches based on Monge-Kantorovich theory, the proposed one greatly reduces the complexity (from n 2 unknowns to n ), and improves the simplicity and efficiency. Experimental results on caudate nucleus surface mapping and cortical surface mapping demonstrate the efficacy and efficiency of the proposed method. Conventional methods for caudate nucleus surface mapping may suffer from numerical instability, in contrast, current method produces diffeomorpic mappings stably. In the study of cortical surface classification for recognition of Alzheimer's Disease, the proposed method outperforms some other morphometry features. 2013 IEEE.",,
Zeng,"Shi R., Zeng W., Su Z., Damasio H., Lu Z., Wang Y., Yau S.-T., Gu X.","Automatic computation of surface correspondence via harmonic map is an active research field in computer vision, computer graphics and computational geometry. It may help document and understand physical and biological phenomena and also has broad applications in biometrics, medical imaging and motion capture. Although numerous studies have been devoted to harmonic map research, limited progress has been made to compute a diffeomorphic harmonic map on general topology surfaces with landmark constraints. This work conquer this problem by changing the Riemannian metric on the target surface to a hyperbolic metric, so that the harmonic mapping is guaranteed to be a diffeomorphism under landmark constraints. The computational algorithms are based on the Ricci flow method and the method is general and robust. We apply our algorithm to study constrained human brain surface registration problem. Experimental results demonstrate that, by changing the Riemannian metric, the registrations are always diffeomorphic, and achieve relative high performance when evaluated with some popular cortical surface registration evaluation standards. 2013 IEEE.",,
Zeng,"Gurijala K.C., Shi R., Zeng W., Gu X., Kaufman A.","We propose a new colon flattening algorithm that is efficient, shape-preserving, and robust to topological noise. Unlike previous approaches, which require a mandatory topological denoising to remove fake handles, our algorithm directly flattens the colon surface without any denoising. In our method, we replace the original Euclidean metric of the colon surface with a heat diffusion metric that is insensitive to topological noise. Using this heat diffusion metric, we then solve a Laplacian equation followed by an integration step to compute the final flattening. We demonstrate that our method is shape-preserving and the shape of the polyps are well preserved. The flattened colon also provides an efficient way to enhance the navigation and inspection in virtual colonoscopy. We further show how the existing colon registration pipeline is made more robust by using our colon flattening. We have tested our method on several colon wall surfaces and the experimental results demonstrate the robustness and the efficiency of our method. 2013 IEEE.",,
Zeng,"Zeng W., Shi R., Wang Y., Yau S.-T., Gu X.","We propose a novel method to apply Teichmôller space theory to study the signature of a family of nonintersecting closed 3D curves on a general genus zero closed surface. Our algorithm provides an efficient method to encode both global surface and local contour shape information. The signature - Teichmôller shape descriptor - is computed by surface Ricci flow method, which is equivalent to solving an elliptic partial differential equation on surfaces and is numerically stable. We propose to apply the new signature to analyze abnormalities in brain cortical morphometry. Experimental results with 3D MRI data from Alzheimer's disease neuroimaging initiative (ADNI) dataset [152 healthy control subjects versus 169 Alzheimer's disease (AD) patients] demonstrate the effectiveness of our method and illustrate its potential as a novel surface-based cortical morphometry measurement in AD research. 2012 Springer Science+Business Media New York.",,
Zeng,"Ban X., Goswami M., Zeng W., Gu X., Gao J.","In this paper we propose an algorithm to construct a 'space filling' curve for a sensor network with holes. Mathematically, for a given multi-hole domain R, we generate a path P that is provably aperiodic (i.e., any point is covered at most a constant number of times) and dense (i.e., any point of R is arbitrarily close to P). In a discrete setting as in a sensor network, the path visits the nodes with progressive density, which can adapt to the budget of the path length. Given a higher budget, the path covers the network with higher density. With a lower budget the path becomes proportional sparser. We show how this density-adaptive space filling curve can be useful for applications such as serial data fusion, motion planning for data mules, sensor node indexing, and double ruling type in-network data storage and retrieval. We show by simulation results the superior performance of using our algorithm vs standard space filling curves and random walks. 2013 IEEE.",,
Zeng,"Li S., Zeng W., Zhou D., Gu D.X., Gao J.","Motivated by mobile sensor networks as in participatory sensing applications, we are interested in developing a practical, lightweight solution for routing in a mobile network. While greedy routing is robust to mobility, location errors and link dynamics, it may get stuck in a local minimum, which then requires non-trivial recovery methods. We follow the approach taken by Sarkar et. al. [24] to find an embedding of the network such that greedy routing using the virtual coordinates guarantees delivery, thus eliminating the necessity of any recovery methods. Our new contribution is to replace the in-network computation of the embedding by a preprocessing of the domain before network deployment and encode the map of network domain to virtual coordinate space by using a small number of parameters which can be pre-loaded to all sensor nodes. As a result, the map is only dependent on the network domain and is independent of the network connectivity. Each node can directly compute or update its virtual coordinates by applying the locally stored map on its geographical coordinates. This represents the first practical solution for using virtual coordinates for greedy routing in a sensor network and could be easily extended to the case of a mobile network. Being extremely light-weight, greedy routing on the virtual coordinates is shown to be very robust to mobility, link dynamics and non-unit disk graph connectivity models. 2013 IEEE.",,
Zeng,"Gao M., Shi R., Zhang S., Zeng W., Qian Z., Gu X.D., Metaxas D., Axel L.","Current CT techniques are able to produce isotropic high resolution CT images (0.5mm). Recent research has revealed that the interior of the left ventricle has complex structures and topology, which has potentially valuable information. However, this makes the matching between models much more challenging. In this paper, we propose a novel method to match two models with non-trivial topology. 3D mesh models are flattened onto a 2D planar surfaces using discrete hyperbolic Ricci flow. Therefore, the 3D matching problem is converted to a much simpler 2D matching problem. We show the performance on the registration of high resolution left ventricle models. 2013 IEEE.",,
Zeng,"Zeng W., Li H., Chen L., Morvan J.-M., Gu X.D.","We propose a general and fully automatic framework for 3D facial expression recognition by modeling sparse representation of conformal images. According to Riemann Geometry theory, a 3D facial surface S embedded in reals3, which is a topological disk, can be conformally mapped to a 2D unit disk D through the discrete surface Ricci Flow algorithm. Such a conformal mapping induces a unique and intrinsic surface conformal representation denoted by a pair of functions defined on D, called conformal factor image (CFI) and mean curvature image (MCI). As facial expression features, CFI captures the local area distortion of S induced by the conformal mapping; MCI characterizes the geometry information of S. To model sparse representation of conformal images for expression classification, both CFI and MCI are further normalized by a M_bius transformation. This transformation is defined by the three main facial landmarks (i.e. nose tip, left and right inner eye corners) which can be detected automatically and precisely. Expression recognition is carried out by the minimal sparse expression-class-dependent reconstruction error over the conformal image based expression dictionary. Extensive experimental results on the BU-3DFER dataset demonstrate the effectiveness and generalization of the proposed framework. 2013 IEEE.",,
Zeng,"Shi R., Zeng W., Su Z., Wang Y., Damasio H., Lu Z., Yau S.-T., Gu X.","Brain Cortical surface registration is required for inter-subject studies of functional and anatomical data. Harmonic mapping has been applied for brain mapping, due to its existence, uniqueness, regularity and numerical stability. In order to improve the registration accuracy, sculcal landmarks are usually used as constraints for brain registration. Unfortunately, constrained harmonic mappings may not be diffeomorphic and produces invalid registration. This work conquer this problem by changing the Riemannian metric on the target cortical surface to a hyperbolic metric, so that the harmonic mapping is guaranteed to be a diffeomorphism while the landmark constraints are enforced as boundary matching condition. The computational algorithms are based on the Ricci flow method and hyperbolic heat diffusion. Experimental results demonstrate that, by changing the Riemannian metric, the registrations are always diffeomorphic, with higher qualities in terms of landmark alignment, curvature matching, area distortion and overlapping of region of interests. 2013 Springer-Verlag.",,
Zeng,"Zeng W., Goswami M., Luo F., Gu X.","Surface registration plays a fundamental role in many applications in computer vision and aims at finding a one-to-one correspondence between surfaces. Conformal mapping based surface registration methods conformally map 2D/3D surfaces onto 2D canonical domains and perform the matching on the 2D plane. This registration framework reduces dimensionality, and the result is intrinsic to Riemannian metric and invariant under isometric deformation. However, conformal mapping will be affected by inconsistent boundaries and non-isometric deformations of surfaces. In this work, we quantify the effects of boundary variation and non-isometric deformation to conformal mappings, and give the theoretical upper bounds for the distortions of conformal mappings under these two factors. Besides giving the thorough theoretical proofs of the theorems, we verified them by concrete experiments using 3D human facial scans with dynamic expressions and varying boundaries. Furthermore, we used the distortion estimates for reducing search range in feature matching of surface registration applications. The experimental results are consistent with the theoretical predictions and also demonstrate the performance improvements in feature tracking. 2013 IEEE.",,
Zeng,"Yang Y.-J., Zeng W., Yang C.-L., Deng B., Meng X.-X., Sitharama Iyengar S.","The parameterization of rational B_zier surfaces greatly affects rendering and tessellation results. The uniformity and orthogonality of iso-parametric curves are two key properties of the optimal parameterization. The only rational B_zier surfaces with uniform iso-parametric curves are bilinear surfaces, and the only rational B_zier surfaces with uniform and orthogonal iso-parametric curves are rectangles. To improve the uniformity and orthogonality of iso-parametric curves for general rational B_zier surfaces, an optimization algorithm using the rational bilinear reparameterizations is presented, which can produce a better parameterization with the cost of degree elevation. Examples are given to show the performance of our algorithm for rendering and tessellation applications. 2012 Elsevier Ltd. All rights reserved.",,
Zeng,"Shi R., Zeng W., Su Z., Wang Y., Damasio H., Lu Z., Yau S.T., Gu X.","Brain Cortical surface registration is required for inter-subject studies of functional and anatomical data. Harmonic mapping has been applied for brain mapping, due to its existence, uniqueness, regularity and numerical stability. In order to improve the registration accuracy, sculcal landmarks are usually used as constraints for brain registration. Unfortunately, constrained harmonic mappings may not be diffeomorphic and produces invalid registration. This work conquer this problem by changing the Riemannian metric on the target cortical surface o a hyperbolic metric, so that the harmonic mapping is guaranteed to be a diffeomorphism while the landmark constraints are enforced as boundary matching condition. The computational algorithms are based on the Ricci flow method and yperbolic heat diffusion. Experimental results demonstrate that, by changing the Riemannian metric, the registrations are always diffeomorphic, with higher qualities in terms of landmark alignment, curvature matching, area distortion and overlapping of region of interests.",,
Zeng,"Gurijala K.C., Kaufman A., Zeng W., Gu X.","The colon is a very complicated structure with a large number of distortions and bends. Landmarks and features serve as tools to guide colon flattening and to assist in the study of the colon surface segment by segment. Identification of feature points and landmarks is also useful for the registration of colon surfaces. In this paper, we present methods for identifying the locations of the taeniae coli and the four major flexures which form the prominent anatomic landmarks on the colon surface. The colon surface is cut open along these landmarks, and the segments obtained can be used for study of the surface of the colon. We define new feature points on the flattened colon surfaces and use well-established graph-based algorithms for their detection. We demonstrate the results showing the extracted landmarks and the detected features. 2011 Springer-Verlag.",,
Zeng,"Zeng W., Marino J., Gu X., Kaufman A.","In virtual colonoscopy, CT scans are typically acquired with the patient in both supine and prone positions. The registration of these two scans is desirable so that the physician can clarify situations or confirm polyp findings at a location in one scan with the same location in the other, thereby improving polyp detection rates and reducing false positives. However, this supine-prone registration is challenging because of the substantial distortions in the colon shape due to the patient's position shifting. We present an efficient algorithm and framework for performing this registration through the use of conformal geometry to guarantee the registration is a diffeomorphism. The colon surface is conformally flattened to a rectangle using holomorphic differentials. The flattened domains of supine and prone are aligned by the harmonic map with feature correspondence constraints. We demonstrate the efficiency and efficacy of our method by measuring the distance between features on the registered colons. 2011 Springer-Verlag.",,
Zeng,"Zeng W., Gu X.D.","Dynamics analysis for 3D surface sequence is an important task in vision. In this work, 2D shape analysis method [16] based on Teichmôller space theory is generalized to handle deforming surfaces. 2011 IEEE.",,
Zeng,"Zeng W., Gu D.X.","Shape analysis has fundamental importance in computer vision, including surface matching, registration, tracking, classification, recognition, etc. Computational methods based on conformal geometry can map any 3D surface to a 2D canonical domain. They provide a novel approach to 3D problems by converting them to 2D problems, which greatly simplifies the computation process. With this framework, we present a series of matching and registration methods for general surfaces, especially with large non-rigid deformations and complicated topologies. In addition, we introduce a global and unique shape signature for shape recognition applications. Experiments on a variety of real scans from different equipments demonstrate the efficiency and efficacy of the conformal geometric methods in computer vision applications. 2011 IEEE.",,
Zeng,"Gu X.D., Zeng W., Luo F., Yau S.-T.",We report recent progress in the computation of conformal mappings from surfaces with arbitrary topologies to canonical domains. Two major computational methodologies are emphasized; one is holomorphic differentials based on Riemann surface theory and the other is surface Ricci ow from geometric analysis. The applications of surface conformal mapping in the field of engineering are briey reviewed. 2011 Heldermann Verlag.,,
Zeng,"Marino J., Zeng W., Gu X., Kaufman A.","When visualizing tubular 3D structures, external representations are often used for guidance and display, and such views in 2D can often contain occlusions. Virtual dissection methods have been proposed where the entire 3D structure can be mapped to the 2D plane, though these will lose context by straightening curved sections. We present a new method of creating maps of 3D tubular structures that yield a succinct view while preserving the overall geometric structure. Given a dominant view plane for the structure, its curve skeleton is first projected to a 2D skeleton. This 2D skeleton is adjusted to account for distortions in length, modified to remove intersections, and optimized to preserve the shape of the original 3D skeleton. Based on this shaped 2D skeleton, a boundary for the map of the object is obtained based on a slicing path through the structure and the radius around the skeleton. The sliced structure is conformally mapped to a rectangle and then deformed via harmonic mapping to match the boundary placement. This flattened map preserves the general geometric context of a 3D object in a 2D display, and rendering of this flattened map can be accomplished using volumetric ray casting. We have evaluated our method on real datasets of human colon models. 2011 IEEE.",,
Zeng,"Zeng W., Gu X.D.","A novel method for registering 3D surfaces with large deformations is presented, which is based on quasi-conformal geometry. A general diffeomorphism distorts the conformal structure of the surface, which is represented as the Beltrami coefficient. Inversely, the diffeomorphism can be determined by the Beltrami coefficient in an essentially unique way. Our registration method first extracts the features on the surfaces, then estimates the Beltrami coefficient, and finally uniquely determines the registration mapping by solving Beltrami equations using curvature flow. The method is 1) general, it can search the desired registration in the whole space of diffeomorphisms, which includes the conventional searching spaces, such as rigid motions, isometric transformations or conformal mappings; 2) global optimal, the global optimum is determined by the method unique up to a 3 dimensional transformation group; 3) robust, it handles large surfaces with complicated topologies; 4) rigorous, it has solid theoretic foundation. Experiments on the real surfaces with large deformations and complicated topologies demonstrate the efficiency, robustness of the proposed method. 2011 IEEE.",,
Zeng,"Zeng W., Shi R., Gu X.D.","Surface remeshing plays a fundamental role in digital geometry processing. In this paper, we present a novel framework for global surface remeshing based on symmetric Delaunay triangulations on the uniformization spaces. Surfaces with arbitrary topologies can be uniformized to one of three canonical uniformization spaces: the sphere, the Euclidean plane and the hyperbolic plane. The uniformization process induces a symmetry group. A triangulation on the original surface is converted to a triangulation on the uniformization space, which is invariant under the symmetry group, and vice versa. Furthermore, the uniformization process preserves angles, so a symmetric dense Delaunay triangulation on the uniformization space induces a good triangulation on the original surface. We construct the uniformization of surfaces with arbitrary topologies using Ricci flow, and apply orbit insertion technique to the Delaunay refinement algorithm which ensures the symmetry. Experimental results demonstrates that our method is global, intrinsic, general and efficient. 2011 IEEE.",,
Zeng,"Yu X., Ban X., Zeng W., Sarkar R., Gu X., Gao J.","In this paper we address the problem of scalable and load balanced routing for wireless sensor networks. Motivated by the analog of the continuous setting that geodesic routing on a sphere gives perfect load balancing, we embed sensor nodes on a convex polyhedron in 3D and use greedy routing to deliver messages between any pair of nodes with guaranteed success. This embedding is known to exist by the Koebe-Andreev-Thurston Theorem for any 3-connected planar graphs. In our paper we use discrete Ricci flow to develop a distributed algorithm to compute this embedding. Further, such an embedding is not unique and differs from one another by a Mbius transformation. We employ an optimization routine to look for the Mbius transformation such that the nodes are spread on the polyhedron as uniformly as possible. We evaluated the load balancing property of this greedy routing scheme and showed favorable comparison with previous schemes. 2011 IEEE.",,
Zeng,"Jiang R., Ban X., Goswami M., Zeng W., Gao J., Gu X.","In a sensor network there are many paths between a source and a destination. An efficient method to explore and navigate in the path space can help many important routing primitives, in particular, multipath routing and resilient routing (when nodes or links can fail unexpectedly) as considered in this paper. Both problems are challenging for a general graph setting, especially if each node cannot afford to have the global knowledge. In this paper we use a geometric approach to allow efficient exploration of the path space with very little overhead. We are motivated by the recent development on regulating a sensor network geometry using confor-mal mapping [44,45], in which any sensor network can be embedded to be circular (and any possible hole is made circular as well) and greedy routing guarantees delivery. In this paper we explore the freedom of a Mbius transformation inherent to this conformal mapping. By applying a Mbius transformation we can get an alternative embedding with the same property such that greedy routing generates a different path. We present distributed algorithms using local information and limited global information (the positions and sizes of the holes) to generate disjoint multi-paths for a given source destination pair or switch to a different path on the fly when transmission failure is encountered. The overhead of applying a Mbius transformation simply boils down to four parameters that could be carried by a packet or determined at need at the source. Demonstrated by simulation results, this method compares favorably in terms of performance and cost metrics with centralized solutions of using flow algorithms or random walk based decentralized solutions in generating alternative paths. 2011 ACM.",,
Zeng,"Zeng W., Marino J., Kaufman A., Gu X.D.","Volumetric colon wall unfolding is a novel method for virtual colon analysis and visualization with valuable applications in virtual colonoscopy (VC) and computer-aided detection (CAD) systems. A volumetrically unfolded colon enables doctors to visualize the entire colon structure without occlusions due to haustral folds, and is critical for performing efficient and accurate texture analysis on the volumetric colon wall. Though conventional colon surface flattening has been employed for these uses, volumetric colon unfolding offers the advantages of providing the needed quantities of information with needed accuracy. This work presents an efficient and effective volumetric colon unfolding method based on harmonic differentials. The colon volumes are reconstructed from CT images and are represented as tetrahedral meshes. Three harmonic 1-forms, which are linearly independent everywhere, are computed on the tetrahedral mesh. Through integration of the harmonic 1-forms, the colon volume is mapped periodically to a canonical cuboid. The method presented is automatic, simple, and practical. Experimental results are reported to show the performance of the algorithm on real medical datasets. Though applied here specifically to the colon, the method is general and can be generalized for other volumes.",,
Zeng,"Zeng W., Zeng Y., Wang Y., Yin X., Gu X., Samaras D.","3D surface matching is fundamental for shape registration, deformable 3D non-rigid tracking, recognition and classification. In this paper we describe a novel approach for generating an efficient and optimal combined matching from multiple boundary-constrained conformal parameterizations for multiply connected domains (i.e., genus zero open surface with multiple boundaries), which always come from imperfect 3D data acquisition (holes, partial occlusions, change of pose and non-rigid deformation between scans). This optimality criterion is also used to assess how consistent each boundary is, and thus decide to enforce or relax boundary constraints across the two surfaces to be matched. The linear boundary-constrained conformal parameterization is based on the holomorphic differential forms, which map a surface with n boundaries conformally to a planar rectangle with (n - 2) horizontal slits, other two boundaries as constraints. The mapping is a diffeomorphism and intrinsic to the geometry, handles an open surface with arbitrary number of boundaries, and can be implemented as a linear system. Experimental results are given for real facial surface matching, deformable cloth non-rigid tracking, which demonstrate the efficiency of our method, especially for 3D non-rigid surfaces with significantly inconsistent boundaries. 2008 Springer Berlin Heidelberg.",,
Zeng,"Zeng W., Yin X., Zeng Y., Lai Y., Gu X., Samaras D.","3D surface matching is fundamental for shape analysis. As a powerful method in geometric analysis, Ricci flow can flexibly design metrics by prescribed target curvature. In this paper we describe a novel approach for matching surfaces with complicated topologies based on hyperbolic Ricci flow. For surfaces with negative Euler characteristics, such as a human face with holes (eye contours), the canonical hyperbolic metric is conformal to the original and can be efficiently computed. Then the surface can be canonically decomposed to hyperbolic hexagons. By matching the corresponding hyperbolic hexagons, the matching between surfaces can be easily established. Compared to existing methods, hyperbolic Ricci flow induces dijfeomorphisms between surfaces with complicated topologies with negative Euler characteristics, while avoiding singularities. Furthermore, all the boundaries are intrinsically mapped to hyperbolic lines as alignment constraints. Finally, we demonstrate the applicability of this intrinsic shape representation for 3D face matching and registration.",,